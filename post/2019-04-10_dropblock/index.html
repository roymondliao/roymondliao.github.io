<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Roymond Liao">

  
  
  
    
  
  <meta name="description" content="Dropout 相關方法：
 Dropout: 完全隨機丟棄 neuron Sparital Dropout: 按 channel 隨機丟棄 Stochastic Depth: 按 res block 隨機丟棄 DropBlock: 每個 feature map 上按 spatial square 隨機丟棄 Cutout: 在 input layer 按 spatial square 隨機丟棄 DropConnect: 只在連接處丟，不丟 neuron DropBlock  Idea 一般的 Dropout 都是用在 fully connection layer，而在 convolutional network 上使用 dropout 的意義並不大，該文章則認為因為在每一個 feature maps 的位置都具有一個 receptive field，僅對單一像素位置進行 dropout 並不能降低 feature maps 學習特徵範圍，也就是說，network 能夠特過相鄰位置的特徵值去學習，也不會特別加強去學習保留下來的訊息。既然對於單獨的對每個位置進行 dropout 並無法提高 network 本身的泛化能力，那就以區塊的概念來進行 dropout，反而更能讓 network 去學習保留下來的訊息，而加重特徵的權重。 Method  不同 feature maps 共享相同的 dropblock mask，在相同的位置丟棄訊息 每一層的 feature maps 使用各自的 dropblock mask  Parameters block size: 控制要讓 value of feature maps 歸為 0 的區塊大小 $ \gamma $: 用來控制要丟棄特徵的數量 keep_prob: 與 dropout 的參數相同 Code implement https://github.">

  
  <link rel="alternate" hreflang="en-us" href="https://roymondliao.github.io/post/2019-04-10_dropblock/">

  


  
  
  
  <meta name="theme-color" content="#007D8C">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bitter:400,700%7COpen+Sans:400,400italic,700%7CRaleway&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-151909728-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-151909728-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://roymondliao.github.io/post/2019-04-10_dropblock/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Roymond Liao">
  <meta property="og:url" content="https://roymondliao.github.io/post/2019-04-10_dropblock/">
  <meta property="og:title" content="DropBlock | Roymond Liao">
  <meta property="og:description" content="Dropout 相關方法：
 Dropout: 完全隨機丟棄 neuron Sparital Dropout: 按 channel 隨機丟棄 Stochastic Depth: 按 res block 隨機丟棄 DropBlock: 每個 feature map 上按 spatial square 隨機丟棄 Cutout: 在 input layer 按 spatial square 隨機丟棄 DropConnect: 只在連接處丟，不丟 neuron DropBlock  Idea 一般的 Dropout 都是用在 fully connection layer，而在 convolutional network 上使用 dropout 的意義並不大，該文章則認為因為在每一個 feature maps 的位置都具有一個 receptive field，僅對單一像素位置進行 dropout 並不能降低 feature maps 學習特徵範圍，也就是說，network 能夠特過相鄰位置的特徵值去學習，也不會特別加強去學習保留下來的訊息。既然對於單獨的對每個位置進行 dropout 並無法提高 network 本身的泛化能力，那就以區塊的概念來進行 dropout，反而更能讓 network 去學習保留下來的訊息，而加重特徵的權重。 Method  不同 feature maps 共享相同的 dropblock mask，在相同的位置丟棄訊息 每一層的 feature maps 使用各自的 dropblock mask  Parameters block size: 控制要讓 value of feature maps 歸為 0 的區塊大小 $ \gamma $: 用來控制要丟棄特徵的數量 keep_prob: 與 dropout 的參數相同 Code implement https://github."><meta property="og:image" content="https://roymondliao.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://roymondliao.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-04-10T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-04-10T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://roymondliao.github.io/post/2019-04-10_dropblock/"
  },
  "headline": "DropBlock",
  
  "datePublished": "2019-04-10T00:00:00Z",
  "dateModified": "2019-04-10T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Roymond Liao"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Roymond Liao",
    "logo": {
      "@type": "ImageObject",
      "url": "https://roymondliao.github.io/img/icon-512.png"
    }
  },
  "description": "Dropout 相關方法：\n Dropout: 完全隨機丟棄 neuron Sparital Dropout: 按 channel 隨機丟棄 Stochastic Depth: 按 res block 隨機丟棄 DropBlock: 每個 feature map 上按 spatial square 隨機丟棄 Cutout: 在 input layer 按 spatial square 隨機丟棄 DropConnect: 只在連接處丟，不丟 neuron DropBlock  Idea 一般的 Dropout 都是用在 fully connection layer，而在 convolutional network 上使用 dropout 的意義並不大，該文章則認為因為在每一個 feature maps 的位置都具有一個 receptive field，僅對單一像素位置進行 dropout 並不能降低 feature maps 學習特徵範圍，也就是說，network 能夠特過相鄰位置的特徵值去學習，也不會特別加強去學習保留下來的訊息。既然對於單獨的對每個位置進行 dropout 並無法提高 network 本身的泛化能力，那就以區塊的概念來進行 dropout，反而更能讓 network 去學習保留下來的訊息，而加重特徵的權重。 Method  不同 feature maps 共享相同的 dropblock mask，在相同的位置丟棄訊息 每一層的 feature maps 使用各自的 dropblock mask  Parameters block size: 控制要讓 value of feature maps 歸為 0 的區塊大小 $ \\gamma $: 用來控制要丟棄特徵的數量 keep_prob: 與 dropout 的參數相同 Code implement https://github."
}
</script>

  

  


  


  





  <title>DropBlock | Roymond Liao</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Roymond Liao</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/sharing/"><span>Sharing</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>DropBlock</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/roymond-liao/">Roymond Liao</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 10, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/2019-04-10_dropblock/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/optimizer/">Optimizer</a>, <a href="/categories/deep-learning/">Deep Learning</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Dropout 相關方法：</p>

<ul>
<li>Dropout: 完全隨機丟棄 neuron</li>
<li>Sparital Dropout: 按 channel 隨機丟棄</li>
<li>Stochastic Depth: 按 res block 隨機丟棄</li>
<li>DropBlock: 每個 feature map 上按 spatial square 隨機丟棄</li>
<li>Cutout: 在 input layer 按 spatial square 隨機丟棄</li>
<li>DropConnect: 只在連接處丟，不丟 neuron</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650751601&amp;idx=5&amp;sn=6ba09bea3acb116eb9f4902af5261e72&amp;chksm=871a860fb06d0f194c4c0452e53d21cc6c537b4e33a5aea4e3c0a067db0c46d41168afabcc0c&amp;scene=21#wechat_redirect">DropBlock</a>

<ul>
<li>Idea</li>
<li>一般的 Dropout 都是用在 fully connection layer，而在 convolutional network 上使用 dropout 的意義並不大，該文章則認為因為在每一個 feature maps 的位置都具有一個 <a href="https://zhuanlan.zhihu.com/p/28492837">receptive field</a>，僅對單一像素位置進行 dropout 並不能降低 feature maps 學習特徵範圍，也就是說，network 能夠特過<strong>相鄰位置</strong>的特徵值去學習，也不會特別加強去學習保留下來的訊息。既然對於單獨的對每個位置進行 dropout 並無法提高 network 本身的泛化能力，那就以區塊的概念來進行 dropout，反而更能讓 network 去學習保留下來的訊息，而加重特徵的權重。</li>
<li>Method

<ul>
<li>不同 feature maps 共享相同的 dropblock mask，在相同的位置丟棄訊息</li>
<li>每一層的 feature maps 使用各自的 dropblock mask</li>
</ul></li>
<li>Parameters</li>
<li>block size: 控制要讓 value of feature maps 歸為 0 的區塊大小</li>
<li>$ \gamma $: 用來控制要丟棄特徵的數量</li>
<li>keep_prob: 與 dropout 的參數相同</li>
<li>Code implement</li>
<li><a href="https://github.com/DHZS/tf-dropblock/blob/master/nets/dropblock.py">https://github.com/DHZS/tf-dropblock/blob/master/nets/dropblock.py</a></li>
<li><a href="https://github.com/shenmbsw/tensorflow-dropblock/blob/master/dropblock.py">https://github.com/shenmbsw/tensorflow-dropblock/blob/master/dropblock.py</a><br></li>
</ul></li>
</ul>

<p>Bernoulli distrubtion:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
tf<span style="color:#f92672">.</span>reset_default_graph()
<span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>Graph()<span style="color:#f92672">.</span>as_default() <span style="color:#66d9ef">as</span> g: 
    mean <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, [None])
    input_shape <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, [None, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>])
    shape <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>stack(tf<span style="color:#f92672">.</span>shape(input_shape))
    <span style="color:#75715e"># method 1</span>
    <span style="color:#75715e"># 用 uniform distributions 產生值，再透過 sign 轉為 [-1, 1], 最後透過 relu 將 -1 轉換為 0</span>
    uniform_dist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random_uniform(shape, minval<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, maxval<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
    sign_dist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>sign(mean <span style="color:#f92672">-</span> uniform_dist)
    bernoulli <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu(sign_dist)
    <span style="color:#75715e"># method 2</span>
    <span style="color:#75715e"># probs 可以為多個 p, 對應 shape, 產生 n of p 的 bernoulli distributions</span>
    noise_dist <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>Bernoulli(probs<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.1</span>])
    mask <span style="color:#f92672">=</span> noise_dist<span style="color:#f92672">.</span>sample(shape)
<span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>Session(graph<span style="color:#f92672">=</span>g) <span style="color:#66d9ef">as</span> sess:
    tmp_array <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros([<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>], dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>uint8) 
    tmp_array[:,:<span style="color:#ae81ff">100</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">255</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>] <span style="color:#75715e">#Orange left side array[:,100:] = [0, 0, 255] #Blue right side</span>
    batch_array <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([tmp_array]<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span>)
    uniform, sign, bern <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run([uniform_dist, sign_dist, bernoulli], feed_dict<span style="color:#f92672">=</span>{mean: [<span style="color:#ae81ff">1.</span>], input_shape:batch_array})    </code></pre></div>
<p>DropBlock implement:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#f92672">from</span> tensorflow.python.keras <span style="color:#f92672">import</span> backend <span style="color:#66d9ef">as</span> K
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DropBlock</span>(tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Layer) :
    <span style="color:#66d9ef">def</span> __init__(self, keep_prob, block_size, <span style="color:#f92672">**</span>kwargs):
        super(DropBlock, self)<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">**</span>kwargs)
        self<span style="color:#f92672">.</span>keep_prob <span style="color:#f92672">=</span> float(keep_prob) <span style="color:#66d9ef">if</span> isinstance(keep_prob, int) <span style="color:#66d9ef">else</span> keep_prob
        self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">=</span> int(block_size)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_output_shape</span>(self, input_shape):
        <span style="color:#66d9ef">return</span> input_shape
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build</span>(self, input_shape):
        _, self<span style="color:#f92672">.</span>h, self<span style="color:#f92672">.</span>w, self<span style="color:#f92672">.</span>channel <span style="color:#f92672">=</span> input_shape<span style="color:#f92672">.</span>as_list()
        <span style="color:#75715e"># pad the mask</span>
        bottom <span style="color:#f92672">=</span> right <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>
        top <span style="color:#f92672">=</span> left <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> bottom
        self<span style="color:#f92672">.</span>padding <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>], [top, bottom], [left, right], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]]
        self<span style="color:#f92672">.</span>set_keep_prob()
        super(DropBlock, self)<span style="color:#f92672">.</span>build(input_shape)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_keep_prob</span>(self, keep_prob<span style="color:#f92672">=</span>None):
        <span style="color:#e6db74">&#34;&#34;&#34;This method only support Eager Execution&#34;&#34;&#34;</span>
        <span style="color:#66d9ef">if</span> keep_prob <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            self<span style="color:#f92672">.</span>keep_prob <span style="color:#f92672">=</span> keep_prob
        w, h <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>to_float(self<span style="color:#f92672">.</span>w), tf<span style="color:#f92672">.</span>to_float(self<span style="color:#f92672">.</span>h)
        self<span style="color:#f92672">.</span>gamma <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>keep_prob) <span style="color:#f92672">*</span> (w <span style="color:#f92672">*</span> h) <span style="color:#f92672">/</span> (self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>) <span style="color:#f92672">/</span> ((w <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> (h <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>))

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_create_mask</span>(self, input_shape):
        sampling_mask_shape <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>stack([input_shape[<span style="color:#ae81ff">0</span>], 
                                        self<span style="color:#f92672">.</span>h <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, 
                                        self<span style="color:#f92672">.</span>w <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>block_size <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>,
                                        self<span style="color:#f92672">.</span>channel])
        mask <span style="color:#f92672">=</span> DropBlock<span style="color:#f92672">.</span>_bernoulli(sampling_mask_shape, self<span style="color:#f92672">.</span>gamma)
        <span style="color:#75715e"># 擴充行列，並給予0值，依據 paddings 參數給予的上下左右值來做擴充，mode有三種模式可選，可參考 document</span>
        mask <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>pad(tensor<span style="color:#f92672">=</span>mask, paddings<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>padding, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;CONSTANT&#39;</span>) 
        mask <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>max_pool(value<span style="color:#f92672">=</span>mask, 
                              ksize<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>block_size, self<span style="color:#f92672">.</span>block_size, <span style="color:#ae81ff">1</span>], 
                              strides<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], 
                              padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SAME&#39;</span>)
        mask <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> mask
        <span style="color:#66d9ef">return</span> mask
        
    <span style="color:#a6e22e">@staticmethod</span>    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_bernoulli</span>(shape, mean):
        <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu(tf<span style="color:#f92672">.</span>sign(mean <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>random_uniform(shape, minval<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, maxval<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)))
    
    <span style="color:#75715e"># The call function is a built-in function in &#39;tf.keras&#39;.</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">call</span>(self, inputs, training<span style="color:#f92672">=</span>None, scale<span style="color:#f92672">=</span>True, <span style="color:#f92672">**</span>kwargs):
        <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">drop</span>():
            mask <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_create_mask(tf<span style="color:#f92672">.</span>shape(inputs))
            output <span style="color:#f92672">=</span> inputs <span style="color:#f92672">*</span> mask
            output <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cond(tf<span style="color:#f92672">.</span>constant(scale, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>bool) <span style="color:#66d9ef">if</span> isinstance(scale, bool) <span style="color:#66d9ef">else</span> scale,
                             true_fn<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span>: output <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>to_float(tf<span style="color:#f92672">.</span>size(mask)) <span style="color:#f92672">/</span> tf<span style="color:#f92672">.</span>reduce_sum(mask),
                             false_fn<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span>: output)
            <span style="color:#66d9ef">return</span> output
        
        <span style="color:#66d9ef">if</span> training <span style="color:#f92672">is</span> None:
            training <span style="color:#f92672">=</span> K<span style="color:#f92672">.</span>learning_phase()
        output <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>cond(tf<span style="color:#f92672">.</span>logical_or(tf<span style="color:#f92672">.</span>logical_not(training), tf<span style="color:#f92672">.</span>equal(self<span style="color:#f92672">.</span>keep_prob, <span style="color:#ae81ff">1.0</span>)),
                         true_fn<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span>: inputs,
                         false_fn<span style="color:#f92672">=</span>drop)
        <span style="color:#66d9ef">return</span> output</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Testing</span>
a <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, [None, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>])
keep_prob <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32)
training <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>bool)

drop_block <span style="color:#f92672">=</span> DropBlock(keep_prob<span style="color:#f92672">=</span>keep_prob, block_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
b <span style="color:#f92672">=</span> drop_block(inputs<span style="color:#f92672">=</span>a, training<span style="color:#f92672">=</span>training)

sess <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Session()
feed_dict <span style="color:#f92672">=</span> {a: np<span style="color:#f92672">.</span>ones([<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>]), keep_prob: <span style="color:#ae81ff">0.8</span>, training: True}
c <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run(b, feed_dict<span style="color:#f92672">=</span>feed_dict)

<span style="color:#66d9ef">print</span>(c[<span style="color:#ae81ff">0</span>, :, :, <span style="color:#ae81ff">0</span>])</code></pre></div>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650752571&amp;idx=1&amp;sn=8417645148afd8eebdb79c91b37a7409&amp;chksm=871a8245b06d0b53115d79f1ce42bc5a03aad5d038fe51c2f237c5848c41c51c5b756aaa8937&amp;scene=21#wechat_redirect">Targeted Dropout</a></li>
</ul>

<p>Reference:</p>

<ol>
<li><a href="https://cloud.tencent.com/developer/article/1367373">https://cloud.tencent.com/developer/article/1367373</a></li>
</ol>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/optimizer/">Optimizer</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
  </ul>
</div>












  
  
    
  
  






  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/roymond-liao/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>




<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "roymondliao" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>






  
  



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.3.1/mermaid.min.js" integrity="sha256-vOIuDSYDirTfyr+S2MjFnhOz6Rgiz4ODFAHATG0rFxw=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/markdown.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://roymondliao.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d6bd04fdad2ad213aa8111c5a3b72fc5.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © 2019 - © 2020 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
