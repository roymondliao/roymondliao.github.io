<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Roymond Liao">

  
  
  
    
  
  <meta name="description" content="Attention Mechanism Attention 的概念在 2014 年被 Bahdanau et al. [Paper 1.] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。
 用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。
 複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 \({y_1, \cdots, y_{t-1}}\) 的條件下字詞 $y_{t}$ 的機率，所以 decoder 的定義是在有序的條件下所有預測字詞的聯合機率：
\[ \begin{align} p(\mathrm{y}) & = \prod_{t=1}^T p(y_t | \{y_1, \cdots, y_{t-1}\}, c) \tag 1 \\ \mathrm{y} & = (y_1, \cdots, y_T) \end{align} \]">

  
  <link rel="alternate" hreflang="en-us" href="https://roymondliao.netlify.com/post/2019-12-16_transformer_part2/">

  


  
  
  
  <meta name="theme-color" content="#007D8C">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bitter:400,700%7COpen+Sans:400,400italic,700%7CRaleway&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-151909728-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-151909728-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://roymondliao.netlify.com/post/2019-12-16_transformer_part2/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Roymond Liao">
  <meta property="og:url" content="https://roymondliao.netlify.com/post/2019-12-16_transformer_part2/">
  <meta property="og:title" content="Transformer Part 2 - Attention | Roymond Liao">
  <meta property="og:description" content="Attention Mechanism Attention 的概念在 2014 年被 Bahdanau et al. [Paper 1.] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。
 用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。
 複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 \({y_1, \cdots, y_{t-1}}\) 的條件下字詞 $y_{t}$ 的機率，所以 decoder 的定義是在有序的條件下所有預測字詞的聯合機率：
\[ \begin{align} p(\mathrm{y}) & = \prod_{t=1}^T p(y_t | \{y_1, \cdots, y_{t-1}\}, c) \tag 1 \\ \mathrm{y} & = (y_1, \cdots, y_T) \end{align} \]"><meta property="og:image" content="https://roymondliao.netlify.com/post/2019-12-16_transformer_part2/featured.jpg">
  <meta property="twitter:image" content="https://roymondliao.netlify.com/post/2019-12-16_transformer_part2/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-02-28T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-02-28T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://roymondliao.netlify.com/post/2019-12-16_transformer_part2/"
  },
  "headline": "Transformer Part 2 - Attention",
  
  "image": [
    "https://roymondliao.netlify.com/post/2019-12-16_transformer_part2/featured.jpg"
  ],
  
  "datePublished": "2020-02-28T00:00:00Z",
  "dateModified": "2020-02-28T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Roymond Liao"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Roymond Liao",
    "logo": {
      "@type": "ImageObject",
      "url": "https://roymondliao.netlify.com/img/icon-512.png"
    }
  },
  "description": "Attention Mechanism Attention 的概念在 2014 年被 Bahdanau et al. [Paper 1.] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。\n 用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。\n 複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 \\({y_1, \\cdots, y_{t-1}}\\) 的條件下字詞 $y_{t}$ 的機率，所以 decoder 的定義是在有序的條件下所有預測字詞的聯合機率：\n\\[ \\begin{align} p(\\mathrm{y}) \u0026 = \\prod_{t=1}^T p(y_t | \\{y_1, \\cdots, y_{t-1}\\}, c) \\tag 1 \\\\ \\mathrm{y} \u0026 = (y_1, \\cdots, y_T) \\end{align} \\]"
}
</script>

  

  


  


  





  <title>Transformer Part 2 - Attention | Roymond Liao</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Roymond Liao</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/sharing/"><span>Sharing</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Transformer Part 2 - Attention</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/roymond-liao/">Roymond Liao</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Feb 28, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/2019-12-16_transformer_part2/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/nlp/">NLP</a>, <a href="/categories/deep-learning/">Deep Learning</a></span>
  

</div>

  














</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 1200px; max-height: 800px;">
  <div style="position: relative">
    <img src="/post/2019-12-16_transformer_part2/featured_hu9d2f8fc9da6765ac8734599c6de81438_973737_1200x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    <span class="article-header-caption">Photo by Martin Adams on Unsplash</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="attention-mechanism">Attention Mechanism</h1>

<p><strong>Attention</strong> 的概念在 2014 年被 Bahdanau et al. [Paper 1.] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。</p>

<blockquote>
<p>用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。</p>
</blockquote>

<p>複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 <span  class="math">\({y_1, \cdots, y_{t-1}}\)</span> 的條件下字詞 $y_{t}$ 的機率，所以 decoder  的定義是在有序的條件下所有預測字詞的聯合機率：</p>

<p><span  class="math">\[
\begin{align}
p(\mathrm{y}) & = \prod_{t=1}^T p(y_t | \{y_1, \cdots, y_{t-1}\}, c) \tag 1 \\
\mathrm{y} & = (y_1, \cdots, y_T)
\end{align}
\]</span></p>

<p>在第 $t$ 個字詞，字詞 $y_t$ 的條件機率：</p>

<p><span  class="math">\[
\begin{align}
p(y_t | \{y_1, \cdots, y_{t-1}\}, c) = g(y_{t-1}, s_t, c) \tag 2
\end{align}
\]</span></p>

<p>當中 $g$ 唯一個 nonlinear function，$s_t$ 為 hidden state，c 為 context vector。</p>

<p>而在 Attention model 中，作者將 decoder 預測下一個字詞的的條件機率重新定義為：</p>

<p><span  class="math">\[
\begin{align}
p(y_i | \{y_1, \cdots, y_{i-1}\}, \mathrm{x}) = g(y_{i-1}, s_t, c_i) \tag 3
\end{align}
\]</span></p>

<p>當中 $s_i$ 表示 RNN 在 $i$ 時間的 hiddent state。</p>

<p><span  class="math">\[
\begin{align}
s_i = f\left(s_{i-1}, y_{i-1}, c_i\right) \tag 4
\end{align}
\]</span></p>

<p>將式子 (3) 與 (2) 相比就可以發現，每一個預測字詞 $y_i$ 對於 context vector 的取得，由原本都是固定的 C  轉變成 每個字詞預測都會取得不同的 $C_i$。</p>

<p>Bahdanau Attention model 的架構如圖一：</p>

<figure class="image">
<center>
  <img src="./attention_bahdanau.png" style="zoom:60%" />
  <figcaption>
  圖一(Image credit:[Paper 1.])
  </figcaption>
</center>
</figure>

<p>Context vector $c_i$ 是取決於 sequence of annotations <span  class="math">\((h_1, h_2, \cdots, h_{T_x})\)</span> 的訊息，annotation $h_i$ 包含了在第 $i$ 步下， input sequence 輸入到 econder 的訊息。計算方法是透過序列權重加總 annotation $h_i$，公式如下：</p>

<p><span  class="math">\[
\begin{equation}
c_i = \displaystyle\sum_{j=1}^{T_x}\alpha_{ij}h_j \tag5
\end{equation}
\]</span></p>

<p>其中 $i$ 表示 decoder 在第 $i$ 個字詞，$j$ 表示 encoder 中第 $j$ 個詞。</p>

<p>$\alpha_{ij} $ 則稱之為 attention distribution，可以用來衡量 input sequence 中的每個文字對 output sequence 中的每個文字所帶來重要性的程度，計算方式如下
：
<span  class="math">\(
\begin{align}
\alpha_{ij} & = softmax(e_{ij}) \\
& = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} \tag6 \\
\end{align}
\)</span></p>

<p><span  class="math">\[
e_{ij} = a(s_{i-1}, h_j) \tag7
\]</span></p>

<p><strong>計算 attention  score $e_{ij}$ 中 $a$ 表示為 alignment model (對齊模型)，是衡量 input sequence 在位置 $j$ 與 output sequence 位置 $i$ 這兩者之間的關係</strong>。
這邊作者為了解決在計算上需要 $T_{x} \times T_{y}$ 的計算量，所以採用了 singlelayer multilayer perceptron 的方式來減少計算量，其計算公式：</p>

<p><span  class="math">\[
\begin{align}
a(s_{i-1}, h_j) = v_a^Ttanh(W_aS_{i-1} + U_ah_j) \tag8
\end{align}
\]</span></p>

<p>其中 $W_a \in R^{n\times n}，U_a \in R^{n \times 2n}，v_a \in R^n$ 都是 weight。</p>

<p>另外作者在此採用了 BiRNN(Bi-directional RNN) 的 forward 與 backward 的架構，由圖一可以得知</p>

<ul>
<li>Forward hidden state 為 <span  class="math">\((\overrightarrow{h_1}, \cdots, \overrightarrow{h_{T_x}})\)</span></li>
<li>Backward hidden state 為 <span  class="math">\((\overleftarrow{h_1}, \cdots, \overleftarrow{h_{T_x}})\)</span></li>
<li>Concatenate forward 與 backward 的 hidden state，所以 annotation $h_j$ 為 <span  class="math">\(\left[\overrightarrow{h_j^T};\overleftarrow{h_j^T}\right]^T\)</span></li>
</ul>

<p>這樣的方式更能理解句子所要表達的意思，並得到更好的預測結果。</p>

<blockquote>
<p>例如以下兩個句子的比較：</p>

<ol>
<li>我喜歡蘋果，因為它很好吃。</li>
<li>我喜歡蘋果，因為它很潮。</li>
</ol>
</blockquote>

<p>下圖為 Bahdanau Attention model 的解析可以與圖一對照理解，這樣更能了解圖一的結構：</p>

<blockquote>
<p>需要注意的一點是在最一開始的 decoder hidden state $S_0$ 是採用 encoder 最後一層的 output</p>
</blockquote>

<figure class="image">
<center>
  <img src="./attention_bahdanau_example.png" style="zoom:100%" />
  <figcaption>
  圖二
  </figcaption>
</center>
</figure>

<p>下圖為論文中英文翻譯成法語的 attention distribution：</p>

<figure class="image">
<center>
  <img src="./attention_bahdanau_output.png" style="zoom:90%" />
  <figcaption>
  圖三(Image credit:[Paper 1.])
  </figcaption>
</center>
</figure>

<p>在上圖中 $[European \space Economic \space Area]$ 翻譯成$ [zone \space \acute{a}conomique \space europ\acute{e}enne] $ 的注意力分數上，模型成功地專注在對應的字詞上。</p>

<h1 id="attention-mechanism-family">Attention Mechanism Family</h1>

<p>Attention score function:</p>

<table>
<thead>
<tr>
<th>Name</th>
<th>Attention score function</th>
</tr>
</thead>

<tbody>
<tr>
<td>Dot-product</td>
<td>$e_{ij} = S_i^Th_j$</td>
</tr>

<tr>
<td>General</td>
<td>$e_{ij} = S_i^TWh_j$</td>
</tr>

<tr>
<td>Additive</td>
<td>$e_{ij} = v^Ttanh(WS_{i-1} + Uh_j) $</td>
</tr>

<tr>
<td>Scaled Dot-product</td>
<td>$e_{ij} = \frac{S_i^Th_j}{\sqrt{d}}$</td>
</tr>

<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<h3 id="soft-attention--hard-attention">Soft Attention &amp; Hard Attention</h3>

<h3 id="global-attention--local-attention">Global Attention &amp; Local Attention</h3>

<p>attenion value and query 的理解不要被公式混淆，而是從 attention 的概念去了解，query 就是</p>

<h2 id="refenece">Refenece</h2>

<p>Paper:</p>

<ol>
<li><a href="https://arxiv.org/pdf/1409.0473.pdf">Dzmitry Bahdanau, KyungHyun Cho Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2015)</a></li>
<li><a href="https://arxiv.org/pdf/1502.03044.pdf">Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015)</a></li>
<li><a href="https://arxiv.org/pdf/1508.04025.pdf">Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)</a></li>
<li><a href="https://arxiv.org/abs/1904.02874">Sneha Chaudhari, Gungor Polatkan , Rohan Ramanath , Varun Mithal, An Attentive Survey of Attention Models(2019)</a></li>
</ol>

<p>Illustrate:</p>

<ol>
<li><a href="https://zhuanlan.zhihu.com/p/37601161h">https://zhuanlan.zhihu.com/p/37601161h</a></li>
<li><a href="https://blog.floydhub.com/attention-mechanism/#bahdanau-atth">https://blog.floydhub.com/attention-mechanism/#bahdanau-atth</a></li>
<li><a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf">https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf</a></li>
</ol>

<p>Tutorial:</p>

<ol>
<li><a href="https://github.com/tensorflow/nmt#background-on-the-attention-mechanism">Neural Machine Translation (seq2seq) Tutorial</a></li>
<li><a href="https://www.tensorflow.org/tutorials/text/transformerG">https://www.tensorflow.org/tutorials/text/transformerG</a></li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Guide annotating the paper with PyTorch implementation</a></li>
</ol>

<p>Visualization:</p>

<ol>
<li><a href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</a></li>
</ol>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/attention/">Attention</a>
  
  <a class="badge badge-light" href="/tags/attention-model/">Attention model</a>
  
  <a class="badge badge-light" href="/tags/attention-mechanism/">Attention Mechanism</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
  </ul>
</div>












  
  
    
  
  






  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/roymond-liao/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>




<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "roymondliao" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>






  
  



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.3.1/mermaid.min.js" integrity="sha256-vOIuDSYDirTfyr+S2MjFnhOz6Rgiz4ODFAHATG0rFxw=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/markdown.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://roymondliao.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d6bd04fdad2ad213aa8111c5a3b72fc5.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2019 - &copy; 2020 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
