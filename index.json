[{"authors":["admin"],"categories":null,"content":"Yuyu are being a Data Engineer at Gogolook Inc. He has four years of experience in the field of data science. The main languages use Python and R. Also, has the passion for data analysis, machine learning and deep learning and through the process of understanding theory and practice with relevant research is used to solve various problems in products.\nIn addition, willing to learning the various tools in data engineering and system architecture, towards the field of data science to continue to learn and expand the knowledge.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://roymondliao.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Yuyu are being a Data Engineer at Gogolook Inc. He has four years of experience in the field of data science. The main languages use Python and R. Also, has the passion for data analysis, machine learning and deep learning and through the process of understanding theory and practice with relevant research is used to solve various problems in products.\nIn addition, willing to learning the various tools in data engineering and system architecture, towards the field of data science to continue to learn and expand the knowledge.","tags":null,"title":"Roymond Liao","type":"authors"},{"authors":["Roymond Liao"],"categories":["NLP","Deep Learning"],"content":"Attention Mechanism Attention 的概念在 2014 年被 Bahdanau et al. [Paper 1] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。\n 用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。\n 複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 \\({y_1, \\cdots, y_{t-1}}\\) 的條件下字詞 $y_{t}$ 的機率，所以 decoder 的定義是在有序的條件下所有預測字詞的聯合機率：\n\\[ \\begin{align} p(\\mathrm{y}) \u0026 = \\prod_{t=1}^T p(y_t | \\{y_1, \\cdots, y_{t-1}\\}, c) \\tag 1 \\\\ \\mathrm{y} \u0026 = (y_1, \\cdots, y_T) \\end{align} \\]\n在第 $t$ 時間，字詞 $y_t$ 的條件機率：\n\\[ \\begin{align} p(y_t | \\{y_1, \\cdots, y_{t-1}\\}, c) = g(y_{t-1}, s_t, c) \\tag 2 \\end{align} \\]\n當中 $g$ 唯一個 nonlinear function，可以為多層的架構，$s_t$ 為 hidden state，c 為 context vector。\n而在 Attention model 中，作者將 decoder 預測下一個字詞的的條件機率重新定義為：\n\\[ \\begin{align} p(y_i | \\{y_1, \\cdots, y_{i-1}\\}, \\mathrm{x}) = g(y_{i-1}, s_t, c_i) \\tag 3 \\end{align} \\]\n當中 $s_i$ 表示 RNN 在 $i$ 時間的 hiddent state。\n\\[ \\begin{align} s_i = f\\left(s_{i-1}, y_{i-1}, c_i\\right) \\tag 4 \\end{align} \\]\n將式子 (3) 與 (2) 相比就可以發現，每一個預測字詞 $y_i$ 對於 context vector 的取得，由原本都是固定的 C 轉變成 每個字詞預測都會取得不同的 $C_i$。\nBahdanau Attention model 的架構如圖1：\n  圖1 (Image credit:[Paper 1])    Context vector $c_i$ 是取決於 sequence of annotations \\((h_1, h_2, \\cdots, h_{T_x})\\) 的訊息，annotation $h_i$ 包含了在第 $i$ 步下， input sequence 輸入到 econder 的訊息。計算方法是透過序列權重加總 annotation $h_i$，公式如下：\n\\[ \\begin{equation} c_i = \\displaystyle\\sum_{j=1}^{T_x}\\alpha_{ij}h_j \\tag5 \\end{equation} \\]\n其中 $i$ 表示 decoder 在第 $i$ 個字詞，$j$ 表示 encoder 中第 $j$ 個詞。\n$\\alpha_{ij} $ 則稱之為 attention distribution，可以用來衡量 input sequence 中的每個文字對 output sequence 中的每個文字所帶來重要性的程度，計算方式如下 ： \\( \\begin{align} \\alpha_{ij} \u0026 = softmax(e_{ij}) \\\\ \u0026 = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})} \\tag6 \\\\ \\end{align} \\)\n\\[ e_{ij} = a(s_{i-1}, h_j) \\tag7 \\]\n計算 attention score $e_{ij}$ 中 $a$ 表示為 alignment model (對齊模型)，是衡量 input sequence 在位置 $j$ 與 output sequence 位置 $i$ 這兩者之間的關係。 這邊作者為了解決在計算上需要 $T_{x} \\times T_{y}$ 的計算量，所以採用了 singlelayer multilayer perceptron 的方式來減少計算量，其計算公式： \\( \\begin{align} a(s_{i-1}, h_j) = v_a^Ttanh(W_aS_{i-1} + U_ah_j) \\tag8 \\end{align} \\)\n其中 $W_a \\in R^{n\\times n}，U_a \\in R^{n \\times 2n}，v_a \\in R^n$ 都是 weight。\n另外作者在此採用了 BiRNN(Bi-directional RNN) 的 forward 與 backward 的架構，由圖一可以得知\n Forward hidden state 為 \\((\\overrightarrow{h_1}, \\cdots, \\overrightarrow{h_{T_x}})\\) Backward hidden state 為 \\((\\overleftarrow{h_1}, \\cdots, \\overleftarrow{h_{T_x}})\\) Concatenate forward 與 backward 的 hidden state，所以 annotation $h_j$ 為 \\(\\left[\\overrightarrow{h_j^T};\\overleftarrow{h_j^T}\\right]^T\\)  這樣的方式更能理解句子所要表達的意思，並得到更好的預測結果。\n 例如以下兩個句子的比較：\n 我喜歡蘋果，因為它很好吃。 我喜歡蘋果，因為它很潮。   下圖為 Bahdanau Attention model 的解析可以與圖1對照理解，這樣更能了解圖一的結構：\n 需要注意的一點是在最一開始的 decoder hidden state $S_0$ 是採用 encoder 最後一層的 output\n   圖2    下圖為論文中英文翻譯成法語的 attention distribution：\n  圖3 (Image credit:[Paper 1])    在上圖中 $[European \\space Economic \\space Area]$ 翻譯成$ [zone \\space \\acute{a}conomique \\space europ\\acute{e}enne] $ 的注意力分數上，模型成功地專注在對應的字詞上。\n最後作者後續還有實驗了採用 LSTM 來替換 Vainlla RNN 進行實驗，詳細的公式都有列出在論文中，有興趣的可以看一下。\nAttention Mechanism Family Hard Attention \u0026amp; Soft Attention Xu et al. [Paper 2] 對於圖像標題(caption)的生成研究中提出了 hard attention 與 soft attention 的方法，作者希望透過 attention mechanism 的方法能夠讓 caption 的生成從圖像中獲得更多有幫助的訊息。下圖為作者所提出的模型架構：\n  圖4 (Image credit:[Paper 2])    模型結構\n Encoder  在 encoder 端模型使用 CNN 來提取 low-level 的卷積層特徵，每一個特徵都對應圖像的一個區域\n\\[ a = \\{a_1, \\dots, a_L\\}, a_i \\in R^D \\]\n總共有 $L$ 個特徵，特徵向量維度為 $D$。\n Decoder  採用 LSTM 模型來生成字詞，而因應圖片的內容不同，所以標題的長度是不相同的，作者將標題 $y $ encoded 成一個 one-hot encoding 的方式來表示\n\\[ y = \\{y_1, \\dots, y_C\\}, y_i \\in R^K \\]\nK 為字詞的數量，C 為標題的長度。下圖為作者這本篇論文所採用的 LSTM 架構：\n  圖5 (Image credit:[Paper 2])   \n利用 affine transformation 的方式 \\(T_{s, t} : R^s \\rightarrow R^t\\) 來表達 LSTM 的公式：\n\\[ \\begin{pmatrix} i_t \\\\ f_t \\\\ o_t \\\\ g_t \\end{pmatrix} = \\begin{pmatrix} \\sigma \\\\ \\sigma \\\\ \\sigma \\\\ tanh \\end{pmatrix} T_{D+m+n, n} \\begin{pmatrix} Ey_{t-1} \\\\ h_{t-1} \\\\ \\hat{Z_t} \\end{pmatrix} \\tag1 \\\\ \\]\n\\[ \\begin{align} c_t \u0026 = f_t \\odot c_{t-1} + i_t \\odot g_t \\tag2 \\\\ h_t \u0026 = o_t \\odot tanh(c_t) \\tag3 \\end{align} \\]\n其中\n \\(i_t\\) : input gate \\(f_t\\) : forget gate \\(o_t\\) : ouput gate \\(g_t\\) : canaidate cell \\(c_t\\) : memory cell \\(h_t\\) : hidden state \\(Ey_{t-1}\\) 是詞 \\(y_{t-1}\\) 的 embedding vector，\\(E \\in R^{m \\times k}\\) 為 embedding matrix，m 為 embedding dimention \\(\\hat{Z} \\in R^D\\) 是 context vector，代表捕捉特定區域視覺訊息的上下文向量，與時間 $t$ 有關，所以是一個動態變化的量   特別注意的是作者在給定 memory state 與 hidden state 的初始值的計算方式使用了兩個獨立的多層感知器(MLP)，其輸入是各個圖像區域特徵的平均，計算公式如下：\n\\[ \\begin{align} c_0 = f_{init, c}( \\frac{1}{L} \\sum_{i}^L a_i) \\\\ h_0 = f_{init, h}( \\frac{1}{L} \\sum_{i}^L a_i) \\end{align} \\]\n以及作者為了計算在 $t$ 時間下所關注的 context vector \\(\\hat{Z_t}\\) 定義了 attention machansim $\\phi$ 為在 $t$ 時間，對於每個區域 $i$ 計算出一個權重 \\(\\alpha_{ti}\\) 來表示產生字詞 $y_t$ 需要關注哪個圖像區域 annotation vectors $a_i, i=1, \\dots, L$ 的訊息。\n權重 \\(\\alpha_i\\) 的產生是透過輸入 annotation vector \\(a_i\\) 與前一個時間的 hidden state $h_{t-1}$ 經由 attention model $f_{att}$ 計算所產生。\n\\[ \\begin{align} e_{ti} = f_{att}(a_i, h_{t-1}) \\tag4 \\\\ \\alpha_{ti} = \\frac{exp(e_{ti})}{\\sum_{k=1}^{L}exp{e_{tk}}} \\tag5 \\\\ \\hat{Z_t} = \\phi(\\{a_i\\}, \\{\\alpha_{i}\\}) \\tag6 \\end{align} \\]\n有了上述的資訊，在生成下一個 $t$ 時間的字詞機率可以定義為：\n\\[ p(y_t | a, y_1, y_2, \\dots, y_{t-1}) \\propto exp(L_o(Ey_{t-1} + L_hh_t + L_z\\hat{Z_t})) \\tag7 \\]\n其中 \\(L_o \\in R^{K \\times m}, L_h \\in R^{m \\times n}, L_z \\in R^{m \\times D}\\)，m 與 n 分別為 embedding dimension 與 LSTM dimension。\n對於函數 $\\phi$ 作者提出了兩種 attention machansim，對應於將權重附加到圖像區域的兩個不同策略。根據上述的講解，搭配下圖為 Xu et al. [Paper 2] 的模型架構解析，更能了解整篇論文模型的細節：\n  圖6    Hard attention (Stochastic Hard Attention) 在 hard attention 中定義區域變數(location variables) $s_{t, i}$ 為在 t 時間下，模型決定要關注的圖像區域，用 one-hot 的方式來表示，要關注的區域 $i$ 為 1，否則為 0。\n$s_{t, i}$ 被定為一個淺在變數(latent variables)，並且以 multinoulli distriubtion 作為參數 $\\alpha_{t, i}$ 的分佈，而 $\\hat{Z_t}$ 則被視為一個隨機變數，公式如下：\n\\[ p(s_{t, i} = 1 | s_{j, t}, a) = \\alpha_{t, i} \\tag8 \\]\n\\[ \\hat{Z_t} = \\sum_{i} s_{t, i}a_i \\tag9 \\]\n定義新的 objective function $L_s$ 為 marginal log-likelihood $\\text{log }p(y|a)$ 的下界(lower bound)\n\\[ \\begin{align} L_s \u0026 = \\sum_s p(s|a)\\text{log }p(y|s,a) \\\\ \u0026 \\leq \\text{log } \\sum_s p(s|a)p(y|s,a) \\\\ \u0026 = \\text{log }p(y|a) \\end{align} \\]\n在後續的 $L_s$ 推導求解的過程，作者利用了\n Monte Carlo 方法來估計梯度，利用 moving average 的方式來減小梯度的變異數 加入了 multinouilli distriubtion 的 entropy term $H[s]$  透過這兩個方法提升隨機算法的學習，作者在文中也提到，最終的公式其實等價於 Reinforce learing。作者在論文中有列出推導的公式，有興趣的可以直接參考論文。\nSoft attention (Deterministic Soft Attention) Soft attention 所關注的圖像區域並不像 hard attention 在特定時間只關注特定的區域，在 soft attention 中則是每一個區域都關注，只是關注的程度不同。透過對每個圖像區域 $a_{i}$ 與對應的 weight $\\alpha_{t,i}$ ，$\\hat{Z}_t$ 就可以直接對權重做加總求和，從 hard attention 轉換到 soft attention 的 context vector：\n\\[ \\hat{Z_t} = \\sum_{i} s_{t, i}a_i \\implies \\mathbb{E}{p(s_t|a)}[\\hat{Z_t}] = \\sum_{i=1}^L \\alpha_{t,i}a_i \\]\n這計算方式將 weight vector $\\alpha_i$ 參數化，讓公式是可微的，可以透過 backpropagation 做到 end-to-end 的學習。其方法是參考前面所介紹的 Bahdanau attention 而來。\n作者在這邊提出三個理論：\n \\(\\mathbb{E}{p(s_t|a)}[h_t]\\) 等同於透過 context vector \\(\\mathbb{E}{p(s_t|a)}[\\hat{Z_t}]\\) 使用 forward propagation 的方法計算 $h_t$ Normalized weighted geometric mean approximation 根據公式(7)定義 \\(n_t = L_o(Ey_{t-1} + L_hh_t + L_z\\hat{Z_t})\\)  所以 soft attention 在最後做文字的預測時作者定義了 softmax $k^{th}$ 的 normalized weighted geometric mean。\n\\[ \\begin{align} NWGM[p(y_t=k|a)] \u0026 = \\frac{\\prod_i exp(n_{t,k,i})^{p(s_{t,i} = 1 | a)}}{\\sum_j\\prod_i exp(n_{t,j,i})^{p(s_{t,i} = 1 | a)}} \\\\ \u0026 = \\frac{exp\\left(\\mathbb{E_{p(s_t) | a}[n_{t,k}]}\\right)}{\\sum_j exp\\left(\\mathbb{E_{p(s_t) | a}[n_{t,j}]}\\right)} \\end{align} \\]\n\\[ \\mathbb{E}[n_t] = L_o(Ey_{t-1} + L_h\\mathbb{E}[h_t] + L_z\\mathbb{E}[\\hat{Z_t}]) \\]\n這邊的部分就是 soft attention 與 Bahdanau attention 的主要差異，在 Bahdanau attention 最後的 output 是透過 softmax 來取得下一次詞的機率，而作者在這邊採用了 NWGM 的方式。這邊並不是很清楚作者怎麼來證明這樣的論述，日後有理解出來或是有找到相關的參考再補上來。\n最後來看看 soft attention 與 hard attention 的圖像視覺化結果，下圖是兩種 attention 對於圖像區域注意程度，可以看得出 hard attention 都會專注在很小的區域，而 soft attention 的注意力相對發散，這也是因為 soft 與 hard 在關注圖像區域上的一個是注意全部的圖像區域，一個是注意特定的區域。\n  圖7 (Image credit:[Paper 2])    Global Attention \u0026amp; Local Attention Loung et al. [Paper 3] 在 2015 年所發表了 global / local attention 來提升 NMT 任務上的準確度，global attention 類似於 soft attention，而 local attention 則介於 hard 與 soft attention 的混合。\nGlobal / local attention 相同之處：\n 採用的 target hidden state $h_t$ 是 stacking LSTM 最後一層的 output Context vector $c_t$ 都是將 $h_t$ 與 source-side $\\bar{h_s}$ 作為 input 計算 結合 $c_t$ 與 $h_t$ 的訊息計算 \\(\\tilde{h_t} = tanh\\left(W_c[c_t;h_t]\\right)\\)，稱為 attentional vector 預測 $t$ 時間下的生成字詞的機率 \\( p(y_t|y_{\\text{  Global / local attention 不同之處：\n Context vector $c_t$ 的計算方式不同 採用 source side $\\bar{h_s}$ 的數量不同  接下來分別介紹 global attention 與 local attention 當中的細節。\nGlobal attention 作者將 alignment vector $a_t$ 定義是一個可變長度向量，所以在 global attention 中 $a_t$ 將全部時間的 source side 的資訊當作 input ，公式如下： \\( \\begin{align} a_t(s) \u0026= align(h_t, \\bar{h_s}) \\tag 1 \\\\ \u0026= \\frac{exp(score(h_t,\\bar{h_s}))}{\\sum_{s'}exp\\left(score(h_t,\\bar{h_{s'}})\\right)} \\end{align} \\) 在 score function 的部分，這邊定義了 content-based function，可以有以下三種形式：\n\\[ score(h_t, \\bar{h_s}) = \\begin{cases} h_{t}^T\\bar{h_s} \u0026 \\text{dot} \\\\ h_{t}^TW_a\\bar{h_s} \u0026 \\text{general} \\\\ v_a^Ttanh\\left(W_a[h_t;\\bar{h_s}]\\right) \u0026 \\text{concat} \\end{cases} \\]\n其中 $W_a, v_a$ 都是透過訓練所得到的參數。\n另外還有一種 local-based function，score只單存參考 target hidden state $h_t$ 的結果\n\\[ \\begin{align} a_t = softmax(W_ah_t) \u0026\u0026 \\text{location} \\end{align} \\]\n下圖為 global attention 的模型架構：\n  圖8 Global Attention (Image credit:[Paper 3])    Global attention 與 Bahdanau attention 對比，差異的地方如下：\n 在 encoder 與 decoder 都採用最後一層的 LSTM output 作為 hidden state 計算順序為 \\(h_t \\rightarrow a_t \\rightarrow c_t \\rightarrow \\tilde{h_t}\\)，而 bahdanau attention 是 \\(h_{t-1} \\rightarrow a_t \\rightarrow c_t \\rightarrow h_t\\)  Local attention 相較於 global attention 採用的所有 soucre side 的字詞，在計算上可能較為龐大，而且面對較長的句子可能無法翻譯正確的狀況，提出了 local attention，只專注關心一小部分的字詞來替代關注全部的字詞。\n前面提到 local attention 是 hard 與 soft attention 的混合，選擇性地關注一個數量較小的上下文窗口，並且是可以微分的，減少了 soft attention 的計算量以及避免了 hard attention 不可微分的問題，更容易的訓練。\n注意的重點：\n 在每個 $t$ 時間下生成 aligned position $p_t$ Context vector $c_t$ 則是計算 $[p_t - D, p_t + D]$ 之間的 source hidden 做加權平均，$D$ 是可調的參數。如果所選擇的範圍超過句子本身的長度，則忽略掉多出來的部分，只考慮有存在的部分。 Alignment vector $a_t \\in R^{2D+1}$ 是固定的維度  作者針對模型提出了兩遍變形：\n Monotonic aligment (local-m)\n 假設 source 與 target sequences 是單調對齊，就是指 source 與 target 長度相同： \\( p_t = t \\) Alignment vector $a_t$ 的計算就跟公式(1)一致。\n Predictive alignment (local-p)\n 認為 source 與 target sequences 是並非單調對齊，就是長度不相同： \\( p_t = S \\cdot sigmoid\\left( v_p^Ttanh(W_ph_t)\\right) \\) $W_p, v_p$ 都是可訓練的模型參數，$S$ 則表示 source sequence 的長度，$p_t \\in [0, S]$。\n  Alignment vector $a_t$ 的計算採用的 Gaussian distribution 來賦予 alignment 的權重：\n\\[ a_t(s) = align(h_t,\\bar{h_s})exp\\left(-\\frac{(s-p_t)^2}{2\\sigma^2}\\right) \\]\n$align(h_t,\\bar{h_s})$ 與公式(1)一致，標準差設定為 $\\sigma = \\frac{D}{2}$，這是透過實驗所得來。\n下圖為 local attention 的模型架構：\n  圖9 Local Attention (Image credit:[Paper 3])    Local attention 可能的會遇到的問題：\n 當 encoder 的 input sequence 長度不長時，計算量並不會減少 當 Aligned position $p_t$ 不準確時，會直接影響到 local attention 的準確度  Input-feeding Approach 作者認為在 global 與 local attention 的方法中，模型的注意力機制是獨立的，但是在整個翻譯的過程中，必須要去了解哪些資訊已經被翻譯了，所以在預測下一個翻譯字詞時，應該結合過去 attentional vectors $\\tilde{h_t}$ 的資訊，也就是說在 deocder 這邊多考慮了 alignment model 的結果，如下圖所示：\n  圖10 Image credit:[Paper 3])    來看看論文中的針對各個模型在 WMT'14 英文翻譯成德文資料集的訓練結果：\n 這邊的實驗限制了字詞的數量，只取在資料集中最常出現的 50k 字詞當作 corpos 如果出現字詞沒有在 corpos 中，則用 \u0026lt;unk\u0026gt; 來取代    圖11 (Image credit:[Paper 3])    Global attention 與 local attention 都有自己的優勢，如果說要選用哪個方式來當作模型，認為因應不同的任務可能表現都會有所差異，所以建議兩種都實驗看看結果來比較優劣，而在實際上大多數採用的都是以 Global attention 為主。\n底下列出上面各篇論文所提到的 Attention score function：\n   Name Attention score function     Dot-product $score(s_t, h_i) = S_t^Th_i$   General $score(s_t, h_i) = S_t^TW_ah_i$   Additive $score(s_t, hi) = v^Ttanh(WS{t-1} + Uh_i) $   Scaled Dot-product $score(s_t, h_i) = \\frac{S_t^Th_i}{\\sqrt{d}}$   Loocation $a_{t} = softmax(W_aS_t)$    總結來說：\n Attention 的目的就是要實現的就是在 decoder 的不同時刻可以關注不同的圖像區域或是句子中的文字，進而可以生成更合理的詞或是結果。\n Refenece Paper:\n Dzmitry Bahdanau, KyungHyun Cho Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015) Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015) Sneha Chaudhari, Gungor Polatkan , Rohan Ramanath , Varun Mithal, An Attentive Survey of Attention Models(2019)  Illustrate:\n https://zhuanlan.zhihu.com/p/37601161h https://zhuanlan.zhihu.com/p/31547842 https://blog.floydhub.com/attention-mechanism/#bahdanau-atth https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf https://www.cnblogs.com/Determined22/p/6914926.html https://jhui.github.io/2017/03/15/Soft-and-hard-attention/ http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf https://www.jiqizhixin.com/articles/2018-06-11-16 https://medium.com/@joealato/attention-in-nlp-734c6fa9d983 https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3 https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms https://zhuanlan.zhihu.com/p/80692530  Tutorial:\n Neural Machine Translation (seq2seq) Tutorial https://www.tensorflow.org/tutorials/text/transformerG Guide annotating the paper with PyTorch implementation  Visualization:\n https://github.com/jessevig/bertviz  ","date":1582848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582848000,"objectID":"5f09814756a94978ed15496963d0ac80","permalink":"https://roymondliao.github.io/post/2019-12-16_transformer_part2/","publishdate":"2020-02-28T00:00:00Z","relpermalink":"/post/2019-12-16_transformer_part2/","section":"post","summary":"Attention Mechanism Attention 的概念在 2014 年被 Bahdanau et al. [Paper 1] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。\n 用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。\n 複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 \\({y_1, \\cdots, y_{t-1}}\\) 的條件下字詞 $y_{t}$ 的機率，所以 decoder 的定義是在有序的條件下所有預測字詞的聯合機率：\n\\[ \\begin{align} p(\\mathrm{y}) \u0026 = \\prod_{t=1}^T p(y_t | \\{y_1, \\cdots, y_{t-1}\\}, c) \\tag 1 \\\\ \\mathrm{y} \u0026 = (y_1, \\cdots, y_T) \\end{align} \\]","tags":["Attention","Attention Model","Attention Mechanism","Soft Attention","Hard Attention","Global Attention","Local Attention"],"title":"Transformer Part 2 - Attention","type":"post"},{"authors":["Roymond Liao"],"categories":["NLP","Deep Learning"],"content":"一開始接觸 Attention Is All You Need 這篇論文是從 Kaggle Reading Group 這個 channel 開始，非常推薦可以跟著一起讀!!\n主持人 Rachael Atman 本身是 Kaggle 的 Data Scientist，她的導讀我覺得是流暢的，但她自己本身有說過並非是 NLP 領域的專家，所以在 kaggle reading group 裡閱讀的論文也有可能是她完全沒接觸過的，整個 channel 帶給你的就是一個啟發，讓你覺得有人跟你一起閱讀的感覺，然後過程中也些人會在 channel 的 chat room 提出一些看法或是連結，Rachael 本身也會提出自己的見解，可以多方面參考。\n在跟完整個 Attention Is All You Need 的影片後，還是有太多細節是不清楚的，因為自己本身也不是這個領域的，所以開始追論文中所提到的一些關鍵名詞，就開始從 $seq2seq \\rightarrow attention \\rightarrow self-attention$。這中間 有太多知識需要記錄下來，所以將論文的內容分成三部曲，來記錄閱讀下來的點點滴滴:\n Part 1: Sequence to sequence model 起源 Part 2: Attention 與 Self-attention 的理解 Part 3: Transformer 的架構探討與深入理解  要談論 Attention Is All You Need 這篇 paper 就必須從 seq2seq 講起，seq2seq 全名為 Sequence to Sequence[1]，是一個 Encoder - Decoder 架構的模型，在 2014 年被提出，被廣泛的應用於 Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.\nSequence to sequence model Introduction 簡單來說，期望輸入一串序列(source)，輸出一串序列(target)，而這個 source 與 target 可以是什麼呢？\n 如果以 machine translation 來說，任務是中翻英，輸入是一句中文，而輸出則會是一句英文。\n 如果是 text summarization，輸入則會是一段文章，而輸出則會是一段摘要\n  這就是 seq2seq model 所要解決的問題，在輸入一些資訊後，經過 encoder-decoder 的訓練，可以得到相對應的回答或是其他資訊。\nModel 模型的架構也非常簡單，就如下圖所示：\n 圖一   假設今天我們執行中翻英的任務，輸入(source: X)是一句中文，可以是 \u0026quot;我愛機器學習\u0026quot;，而輸出(target: Y)則會是 \u0026quot;I love machine learning\u0026quot;，所以整個訓練的步驟如下：\n Input sequence $(x_1, x_2, x_3, \\dots, x_s)$ 經過 embedding layer 的轉換，得到每個 word 的 embedding vector\n Encoder 把所有輸入序列 embedding vector 消化後，將資訊壓縮轉換為一個向量 $C$，稱之為 context vector\n  \\[ \\begin{align} h_s^{e} \u0026 = f_{enc}\\left(h_{s-1}^{e}, e_{x_{s-1}}, W_{enc}\\right) \\\\ C \u0026 = h_s^e \\text {，最後一步的 hidden state} \\\\ C \u0026 = q(h_s^e) \\text {，最後一步的 hidden state 做 transform } \\\\ C \u0026 = q\\left(h_1^e, h_2^e, \\dots, h_s^e\\right) \\text {，每一步的 hidden state 做 transform } \\end{align} \\]\n其中 $f_{enc}(\\cdot)$ 表示 Encoder 中的 RNN function，參數為 $W_{enc}$。$e_{x_s}$ 表示 $x_s$ 的 embedding vector，$h_s^e$ 表示在時間 $s$ 的 hidden state，$C$ 可以表示為 Encoder 最後的 hidden state 或是經過函數 $q(\\cdot)$ 的轉換。\nDecoder 則根據 context vector 的資訊來生成文字，output sequence $(y_1, y_2, y_3, \\dots, y_t)$  \\[ \\begin{align} h_0^{d} \u0026 = C \\\\ h_{t}^{d} \u0026 = f_{dec}\\left(h_{t-1}^{d}, e_{y_{t-1}}, W_{dec}\\right) \\\\ O_{t} \u0026 = g\\left(h_t^d\\right) \\end{align} \\]\n其中 $h_{0}^{d}$ 為 context vector 傳進來當作 Decoder 的初始 hidden state，$f_{dec}(\\cdot)$ 表示 Decoder 中的 RNN function，參數為 $W_{dec}$。$h_{t}^{d}$ 表示 Decoder在時間 $t$ 的 hidden state，$e_{y_{t}}$ 表示前一步的所得到的 $y_{t-1}$ 結果當作輸入，$y_0$ 都是以特殊索引 \u0026lt;BOS\u0026gt; 當作輸入。 $g(\\cdot)$ 為 output layer，一般都是 softmax function。\n過程就只是簡單的三個步驟，雖然看起來簡單，但當中有些細節是需要注意的。\nTraining  特殊索引\n在每個句子做 one-hot-encoder 的轉換時，會在句子的前後加上 \u0026lt;BOS\u0026gt; 與 \u0026lt;EOS\u0026gt;\n BOS: Begin of sequence，在預測的時候我們並沒有對應的答案，所以會先以 \u0026lt;BOS\u0026gt; 當作 $Y_0$ 的 target input EOS: End of sequence，用意是要告訴 model 當出現這個詞的時候就是停止生成文字，如果沒有這個詞，模型會無限迴圈的一直生成下去  除了上述的 \u0026lt;BOS\u0026gt; 與 \u0026lt;EOS\u0026gt; 外，還有 \u0026lt;PAD\u0026gt; 與 \u0026lt;UNK\u0026gt;\n PAD: 由於 RNN 的 parameters 是共享的，所以在 input 的維度就需要保持相同，但並不是每個句子的長度都是同樣的，有的可能長度是 3 ，有的長度可能是 5，所以為了處理不同 input sequence 長度不同的狀況，增加了 \u0026lt;PAD\u0026gt; 的字詞，來讓每次 batch 的 input sequence 的長度都是相同的 UNK: 如果輸入的字詞在 corpus 是沒有出現過的，就會用 \u0026lt;UNK\u0026gt; 索引來代替  Encoder layer 與 Decoder layer 的選擇\nEncoder 與 Decoder 中的 RNN function 可以是 simple RNN / LSTM / GRU，又或者是一個 bidirectional LSTM 的架構在裡面，也可以是一個 multi layer LSTM\n Teacher forcing\n在 training model 時 ，為了提高 model 的準確度與訓練速度，採用了 Teacher forcing training 方法，圖二就是 teacher foring 的概念，在 training 的時候直接告訴 model 實際的答案，省去 model 自己去尋找到正確的答案。另外也有提出 Professor Forcing 的做法，尚未理解這方法的概念，提供當作參考。\n    圖二 (Image credit: LINK)    Prediction  Beam search\n在 prediction model 時，每一步的 output 都是要計算出在 corpus 中生成最可能的那個字詞\n\\[ \\begin{align} \\hat{y_t} = argmax\\space p_{\\theta}(y | \\hat{y}_{1:(t-1)}) \\end{align} \\]\n其中 \\(\\hat{y}_{1:(t-1)} = \\hat{y}_1,\\dots,\\hat{y}_{t-1}\\) 為前面 $t-1$ 步所生成的字詞。以一個簡單的概念來思考，每一步的 output 都是該步所得到的最大條件機率，那這樣的 greedy search 所得到的結果對於我們的目標並非是最優的，得到的是每個字詞的最大條件機率，而並非是整個句子的最大條件機率，所以這樣的狀況下你所得的的翻譯可能不會是最適合的。\nBeam search 就是為了解決這樣的問題而提出的，在每一步的生成過程中，生成 $B$ 的最可能的文字序列作為約束，其中 $B$ 的大小為 beam width，是一個 hyperparamter。$B$ 值越大可以得到更好的結果，但相對的計算量也增加。\n過程就如同圖三所示:\n 建立一個 search tree，該 root 為一個開始符號 (非序列的第一個詞) 由序列的左到右開始，順序生成目標語言序列，同時成長對應的搜尋樹 在生成序列的每一步，對 search tree 的每個 leaf node，選取 $B$ 個擁有最高條件機率的生成單詞，並生成 B 個子節點。 在成長搜尋樹後，進行剪枝的工作，只留下 B 個最高條件機率的葉節點後，再進行下一個位置的序列生成。   Beam search 的實作可以參考 Blog:[4]。\n   圖三 (Image credit: LINK)    另外要注意在使用 beam search 所謂遇到的問題，在 Andrew Ng 大師的課程中提到\n 消除長度對計算機率影響（Length Normalization） 如何選擇 Beam Width 參數（The Choice of Beam Width）  詳細的解說可以參考課程或是 Blog:[3]。\nProblems  所有資訊只壓縮成一個 context vector，input sequence 的資訊很難全部保存在一個固定維度的向量裡 當 sequence 的長度很長時，在 decoder 解碼時，由於 Recurrent neural network 的依賴問題，容易丟失 input sequence 的訊息   在理解完 Seq2Seq model 後，所遇到的問題該如何解決? 那就是要靠 Attention Is All You Need 這篇 paper 的主要重點之一 Attention mechanism(注意力機制)， 這部分將會在 part 2 的時候介紹。\nReference Paper:\n[1] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, Sequence to Sequence Learning with Neural Networks(2015)\n[2] Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio, Professor Forcing: A New Algorithm for Training Recurrent Networks(2016)\n[3] Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2014)\n[4] Minh-Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)\nBlog:\n[1] https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n[2] https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdfh\n[3] https://ithelp.ithome.com.tw/articles/10208587\n[4] https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n[5] Seq2seq pay Attention to Self Attention: Part 1\n[6] Seq2seq pay Attention to Self Attention: Part 2\n[7] Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\n[8] http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/\n[9] https://www.zhihu.com/question/54356960\n","date":1576454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577923200,"objectID":"028c88d04cf72883aaf14ea9a1fc7710","permalink":"https://roymondliao.github.io/post/2019-12-16_transformer_part1/","publishdate":"2019-12-16T00:00:00Z","relpermalink":"/post/2019-12-16_transformer_part1/","section":"post","summary":"一開始接觸 Attention Is All You Need 這篇論文是從 Kaggle Reading Group 這個 channel 開始，非常推薦可以跟著一起讀!!\n主持人 Rachael Atman 本身是 Kaggle 的 Data Scientist，她的導讀我覺得是流暢的，但她自己本身有說過並非是 NLP 領域的專家，所以在 kaggle reading group 裡閱讀的論文也有可能是她完全沒接觸過的，整個 channel 帶給你的就是一個啟發，讓你覺得有人跟你一起閱讀的感覺，然後過程中也些人會在 channel 的 chat room 提出一些看法或是連結，Rachael 本身也會提出自己的見解，可以多方面參考。\n在跟完整個 Attention Is All You Need 的影片後，還是有太多細節是不清楚的，因為自己本身也不是這個領域的，所以開始追論文中所提到的一些關鍵名詞，就開始從 $seq2seq \\rightarrow attention \\rightarrow self-attention$。這中間 有太多知識需要記錄下來，所以將論文的內容分成三部曲，來記錄閱讀下來的點點滴滴:\n Part 1: Sequence to sequence model 起源 Part 2: Attention 與 Self-attention 的理解 Part 3: Transformer 的架構探討與深入理解  要談論 Attention Is All You Need 這篇 paper 就必須從 seq2seq 講起，seq2seq 全名為 Sequence to Sequence[1]，是一個 Encoder - Decoder 架構的模型，在 2014 年被提出，被廣泛的應用於 Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.","tags":["Seq2Seq","Teacher forcing","Beam search"],"title":"Transformer Part 1 - Seq2Seq","type":"post"},{"authors":["Roymond Liao"],"categories":["Kaggle"],"content":"在重新回顧 Kaggle 近期的 IEEE-CIS Fraud Detection 的比賽中，發現有人提到一個 Features selection 的方法 Adversarial validation。\nProblem 在建立模型時常常都會遇到 training set 與 testing set 的分佈存在明顯的差異的，而在分佈不相同的狀況下，即使我們使用 Kfold 的方法來驗證 model，也不會得到較好的結果，因為在驗證所取得的 validation set 也會與 testing set 有著分佈上的差異。\n在現實的處理方法，可以透過重新收集數據或是一些處理手段，來取得 training set 與 testing set 分佈相同的，但在資料的比賽中， training set 與 testing set 都是給定好的數據，並無法做其他跟改，而面對這樣的狀況， Adversarial validation 就是一個很好來處理這樣的問題。\nMothed 其實 Adversarial validation 的概念非常簡單，只需要幾個步驟：\n 將 training set 與 testing set 合併，並標注新的 target column is_train ($training = 1, testing = 0$) 建立一個 classifier 將 training set 的預測機率按照 Ascending 的方式排序，由小排到大。 取 Top $n\\%$ 的數據當作 validation set  藉由這樣的方式所取得的 validation set 在分佈上就與 testing set 相似，如果 model 在 validation 上取得好的預測結果，那相對地也能反映在 testing set。\nUnderstanding 依據 $(2)$ 建模的結果：\n Model 的 AUC 大約等於 0.5，表示 training set 與 testing set 來自相同的分佈 Model 的 AUC 非常高時，表示 training set 與 testing set 來自不相同的分佈，可以明顯地分開  Other 這邊提一下另一個 trick 的 features selection 方法，稱為 time consistency。在 IEEE-CIS Fraud Detection 比賽第一名的隊伍中，Chris Deotte 提出用了這個方法來去除掉對模型沒有影響力的 features。\nProblem 不管在現實的資料或是比賽的資料，部分資料都有可能因為時間的改變而分佈有所改變，這是我們在建立模型上不太希望發生的事情。因為如果 features 會因為時間的因素而分佈有明顯變化的話，在建模的過程中，受時間影響的 features 可能就會傷害模型本身，可能在時間相近的資料驗證有好的表現，但當預測時間間隔較長的資料時就會發生 overfitting。在處理上述的情況，我們期望 features 的分佈是穩定的，不希望因為時間的影響而有所改變，所以可以使用 time consistency 的方法來剔除這些受時間影響的 features。\nMothed Time consistency 的步驟，這邊以 IEEE-CIS Fraud Detection 的比賽資料為例：\n 將 training set 依據月為單位切分資料 training data 與 validation data 策略\u0008，這邊的策略可以自由調整改變，以下只舉幾個例子\n 選擇前 n 個月的資料為 training data，最後一個月的資料為 validation data 選擇前 n 個月的資料為 training data，中間跳過 m 個月份，最後一個月的資料為 validation data  選擇一個 feature，進行模型建立，分別查看模型的 AUC 在 training 與 validation 是否有差異\n  Understanding  如果 training 的 AUC 與 validation 的 AUC 差不多，表示這 feature 不受時間的變化影響 如果 training 的 AUC 與 validation 的 AUC 有明顯差異，表示這 feature 時間的變化影響，會影響模型本身，可以考慮移除  Code 以下是 Chris Deotte 所提供的簡單的程式碼：\n# ADD MONTH FEATURE import datetime START_DATE = datetime.datetime.strptime(\u0026#39;2017-11-30\u0026#39;, \u0026#39;%Y-%m-%d\u0026#39;) train[\u0026#39;DT_M\u0026#39;] = train[\u0026#39;TransactionDT\u0026#39;].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x))) train[\u0026#39;DT_M\u0026#39;] = (train[\u0026#39;DT_M\u0026#39;].dt.year-2017)*12 + train[\u0026#39;DT_M\u0026#39;].dt.month # SPLIT DATA INTO FIRST MONTH AND LAST MONTH train = train[train.DT_M==12].copy() validate = train[train.DT_M==17].copy() # TRAIN AND VALIDATE lgbm = lgb.LGBMClassifier(n_estimators=500, objective=\u0026#39;binary\u0026#39;, num_leaves=8, learning_rate=0.02) h = lgbm.fit(train[[col]], train.isFraud, eval_metric=\u0026#39;auc\u0026#39;, eval_names=[\u0026#39;train\u0026#39;, \u0026#39;valid\u0026#39;], eval_set=[(train[[col]],train.isFraud),(validate[[col]],validate.isFraud)], verbose=10) auc_train = np.round(h._best_score[\u0026#39;train\u0026#39;][\u0026#39;auc\u0026#39;], 4) auc_val = np.round(h._best_score[\u0026#39;valid\u0026#39;][\u0026#39;auc\u0026#39;], 4) print(\u0026#39;Best score in trian:{}, valid:{}\u0026#39;.format(auc_train, auc_val)) Btw，最近有看到一個驗證的方法叫做 Double Cross-Validation，這邊紀錄一下，有機會再來講講這方法的概念與應用。\nRefenece  http://fastml.com/adversarial-validation-part-one/ http://fastml.com/adversarial-validation-part-two/ https://blog.csdn.net/weixin_43896398/article/details/84762922 https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308  ","date":1574035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574035200,"objectID":"d3a4e4f268d370833711c6a5333a17ba","permalink":"https://roymondliao.github.io/post/2019-11-18_adversarial_validation/","publishdate":"2019-11-18T00:00:00Z","relpermalink":"/post/2019-11-18_adversarial_validation/","section":"post","summary":"在重新回顧 Kaggle 近期的 IEEE-CIS Fraud Detection 的比賽中，發現有人提到一個 Features selection 的方法 Adversarial validation。\nProblem 在建立模型時常常都會遇到 training set 與 testing set 的分佈存在明顯的差異的，而在分佈不相同的狀況下，即使我們使用 Kfold 的方法來驗證 model，也不會得到較好的結果，因為在驗證所取得的 validation set 也會與 testing set 有著分佈上的差異。\n在現實的處理方法，可以透過重新收集數據或是一些處理手段，來取得 training set 與 testing set 分佈相同的，但在資料的比賽中， training set 與 testing set 都是給定好的數據，並無法做其他跟改，而面對這樣的狀況， Adversarial validation 就是一個很好來處理這樣的問題。\nMothed 其實 Adversarial validation 的概念非常簡單，只需要幾個步驟：\n 將 training set 與 testing set 合併，並標注新的 target column is_train ($training = 1, testing = 0$) 建立一個 classifier 將 training set 的預測機率按照 Ascending 的方式排序，由小排到大。 取 Top $n\\%$ 的數據當作 validation set  藉由這樣的方式所取得的 validation set 在分佈上就與 testing set 相似，如果 model 在 validation 上取得好的預測結果，那相對地也能反映在 testing set。","tags":["Adversarial Validation","Time Consistency","Features Selection"],"title":"Adversarial validation","type":"post"},{"authors":["Roymond Liao"],"categories":["Optimizer"],"content":"Introduction 在目前的 optimizer 分為兩個主要發展方向：\n Adaptive learning rate, such as AdaGrad and Adam Accelerated schema (momentum), such as Polyak heavyball and Nesterov momentum  以上都是透過累積過往梯度下降所得到的結果來達到收斂，然而要獲得好的結果，都需要一些超參數的調整。\nLookahead method：\n 是一種新的優化方法，採用兩個不同的權重，分別為 fast weights 與 slow weights。fast weights 是使用一般常見的 optimizer 當作 inner optimizer 先進行 k 次的計算後得到的結果與預先保留的 slow weights 進行線性插值(linearly interpolating)來更新權重 ，更新後的 wieight 為新的 slow weights 並推動之前的 fast weights 往前探索，以這樣的方式進行迭代。\n 在使用不同的 inner optimizer 下，像是 SGD 或是 Adam，減少了對超參數調整的需求，並且可以以最小的計算需求確保在不同的深度學習任務中加快收斂速度。\n  \n演算過程 :\nStep 1 : 先設定 $\\phi$ 的初始值，以及選定 objective function $L$ Step 2 : 確定更新週期 $k$ 值、slow weight 的更新步伐 $\\alpha $ 以及 optimizer $A$ Step 3 : 更新 fast weight $\\theta$ ，$ \\space \\theta_{t,0} \\leftarrow \\phi_{t-1}, t=1,2,\\dots $ Step 4 : 利用 optimizer $A$ 迭代 $k$ 次更新，由 $\\theta_{t, i}$ 更新到 $\\theta_{t, k}, i=1, 2, \\dots, k$ Step 5 : 更新 slow weight $\\phi_{k} \\leftarrow \\phi_{k-1} + \\alpha\\left(\\theta_{t, k} - \\phi_{t-1}\\right)$ 重複 Step 3 - Step 5 直至收斂。\n其可以想像身處在山脈的頂端，而周邊都是山頭林立，有高有低，其中一座山可通往山腳下，其他都只是在山中繞來繞去，無法走下山。如果親自探索是非常困難，因為在選定一條路線的同時，必須要放棄其他路線，直到最終找到正確的通路，但是如果我們在山頂留下一位夥伴，在其狀況看起來不妙時及時把我們叫回，這樣能幫助我們在尋找出路的時候得到快速的進展，因此全部地形的探索速度將更快，而發生迷路的狀況也更低。\nMethod 如同 Algorithm 1 所表示的內循環(inner loop)的 optimizer A 在迭代 $k$ 次後，在 weight space 中，slow weights 的更新為與 fast weights k的線性插值(linearly interpolating)，$\\theta - \\phi$. 我們將 slow weights learning rate 表示為 $\\alpha$, 在 slow weights 更新後，fast weights 會重新設定為 slow weights 的位置。\nStandard optimization method typically require carefully tuned learning rate to prevent oscillation and slow converagence. However, lookahead benefits from a larger learning rate in the inner loop. When oscillation in the high curvature direction, the fast weights updates make rapid progress along the low curvature direction. The slow weights help smooth out the oscillation throught the parameter interpolation.\nSlow weights trajectory We can characterize the trajectory of the slow weights as an exponential moving average (EMA) of the final fast weights within each inner-loop, regardless of the inner optimizer. After k inner-loop steps we have:\n\\[ \\begin{align} \\phi_{t+1} \u0026= \\phi_{t} + \\alpha\\left(\\theta_{t, k} - \\phi_{t}\\right) \\\\ \u0026= \\left(1-\\alpha\\right)\\phi_{t} + \\alpha\\theta_{t, k} \\\\ \u0026= \\left(1-\\alpha\\right)\\left(\\phi_{t-1} + \\alpha\\left(\\theta_{t-1, k} - \\phi_{t-1}\\right) \\right) + \\alpha\\theta_{t, k} \\\\ \u0026 \\vdots \\\\ \u0026= \\alpha\\left[\\theta_{t, k} + (1 - \\alpha)\\theta_{t-1, k} + \\dots + (1 - \\alpha)^{t-1}\\theta_{0, k} \\right] + (1- \\alpha)^{t}\\theta_{0} \\end{align} \\]\nFast weights trajectory Within each inner-loop, the trajectory of the fast weight depends on the choice of underlying optimizer. Given an optimization algorithm A that takes in an objective function $L$ and the current mini-batch training examples $d$, we have the update rule for the fast weights: \\( \\theta_{t, i+1} = \\theta_{t, i} + A\\left(L, \\theta_{t, i-1}, d\\right) \\)\nWe have the choice of maintaining, interpolating, or resetting the internal state (e.g. momentum) of the inner optimizer. Every choice improves convergence of the inner optimizer.\nComputational complexity Lookahead has a constant computational overhead due to parameter copying and basic arithmetic operations that is amortized across the k inner loop updates. The number of operations is $O\\left(\\frac{k+1}{k}\\right)$ times that of the inner optimizer. Lookahead maintains a single additional copy of the number of learnable parameters in the model.\nEmpirical Analysis Robustness to inner optimization algorithm $k$ and $\\alpha$ 在論文中使用 CIFAR 的資料測試，Lookahead 能夠在不同的超參數設定下保有快速收斂的結果。在實驗中固定 slow weight step size $\\alpha = 0.5$ 與 $k=5$，inner optimizer 選擇使用 SGD optimizer，測試不同的 learning rate 與 momentum 參數，結果顯示如下:\n\n同時實驗了在超參數固定的狀況下，inner optimizer 的 fast weights 在歷經不同 $k$ 與 $\\alpha$ 的設定，結果如下圖:\n\nInner loop and outer loop evalation 為了更了解 Lookahead 的在 fast weights 與 slow weights 的更新狀況，透過 test accuracy 的結果來了解 weights 變化的趨勢。如下圖，在每次 inner loop 更新 fast weights 的情況下，對 test accuracy 造成大幅的下降，反映了在每次 inner loop 的更新都具有 high variance 的情況產生。然而，在 slow weights 的更新階段，降低了 variance 的影響，並且慢慢調整 test accuracy 的準確度。\n\nCode implement  https://github.com/bojone/keras_lookahead\n https://github.com/lifeiteng/Optimizers\n  Reference  Lookahead Optimizer: k steps forward, 1 step back https://www.infoq.cn/article/Q7gBMEHNrd2rkjqV6CM3?utm_source=rss\u0026amp;utm_medium=article https://www.infoq.cn/article/Q7gBMEHNrd2rkjqV6CM3  ","date":1570060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570060800,"objectID":"6b10a2c13777edd2780736f8f87e2839","permalink":"https://roymondliao.github.io/post/2019-10-03_lookahead/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/post/2019-10-03_lookahead/","section":"post","summary":"Introduction 在目前的 optimizer 分為兩個主要發展方向：\n Adaptive learning rate, such as AdaGrad and Adam Accelerated schema (momentum), such as Polyak heavyball and Nesterov momentum  以上都是透過累積過往梯度下降所得到的結果來達到收斂，然而要獲得好的結果，都需要一些超參數的調整。\nLookahead method：\n 是一種新的優化方法，採用兩個不同的權重，分別為 fast weights 與 slow weights。fast weights 是使用一般常見的 optimizer 當作 inner optimizer 先進行 k 次的計算後得到的結果與預先保留的 slow weights 進行線性插值(linearly interpolating)來更新權重 ，更新後的 wieight 為新的 slow weights 並推動之前的 fast weights 往前探索，以這樣的方式進行迭代。\n 在使用不同的 inner optimizer 下，像是 SGD 或是 Adam，減少了對超參數調整的需求，並且可以以最小的計算需求確保在不同的深度學習任務中加快收斂速度。\n  \n演算過程 :\nStep 1 : 先設定 $\\phi$ 的初始值，以及選定 objective function $L$ Step 2 : 確定更新週期 $k$ 值、slow weight 的更新步伐 $\\alpha $ 以及 optimizer $A$ Step 3 : 更新 fast weight $\\theta$ ，$ \\space \\theta_{t,0} \\leftarrow \\phi_{t-1}, t=1,2,\\dots $ Step 4 : 利用 optimizer $A$ 迭代 $k$ 次更新，由 $\\theta_{t, i}$ 更新到 $\\theta_{t, k}, i=1, 2, \\dots, k$ Step 5 : 更新 slow weight $\\phi_{k} \\leftarrow \\phi_{k-1} + \\alpha\\left(\\theta_{t, k} - \\phi_{t-1}\\right)$ 重複 Step 3 - Step 5 直至收斂。","tags":["Optimizer"],"title":"Lookahead Optimizer: k steps forward, 1 step back","type":"post"},{"authors":["Roymond Liao"],"categories":["Object Detection","Deep Learning"],"content":"Faster R-CNN 是由 object detection 的大神 Ross Girshick 於 2015 年所提出的一個非常經典的目標檢測(object detection)的方法，當中提出了 Region Proposal Networks 的方法應用在提取候選區域(reigon proposals) 取代了傳統的 Selective Search 的方式，大幅提升了目標檢測的精準度，也提升了整體計算的速度，另外 Kaiming He 博士也是共同作者。\n在介紹 Faster R-CNN 之前需要先了解何為 one stage 與 two stage，目前 object detection 的主流都是以 one stage 為基礎的演算法，建議可以參考下列兩篇很棒的文章:\n 什麼是one stage，什麼是two stage 物件偵測 物件偵測上的模型結構變化  Faster R-CNN 主要是由四個部分來完成:\n Feature Extractor Region Proposal Network (RPN) Regoin Proposal Filter ROI Pooling  下圖為 Faster R-CNN 的簡易架構圖:\n  Image credit: original paper  下圖是我參考了許多相關的部落格文章後，覺得在呈現 Faster R-CNN 的架構上最容易讓人了解的一張圖，可以搭配著上圖來對照一下！\n  Image credit: https://zhuanlan.zhihu.com/p/44599606  1. Feature Extractor 透過五層的 conv layer 來取得 feature maps，作為後續的共享的 input。 作者採用了 ZF Net 以及 VGG-16 為主，依據 input size 的不同，最後 feature maps 的 W x H 也有所不同，但 channel 數是相同的。\nZF Net 取第五層的 Feature maps，output 為 13 x 13 x 256\nVGG-16 取第五層的 Feature maps，output 為 7 x 7 x 256\n2. Region Proposal Networks 在解析 RPN 的內容前，先來談談 RPN 與 Anchors 之所會被提出來，是來解決什麼樣的問題。\n簡略說明兩個方法面向的問題:\nRPN 主要的目的是為了產生只具有 foreground 的候選區域(region proposals)， 所以在 RPN 的輸出會有所謂的 foreground 與 background 的分類機率(此處的foreground 與 background 可以理解成是否含有 objects )。相較原本 Selective Search 用比較相鄰區域的相似度(顏色, 紋理, …等)合併再一起的方式，加快了運算的速度，同時也加強了物件檢測的精準度。\nAnchors 主要的目的是要來解決由於圖片大小不同，所以導致每張圖片在最後要將結果還原成原圖的座標時，會產生複雜的計算。固定 bounding boxes 的大小，可以加快計算的效率，也可以減少過多不必要的後選區域的產生。\n接下來進入 RPN 生成 region proposals 的解析:\n  Image credit: original paper   對最後一層 conv layer 的 feature maps 用一個 n x n 的 spatial window 做 sliding window (stride=1, pad=1)，在論文中 n=3 是因為對於較大的圖片在計算上比較有效率。此處可以將 sliding windows 這個過程想成是做一個 convolution 的過程來理解，output 則會是 ( feature map width, feature map height, channel )。\n Sliding windows 的結果 mapping 到 lower-dimensional feature，此處將帶入關鍵的 anchors。\n  Anchors 是事先設定好的多組 bounding boxes，設定的組成是透過 image 對於 scale 與 aspect ratio 的參數設定來決定的。如下圖的右圖，論文中選擇 3 個 scale x 3 個 aspect ratio，所以共產生 9 個 archors。\n scale: the size of image, ex: $(128^2, 256^2, 512^2)$ aspect ratio: width of image / height of image, ex: (1:1, 1:2, 2:1)  表示在對 feature maps 進行 sliding windows 時，每個 sliding windows 對應原圖區域中的 9 個anchors，而 sliding windows 的中心點就對應 anchors 的中心點位置，藉由中心點與圖片的大小，就可以得到 sliding windows 的位置和原圖位置的映射關係(這邊可以用 receptive field 來理解)，就可以由原圖的位置與 ground truth 計算 Intersection over Union(IOU)，並且判斷是否有 objects。\n下面左圖示的紅點就是表示每個 sliding window 的中心點對應原圖的位置，而右圖是在表示 9 種不同大小的 anchor 在原圖的呈現。\n Image credit: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/  下圖呈現出當 9 個不同 anchor 映射到 sliding windows 的中心點，在原圖上的呈現，由這樣的步驟可以理解這 9 個 anchor 剛好足夠可以框出圖片上的所有 object。這邊要注意，如果 anchor boxes 超出原圖的邊框就要被忽略掉。\n  Image credit: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/  在上圖可以看到，每個 sliding windows 映射到原圖，原圖上每個 sliding windows 的中心點對應 9 個 anchors，所以將 intermediate layer 所得到的 features 輸入給 兩個 sliding fully-connected layers。\n box-regression layer (reg layer): 輸出 4 x k個 boxes 的 encoding 座標值。 box-classification layer (cls layer): 輸出 2 x k 個關於 forground / background 的機率   此方法有效的原因:\n The anchors 與 ground truth 的 intersection-over-union (IOU) 重疊率很高 IOU \u0026gt; 0.7 為 positive，IOU \u0026lt; 0.3 為 negative，介於 0.7 \u0026gt;= IOU \u0026gt;= 0.3 則忽略，期望 positive 的 proposal 包含前景的機率高，negative 包含背景的機率高。  Faster R-CNN 的缺點:\n 在單一 scale 的 feature map 做 object localization and Classification，而且還是 scale=1/32 下，在小物件偵測效果相對不佳，有可能在down-scale時小物件的特徵就消失了  Reference  https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/ https://medium.com/@tanaykarmarkar/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9 https://blog.csdn.net/baderange/article/details/79643478 https://blog.csdn.net/lanran2/article/details/54376126  ","date":1557792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557792000,"objectID":"09ea5eeaaf283ecd68d52f4dc87e3bee","permalink":"https://roymondliao.github.io/post/2019-05-14_faster_rcnn/","publishdate":"2019-05-14T00:00:00Z","relpermalink":"/post/2019-05-14_faster_rcnn/","section":"post","summary":"Faster R-CNN 是由 object detection 的大神 Ross Girshick 於 2015 年所提出的一個非常經典的目標檢測(object detection)的方法，當中提出了 Region Proposal Networks 的方法應用在提取候選區域(reigon proposals) 取代了傳統的 Selective Search 的方式，大幅提升了目標檢測的精準度，也提升了整體計算的速度，另外 Kaiming He 博士也是共同作者。\n在介紹 Faster R-CNN 之前需要先了解何為 one stage 與 two stage，目前 object detection 的主流都是以 one stage 為基礎的演算法，建議可以參考下列兩篇很棒的文章:\n 什麼是one stage，什麼是two stage 物件偵測 物件偵測上的模型結構變化  Faster R-CNN 主要是由四個部分來完成:\n Feature Extractor Region Proposal Network (RPN) Regoin Proposal Filter ROI Pooling  下圖為 Faster R-CNN 的簡易架構圖:\n  Image credit: original paper  下圖是我參考了許多相關的部落格文章後，覺得在呈現 Faster R-CNN 的架構上最容易讓人了解的一張圖，可以搭配著上圖來對照一下！","tags":["Object Detection","CV"],"title":"Faster R-CNN","type":"post"},{"authors":["Roymond Liao"],"categories":["NLP","Deep Learning"],"content":"最近剛好看到一篇關於 Skip-gram word2vec的介紹，內文寫的淺顯易懂，衍生的閱讀也十分詳細，決定動手寫篇記錄下來。\n人對於文字的理解，可以很簡單的就能了解字面的意義，但是對於機器來說，要如何理解文字是一個很困難的問題。 要如何讓機器來理解文字的意義？ 透過將文字轉換成向量，來讓機器能夠讀的懂，所以其實文字對於機器來說只是數字，而我們在做的就只是數字的遊戲。\nWord embeddings 在將字詞轉換成向量的實作中，大家常用的方法肯定是 one-hot-encoding，但是 one-hot-encoding 在計算上卻是非常沒有效率的方式，如果一篇文章中總共有50,000的單詞，用 one-hot-encoding 來表示某個單詞的話，將會變成1與49999個0的向量表示。就如同下圖表示，如果要做 matrix multiplication 的話，那將會浪費許多的計算資源。\n透過 Word Embedding1 可以有效的解決上述的問題。 Embedding 可以想成與 full connected layer 一樣，將這個 layer 稱做為 embedding layer ， weight 則稱為 embedding weights。藉由這樣的概念，可以省略掉 multiplication 的過程，直接透過 hidden layer 的 weigth matrix 來當作輸入字詞的 word vector。之所以可以這樣來執行是因為在處理 one-hot-encoding 與 weight matrix 相乘的結果，其實就是 matrix 所對應\u0026quot;詞\u0026quot;的索引值所得到的結果。\n舉例來說： \u0026quot;heart\u0026quot; 的在 one-hot-encoding 的索引位置為958，我們直接拿取 heart 所對應 hidden layer 的值，也就是 embedding weights 的第958列(row)，這樣的過程叫做 embedding lookup，而 hidden layer 的神經元數量則為 embedding dimension。\n另一個解釋是在於 word2vec 是一個三層架構，分別是 input layer、hidden layer、output layer，但是在 hidden layer 並沒有非線性的 activation function，由於 input layer 是經由 one-hot-encoding 過的資訊，所以在 hidden layer 所取得的值，其實就是對應輸入層得值；另外一提 output layer 的 activation function 是 sigmoid。\n原文中最後提到的三個主要重點：\n The embedding lookup table is just a weight matrix. The embedding layer is just a hidden layer. The lookup is just a shortcut for the matrix multiplication.  Models 介紹完 word embedding 後，要來介紹 word2vec algorithm 中的兩個 model：\n Skip-gram CBOW(Continous Bag-Of-Words)  Skip-gram model 用下列兩張圖來解釋 skip-gram model 的結構，假設model是一個simple logistic regression(softmax)，左邊的圖表示為概念上的架構(conceptual architecture)，右邊的圖則為實作上的架構(implemented architectures)，雖然圖的架構有些微不同，但是實際上是一樣的，並沒有任何的改變。 首先定義參數：\n V - Vocabulary Size (Number of unique words in the corpora) P - The Projection or the Embedding Layer D - Dimensionality of the Embedding Space b - Size of a single Batch  由左圖可以了解到，Skip-gram model 的 input(X) 為一個單詞，而你的目標，也就是你的 output(Y) 為相鄰的單詞。換句話就是在一句話中，選定句子當中的任意詞作為 input word，而與 input word 相鄰的字詞則為 model 的所要預測的目標(labels)，最後會得到相鄰字詞與 input word 相對應的機率。\n但是上述的想法會出現一個問題，就是你只提供一個字詞的訊息，然而要得到相鄰字詞出現的機率，這是很困難的一件事，效果也不佳。所以這邊提出兩個方法:\n 針對\u0026quot;相鄰字詞\u0026quot;這部分，加入了 window size 的參數做調整 將輸出所有字詞的方式轉成一對一成對的方式  舉例來說：\u0026quot;The dog barked at the mailman.\u0026quot; 這樣一句話，選定 dog 做為 input，設定window size = 2，則 \u0026quot;dag\u0026quot; 下上兩個相鄰字詞為 ['the', 'barked', 'at'] 就會是我們的 output。此外將原本的(input: 'dag', output: '['the', 'barked', 'at']) 轉換成 (input: 'dag', output: 'the'), (input: 'dag', output: 'barked'), (input: 'dag', output: 'at') 這樣一對一的方式。這樣的過程就如同右圖 implemented architectures。\n下圖解釋一個語句的training samples產生:\n所以當training samples: (brown, fox)的數量越多時，輸入brown得到fox的機率越高。\nModel Details Input layer: 字詞經過 one-hot-encoding 的向量表示。 hidden layer: no activation function，上述介紹 embedding layer 已經解釋過。 output layer: use softmax regression classifier，output 的結果介於0與1之間，且加總所有的值和為1。\n假設輸入的 word pair 為(ants, able)，則模型的目標是 \\(max P\\left(able | ants \\right)\\)，同時也需要滿足 \\(min P\\left(other \\space words | ants \\right)\\)，這裡利用 log-likehood function 作為目標函數。\n\\[ P\\left(able | ants \\right) = softmax\\left( X_{ants 1\\times 10000} \\cdot W_{10000 \\times 300}\\right) \\]\n\\[ Y = sotfmax(Y) =\\frac{exp(X_{1 \\times 300} \\cdot W_{300 \\times 1})}{\\sum_{i=1}^{10000} exp(X_{1 \\times 300}^i \\cdot W_{300 \\times 1})} \\]\nlog-likehood function:\n\\[ L(W) = P\\left(able \\mid ants \\right)^{y=able} \\times P\\left(other \\space words | ants \\right)^{y=other \\space words} \\]\nObjective function可以表示如下：\n\\[ \\begin{align} LogL\\left(W\\right) \u0026 = \\{y = target \\space word\\} \\{logP\\left(able | ants \\right) + logP\\left(other \\space words | ants \\right)\\}\\\\ \u0026 = \\sum_{i}^{10000}\\{ y = target \\space word\\}logP\\left( word_{i} | ants \\right) \\end{align} \\]\n之後就是 Maxmium log-likehood function。 由上述的介紹，會發現一個問題，這是一個非常巨大的 NN model。假設 word vectors 為300維的向量，具有10,000個字詞時，總共會有 \\(300 \\times10000 = 3\\) 百萬的 weight 需要訓練!! 這樣的計算 gradient descent 時造成模型的訓練時間會非常的久。\n對於這問題，Word2Vec 的作者在paper第二部分有提出以下的解決方法:\n Treating common word pairs or phrases as single words in their model. Subsampling frequent words to decrease the number of training examples. Modifying the optimization objective with a technique they called Negative Sampling, which causes each training sample to update only a small percentage of the model’s weights A computationally efficient approximation of the full softmax is the hierarchical softmax.  Subsampling 與 Negative Sampling 這兩個實作方法不只加速了模型的訓練速度，同時也提升模型的準確率。\n Words pairs and phrases  比如說 New York City 在字詞訓練時，會拆成 New、York、City 三個字詞，但是這樣分開來無法表達出原意，所以將\u0026quot;New York City\u0026quot;組合為一個單詞做訓練。\n Subsampling frequent words  在剛剛透過下圖解釋了相關的原理，但是這會發現兩個問題，一是像是出現(the, fox)這樣的 pair，並沒有告訴我們有用的資訊，並且\u0026quot;the\u0026quot;是常出現的字詞；二是有大量像是\u0026quot;the\u0026quot;這類的字詞出現在文章，要如何有意義地學習\u0026quot;the\u0026quot;字詞表示的意思。\nsubsamplig 針對這樣的狀況，透過一個機率值來判斷詞是否應該保留。機率值計算公式如下: \\( P\\left( w_{i} \\right) = \\left( \\sqrt{\\frac{Z(w_{i})}{0.001} + 1} \\right) \\cdot \\frac{0.001}{Z(w_{i})} \\) 其中$P\\left( w_{i} \\right)$表示$w_{i}$的出現機率，0.001為默認值。具體結果如下圖，字詞出現的頻率越高，相對被採用的機率越低。  Negative Sampling  此方法目的是希望只透過正確的目標字詞來小改動 weight。比如說，ward pair (fox, qiuck)，在這個例子中\u0026quot;qiuck\u0026quot;為目標字詞，所以標記為1，而其他與 fox 無相關的字詞標記為0，就稱之為 negative sampling，這樣的 output 就有像是 one-hot vector，只有正確的目標字詞為1(positive word)，其他為0(negative word)。 至於 Negative sampling size 需要多少，底下是Word2Vec的作者給出的建議:\n The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.\n 所以假設是以上面描述的狀況，qiuck 則為 postive word，另外加上5個 negative word，output 值為6個值，總共有 \\(300 \\times 6 = 1800\\) 個 weight 需要更新，這樣只佔了原本300萬的 weight 0.06%而已!\n該如何挑選 negative sampling? 則是透過 unigram distribution 的機率來挑選， 在C語言實作 word2vec 的程式碼中得到以下公式\n\\[ P\\left( w_{i} \\right) = \\frac{f\\left( w_{i} \\right)^{\\frac{3}{4}} }{\\sum_{j=0}^{n} \\left( f\\left( w_{j}\\right)^{\\frac{3}{4}} \\right)} \\]\n$\\frac{3}{4}$次方的選擇是來至於實驗測試的結果。\nDefine Objective function:\n\\( log \\space \\sigma \\left( v_{I}^{T} \\cdot v_{o} \\right) - \\sum_{i=1}^{k} E_{w_{i} - P_{v}}[\\sigma\\left( -v_{w_{i}}^{T}v_{wI} \\right)] \\) $ Note \\space \\sigma(-x) = 1 - \\sigma(x)$\nReference  An implementation skip-gram of word2vec from Thushan Ganegedara An implementation CBOW of word2vec from Thushan Ganegedara Word2Vec Tutorial Part1 and Part2 from Chris McCormick Deep understand with word2vec form Mark Chang's Blog (Chinese) Efficient Estimation of Word Representations in Vector Space Distributed Representations of Words and Phrases and their Compositionality  Plus reference  FastText    word embedding: 將單詞word映射到另一個空間，其中這個映射具有injective和structure-preserving的特性。 ^   ","date":1556755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556755200,"objectID":"6eb40e0ce546bb278d1feedef2f5e3c2","permalink":"https://roymondliao.github.io/post/2019-05-02_word2vec/","publishdate":"2019-05-02T00:00:00Z","relpermalink":"/post/2019-05-02_word2vec/","section":"post","summary":"最近剛好看到一篇關於 Skip-gram word2vec的介紹，內文寫的淺顯易懂，衍生的閱讀也十分詳細，決定動手寫篇記錄下來。\n人對於文字的理解，可以很簡單的就能了解字面的意義，但是對於機器來說，要如何理解文字是一個很困難的問題。 要如何讓機器來理解文字的意義？ 透過將文字轉換成向量，來讓機器能夠讀的懂，所以其實文字對於機器來說只是數字，而我們在做的就只是數字的遊戲。\nWord embeddings 在將字詞轉換成向量的實作中，大家常用的方法肯定是 one-hot-encoding，但是 one-hot-encoding 在計算上卻是非常沒有效率的方式，如果一篇文章中總共有50,000的單詞，用 one-hot-encoding 來表示某個單詞的話，將會變成1與49999個0的向量表示。就如同下圖表示，如果要做 matrix multiplication 的話，那將會浪費許多的計算資源。\n透過 Word Embedding1 可以有效的解決上述的問題。 Embedding 可以想成與 full connected layer 一樣，將這個 layer 稱做為 embedding layer ， weight 則稱為 embedding weights。藉由這樣的概念，可以省略掉 multiplication 的過程，直接透過 hidden layer 的 weigth matrix 來當作輸入字詞的 word vector。之所以可以這樣來執行是因為在處理 one-hot-encoding 與 weight matrix 相乘的結果，其實就是 matrix 所對應\u0026quot;詞\u0026quot;的索引值所得到的結果。\n舉例來說： \u0026quot;heart\u0026quot; 的在 one-hot-encoding 的索引位置為958，我們直接拿取 heart 所對應 hidden layer 的值，也就是 embedding weights 的第958列(row)，這樣的過程叫做 embedding lookup，而 hidden layer 的神經元數量則為 embedding dimension。","tags":["Word Embedding"],"title":"Word2Vec","type":"post"},{"authors":["Roymond Liao"],"categories":["Optimizer"],"content":"Dropout 相關方法：\n Dropout: 完全隨機丟棄 neuron Sparital Dropout: 按 channel 隨機丟棄 Stochastic Depth: 按 res block 隨機丟棄 DropBlock: 每個 feature map 上按 spatial square 隨機丟棄 Cutout: 在 input layer 按 spatial square 隨機丟棄 DropConnect: 只在連接處丟，不丟 neuron DropBlock  Idea 一般的 Dropout 都是用在 fully connection layer，而在 convolutional network 上使用 dropout 的意義並不大，該文章則認為因為在每一個 feature maps 的位置都具有一個 receptive field，僅對單一像素位置進行 dropout 並不能降低 feature maps 學習特徵範圍，也就是說，network 能夠特過相鄰位置的特徵值去學習，也不會特別加強去學習保留下來的訊息。既然對於單獨的對每個位置進行 dropout 並無法提高 network 本身的泛化能力，那就以區塊的概念來進行 dropout，反而更能讓 network 去學習保留下來的訊息，而加重特徵的權重。 Method  不同 feature maps 共享相同的 dropblock mask，在相同的位置丟棄訊息 每一層的 feature maps 使用各自的 dropblock mask  Parameters block size: 控制要讓 value of feature maps 歸為 0 的區塊大小 $ \\gamma $: 用來控制要丟棄特徵的數量 keep_prob: 與 dropout 的參數相同 Code implement https://github.com/DHZS/tf-dropblock/blob/master/nets/dropblock.py https://github.com/shenmbsw/tensorflow-dropblock/blob/master/dropblock.py\n   Bernoulli distrubtion:\nimport tensorflow as tf tf.reset_default_graph() with tf.Graph().as_default() as g: mean = tf.placeholder(tf.float32, [None]) input_shape = tf.placeholder(tf.float32, [None, 4, 4, 3]) shape = tf.stack(tf.shape(input_shape)) # method 1 # 用 uniform distributions 產生值，再透過 sign 轉為 [-1, 1], 最後透過 relu 將 -1 轉換為 0 uniform_dist = tf.random_uniform(shape, minval=0, maxval=1, dtype=tf.float32) sign_dist = tf.sign(mean - uniform_dist) bernoulli = tf.nn.relu(sign_dist) # method 2 # probs 可以為多個 p, 對應 shape, 產生 n of p 的 bernoulli distributions noise_dist = tf.distributions.Bernoulli(probs=[0.1]) mask = noise_dist.sample(shape) with tf.Session(graph=g) as sess: tmp_array = np.zeros([4, 4, 3], dtype=np.uint8) tmp_array[:,:100] = [255, 0, 0] #Orange left side array[:,100:] = [0, 0, 255] #Blue right side batch_array = np.array([tmp_array]*3) uniform, sign, bern = sess.run([uniform_dist, sign_dist, bernoulli], feed_dict={mean: [1.], input_shape:batch_array})  DropBlock implement:\nimport tensorflow as tf from tensorflow.python.keras import backend as K class DropBlock(tf.keras.layers.Layer) : def __init__(self, keep_prob, block_size, **kwargs): super(DropBlock, self).__init__(**kwargs) self.keep_prob = float(keep_prob) if isinstance(keep_prob, int) else keep_prob self.block_size = int(block_size) def compute_output_shape(self, input_shape): return input_shape def build(self, input_shape): _, self.h, self.w, self.channel = input_shape.as_list() # pad the mask bottom = right = (self.block_size -1) // 2 top = left = (self.block_size -1) - bottom self.padding = [[0, 0], [top, bottom], [left, right], [0, 0]] self.set_keep_prob() super(DropBlock, self).build(input_shape) def set_keep_prob(self, keep_prob=None): \u0026#34;\u0026#34;\u0026#34;This method only support Eager Execution\u0026#34;\u0026#34;\u0026#34; if keep_prob is not None: self.keep_prob = keep_prob w, h = tf.to_float(self.w), tf.to_float(self.h) self.gamma = (1. - self.keep_prob) * (w * h) / (self.block_size ** 2) / ((w - self.block_size + 1) * (h - self.block_size + 1)) def _create_mask(self, input_shape): sampling_mask_shape = tf.stack([input_shape[0], self.h - self.block_size + 1, self.w - self.block_size + 1, self.channel]) mask = DropBlock._bernoulli(sampling_mask_shape, self.gamma) # 擴充行列，並給予0值，依據 paddings 參數給予的上下左右值來做擴充，mode有三種模式可選，可參考 document mask = tf.pad(tensor=mask, paddings=self.padding, mode=\u0026#39;CONSTANT\u0026#39;) mask = tf.nn.max_pool(value=mask, ksize=[1, self.block_size, self.block_size, 1], strides=[1, 1, 1, 1], padding=\u0026#39;SAME\u0026#39;) mask = 1 - mask return mask @staticmethod def _bernoulli(shape, mean): return tf.nn.relu(tf.sign(mean - tf.random_uniform(shape, minval=0, maxval=1, dtype=tf.float32))) # The call function is a built-in function in \u0026#39;tf.keras\u0026#39;. def call(self, inputs, training=None, scale=True, **kwargs): def drop(): mask = self._create_mask(tf.shape(inputs)) output = inputs * mask output = tf.cond(tf.constant(scale, dtype=tf.bool) if isinstance(scale, bool) else scale, true_fn=lambda: output * tf.to_float(tf.size(mask)) / tf.reduce_sum(mask), false_fn=lambda: output) return output if training is None: training = K.learning_phase() output = tf.cond(tf.logical_or(tf.logical_not(training), tf.equal(self.keep_prob, 1.0)), true_fn=lambda: inputs, false_fn=drop) return output# Testing a = tf.placeholder(tf.float32, [None, 5, 5, 3]) keep_prob = tf.placeholder(tf.float32) training = tf.placeholder(tf.bool) drop_block = DropBlock(keep_prob=keep_prob, block_size=3) b = drop_block(inputs=a, training=training) sess = tf.Session() feed_dict = {a: np.ones([2, 5, 5, 3]), keep_prob: 0.8, training: True} c = sess.run(b, feed_dict=feed_dict) print(c[0, :, :, 0])  Targeted Dropout  Reference:\n https://cloud.tencent.com/developer/article/1367373  ","date":1554854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554854400,"objectID":"b19d2259c07dcb7663a5deecff85a62b","permalink":"https://roymondliao.github.io/post/2019-04-10_dropblock/","publishdate":"2019-04-10T00:00:00Z","relpermalink":"/post/2019-04-10_dropblock/","section":"post","summary":"Dropout 相關方法：\n Dropout: 完全隨機丟棄 neuron Sparital Dropout: 按 channel 隨機丟棄 Stochastic Depth: 按 res block 隨機丟棄 DropBlock: 每個 feature map 上按 spatial square 隨機丟棄 Cutout: 在 input layer 按 spatial square 隨機丟棄 DropConnect: 只在連接處丟，不丟 neuron DropBlock  Idea 一般的 Dropout 都是用在 fully connection layer，而在 convolutional network 上使用 dropout 的意義並不大，該文章則認為因為在每一個 feature maps 的位置都具有一個 receptive field，僅對單一像素位置進行 dropout 並不能降低 feature maps 學習特徵範圍，也就是說，network 能夠特過相鄰位置的特徵值去學習，也不會特別加強去學習保留下來的訊息。既然對於單獨的對每個位置進行 dropout 並無法提高 network 本身的泛化能力，那就以區塊的概念來進行 dropout，反而更能讓 network 去學習保留下來的訊息，而加重特徵的權重。 Method  不同 feature maps 共享相同的 dropblock mask，在相同的位置丟棄訊息 每一層的 feature maps 使用各自的 dropblock mask  Parameters block size: 控制要讓 value of feature maps 歸為 0 的區塊大小 $ \\gamma $: 用來控制要丟棄特徵的數量 keep_prob: 與 dropout 的參數相同 Code implement https://github.","tags":["Optimizer"],"title":"DropBlock","type":"post"},{"authors":["Roymond Liao"],"categories":["Machine Learning"],"content":"Review note Bagging\n Concept\nBagging involves creating mulitple copies of the original training data set using the boostrap, fitting a seperate decision tree to each copy, and then combining all of the trees in order to create a single predcitive model. Notably, each tree is built on a bootstrap data set, independent of the other trees.\nAlgorithm\n Random Forest   Boosting\n Concept\nBoosting works in a similar way with bagging, except that the trees are grown sequentially. Each tree is grown using information from previous grown trees.\nAlgorithm\n Adaboost - Yoav Freund and Robert Schapire\n 根據樣本的誤差來調整樣本的權重，誤差較大的樣本給予較高的權重，反之亦然。藉此著重訓練分類錯誤的資料，進而來增進模型的準確度。  Gradient boosting - Friedman, J.H.\n 根據當前模型的殘差來調整權重的大小，其目的是為了降低殘差。通過迭代的方式，使損失函數(loss function)達到最小值(局部最小)。\n Method\n GBDT(Grandien Boosting Decision Tree) XGBoost(eXtreme Gradient Boosting)](https://github.com/dmlc/xgboost) - Tianqi Chen LightGBM(Light Gradient Boosting Machine)](https://github.com/Microsoft/LightGBM) - Microsoft Research Asia     Advantages of XGBoost  傳統 GBDT 是以 CART 作為分類器的基礎，但是XGBoost還可以支援線性分類器，另外在 objective function 可以加入 L1 regularization 和 L2 regularization 的方式來優化，降低了 model 的 variance，避免 overfitting 的狀況。 GBDT 在優化部分只使用到泰勒展開式的一階導數，但 XGBoost 則使用到二階導數，所以在預測準確度上提供更多的訊息。 XGBoost 支援平行運算與分布式運算，所以相較傳統的GBDT在計算速度上有大幅的提升。XGBoost 的平行並非是在 tree 的維度做平行化處理，而是在 features 的維度上做平行化處理，因為 tree 的生長是需要前一次迭代的結果的來進行 tree 的生長。 對 features 進行預排序的處理，然後保存排序的結構，以利後續再 tree 的分裂上能夠快速的計算每個 features 的 gain 的結果，最終選擇 gain 最大的 feature 進行分裂，這樣的方式就可以平行化處理。 加入 shrinkage 和 column subsampling 的優化技術。 有效地處理 missing value 的問題。 先從頭到尾建立所有可能的 sub trees，再從底到頭的方式進行剪枝(pruning)。  Disadvantages of XGBoost  在每次的迭代過程中，都需要掃過整個訓練集合多次。如果把整個訓練集合存到 memory 會限制數據的大小;如果不存到 memory 中，反覆的讀寫訓練集合也會消耗非常多的時間。 預排序方法(pre-sorted): 由於需要先針對 feature 內的 value 進行排序並且保存排序的結果，以利於後續的 gain 的計算，但在這個計算上就需要消耗兩倍的 memory 空間，來執行。  Reference  http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf http://mlnote.com/2016/10/05/a-guide-to-xgboost-A-Scalable-Tree-Boosting-System/ https://www.zybuluo.com/yxd/note/611571#机器学习的关键元素 https://en.wikipedia.org/wiki/Gradient_boosting#Shrinkage http://zhanpengfang.github.io/418home.html  Paper  Fridman J.H. (1999). Greedy Function Approximation: A Gradient Boosting Machine Tianqi Chen, Carlos Gusetrin (2016). XGBoost: A Scalable Tree Boosting System  Doing  https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ https://adeshpande3.github.io/adeshpande3.github.io/Applying-Machine-Learning-to-March-Madness  ","date":1551484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551484800,"objectID":"77ea0ef21fc7feb3cd900b5fd9f2e711","permalink":"https://roymondliao.github.io/post/2019-03-02_xgboost/","publishdate":"2019-03-02T00:00:00Z","relpermalink":"/post/2019-03-02_xgboost/","section":"post","summary":"Review note for XGBoost","tags":["Machine Learning","XGBoost"],"title":"XGBoost","type":"post"},{"authors":null,"categories":null,"content":"title: \u0026ldquo;Transformer Part 3\u0026rdquo; date: 2019-12-16 lastmod: 2020-02-28 draft: true authors: [\u0026ldquo;Roymond Liao\u0026rdquo;] categories: - NLP - Deep Learning tags: [\u0026ldquo;Self-attention\u0026rdquo;, \u0026ldquo;Transformer\u0026rdquo;] markup: mmark image: placement: 2 caption: \u0026ldquo;Photo by Christian Wagner on Unsplash\u0026rdquo; focal_point: \u0026ldquo;Center\u0026rdquo; preview_only: false\nReference  The IIIustrated Transformer w淺談神經機器翻譯 \u0026amp; 用 Transformer 與 Tensorflow2 Attention is all you need 解讀 Transformer model for language understanding by google How Self-Attention with Relative Position Representations works  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8cce97c8b833aab6de21a9d538a5c30c","permalink":"https://roymondliao.github.io/post/2019-12-16_transformer_part3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2019-12-16_transformer_part3/","section":"post","summary":"title: \u0026ldquo;Transformer Part 3\u0026rdquo; date: 2019-12-16 lastmod: 2020-02-28 draft: true authors: [\u0026ldquo;Roymond Liao\u0026rdquo;] categories: - NLP - Deep Learning tags: [\u0026ldquo;Self-attention\u0026rdquo;, \u0026ldquo;Transformer\u0026rdquo;] markup: mmark image: placement: 2 caption: \u0026ldquo;Photo by Christian Wagner on Unsplash\u0026rdquo; focal_point: \u0026ldquo;Center\u0026rdquo; preview_only: false\nReference  The IIIustrated Transformer w淺談神經機器翻譯 \u0026amp; 用 Transformer 與 Tensorflow2 Attention is all you need 解讀 Transformer model for language understanding by google How Self-Attention with Relative Position Representations works  ","tags":null,"title":"","type":"post"}]