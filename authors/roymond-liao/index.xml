<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Roymond Liao</title>
    <link>https://roymondliao.github.io/authors/roymond-liao/</link>
      <atom:link href="https://roymondliao.github.io/authors/roymond-liao/index.xml" rel="self" type="application/rss+xml" />
    <description>Roymond Liao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - © 2020</copyright><lastBuildDate>Thu, 08 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://roymondliao.github.io/img/icon-192.png</url>
      <title>Roymond Liao</title>
      <link>https://roymondliao.github.io/authors/roymond-liao/</link>
    </image>
    
    <item>
      <title>[Paper-NLP] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
      <link>https://roymondliao.github.io/post/albert/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/albert/</guid>
      <description>&lt;h1 id=&#34;albert&#34;&gt;ALBERT&lt;/h1&gt;
&lt;p&gt;在 2017 年 Transformer 的誕生，突破了 RNN、LSTM、GRU &amp;hellip; 等在計算上的限制，也帶來新的觀點，爾後再 2018 年底 &lt;strong&gt;Google&lt;/strong&gt; 發表了 &lt;strong&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/strong&gt; 開啟了通往偉大到航道的路線，也帶起了 pre-training model 的各種應用，不用再辛苦的從頭開始訓練，為了資料問題所苦惱。在 BERT 之後，湧出各種基於 BERT 的架構下進行優化改進，例如： GPT-2、XLNet、RoBERTa、ERNIE &amp;hellip; 等這些耳熟能詳的模型，而這次將是為大家介紹也是基於 BERT 的架構下 Google 在 2019 年推出的輕量化版本的 BERT，&lt;strong&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;如前言所述，在 BERT 後的時代透過大量的資料進行自監督(self-supervised&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;)的訓練，提高模型參數與更深層的結構，讓模型取得更好的表現，但也因為硬體上的限制，要訓練參數量大的模型就可能需要做到平行化處理以及記憶體內存的管控，但這樣的方式並沒有解決成本上的問題。基於這樣的情況，作者提出了下列問題：&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;&lt;b&gt; Is haveing better NLP models as easy as hvaing larger models?  &lt;/b&gt;&lt;/div&gt;
&lt;p&gt;也因為這個問題的討論，造就了 &lt;strong&gt;A Lite BERT(ALBERT)&lt;/strong&gt; 的模型架構出來。&lt;/p&gt;
&lt;h2 id=&#34;model-architecture&#34;&gt;Model architecture&lt;/h2&gt;
&lt;p&gt;ALBERT 的模型架構與 BERT 相似，都是使用 transformer encoder 搭配 GELU nonlinearities 為主軸，但為了降低模型的參數並且獲得更好的表現，採用了兩種降低模型參數的方法與更換不同的預訓練任務，接下來將會一一介紹。&lt;/p&gt;
&lt;h3 id=&#34;reduction-techniques&#34;&gt;Reduction techniques&lt;/h3&gt;
&lt;p&gt;ALBERT 採用了兩種減少參數的方法，解決在預訓練時模型擴展的問題，以下分別來解說採用的方法。&lt;/p&gt;
&lt;h4 id=&#34;1-factorized-embedding-parameterization&#34;&gt;1. Factorized embedding parameterization&lt;/h4&gt;
&lt;p&gt;在 BERT、XLNet、RoBERTa 中，都採用了 &lt;strong&gt;WordPiece&lt;/strong&gt; 的方法，其 WordPiece embedding size $E$ 與 hidden layer size $H$ 是綁定在一起的，也就是說 size  大小一模一樣。這樣的方式對於模型來說是次優的辦法，而非最好的選擇，原因在於：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WordPiece embedding 學習的是語句上下文相互獨立的表示方式&lt;/li&gt;
&lt;li&gt;Hidden-layer embedding 學習的是語句上下文有相關的表示方式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於各別 embedding 在學習語句上的概念不同，所以作者在這邊進行了拆解，將原本 $E$ 與 $H$ 的 size 個別獨立，從 $ E \equiv H \rightarrow H \gg E $，這樣的拆解可以更有效優化模型的參數，也大幅降低了總參數量。因為如果採用原本的方式，綁定在一起，那麼當中提高 $H$ 的 size，那根據 vocabulary size $V$ 的大小(通常在一般的情況下 $V$ 是很大的，而在 BERT 中 $V$ 大約為 30,000)，WordPiece embedding  將會是 $V \times E$ 的矩陣大小，這樣是很容易得到一個有十億級別的模型參數，而且大部分的 embedding 在訓練期間都很少量的更新。&lt;/p&gt;
&lt;p&gt;因此 ALBERT 對 WordPiece embedding 將行因式分解(factorization)，將其拆解成兩個小矩陣。先將 One-hot vector 投影到大小為 $E$ 的低維度空間中，然後再從低維度空間投影回 Hidden-layer embedding。模型在 embedding parameters 從原本的 $O\left(V \times H \right) \rightarrow O\left( V \times E + E \times H \right)$，大幅的降低模型的參數。&lt;/p&gt;
&lt;h4 id=&#34;2-cross-layer-parameter-sharing&#34;&gt;2. Cross-layer parameter sharing&lt;/h4&gt;
&lt;p&gt;參數共享的這個想法，作者指出在 Transformer&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; 時就有被拿來討論過，並使用在 encoder-decoder 的任務上而非是針對 pretraining/finetuning 的訓練上(這邊作者指的部分我認為應該是說 Embedding layer 與 pre-softmax linear transformation layer 的參數共享部分)。在 Dehghani et al. (2018)&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; 所提出的 &lt;strong&gt;Universal Transformer&lt;/strong&gt; 就展現了cross-layer parameter sharing 的方法在語言模型上得到更好的結果，在近年 Bai et al. (2019)&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; 所提出的 &lt;strong&gt;Deep Equilibrium Model (DQE)&lt;/strong&gt; 發現某一些層的 input embedding 與 output embedding 會達到一個平衡點(這邊平衡的解釋，需要去閱讀該篇 paper 才能比較了解，對此初步的了解認為在較深的 layer 時參數會達到一個平穩的狀態，不再震盪，達到收斂)。&lt;/p&gt;
&lt;p&gt;參數共享的方法有很多種，比如說 only sharing  feed-forward network parameters、only sharing attention parameters 或是 sharing all parameters across layers &amp;hellip;等，在 ALBERT 中採用的是 sharing all parameters across layers。作者藉由計算向量相c似度的方法 L2 distances 與 cosine similarity 來衡量 input embedding 與 output embedding 在深層網路中是收斂還是震盪，如下圖：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./figure_1.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;透過圖的顯示，作者在這邊觀察到 embedding 在 BERT 中是呈現震盪變化，而採用 cross-layer parameter sharing 的 ALBERT 在深層網路中是逐漸平滑的趨勢，這樣的現象也告訴我們 weight-sharing 的方法對於穩定網路的參數有著顯著的影響。&lt;/p&gt;
&lt;p&gt;在這部分作者也提到說，僅管的 BERT 相比，有明顯的下降震盪的狀況，但即使經過了 24 層，也不會收斂到 0 (也就是 input embedding 與 output embedding 完全相似)，對比前面提到得 DQE 所找到的解決方案有很大的不同。(這 DQE 感覺很神奇啊！需要好好來閱讀一番)&lt;/p&gt;
&lt;h3 id=&#34;task-change&#34;&gt;Task change&lt;/h3&gt;
&lt;p&gt;BERT 的預訓練有兩個項目，一個是 masked language modeling (MLM)，另一項是 Next-sentence prediction (NSP)，在 ALBERT 的預訓練任務也參考了 BERT 的任務，採用了MLM 作為訓練任務之一，但另一項任務並非使用 NSP，而是採用了新的預訓練任務稱為 Sentence ordering objectives (SOP)，接下來將會解說兩者的差異，為什麼 ALBERT 要更換預訓練任務。&lt;/p&gt;
&lt;h4 id=&#34;sentence-ordering-objectives&#34;&gt;Sentence ordering objectives&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Next-sentence prediction (NSP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目的：學習語句之間的關係，預測第二個句子是否為上一個句的下一句，為一個二分類的訓練&lt;/li&gt;
&lt;li&gt;Targets 產生的方式:
&lt;ul&gt;
&lt;li&gt;是下一句(positive) $\rightarrow$ 同一文檔內的連續句子&lt;/li&gt;
&lt;li&gt;不是下一句(negative) $\rightarrow$ 不同文檔的句子組合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Positive 與 Negative 資料比例各站 50 %&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sentence ordering objectives (SOP)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目的：與 NSP 一樣，都是學習語句之間的關係，預測第二句子是否為上一句的下一句&lt;/li&gt;
&lt;li&gt;Targets 產生的方式:
&lt;ul&gt;
&lt;li&gt;是下一句(positive) $\rightarrow$ 同一文檔內的連續句子&lt;/li&gt;
&lt;li&gt;不是下一句(negative) $\rightarrow$ &lt;strong&gt;同一文檔內的連續句子，但是順序對調&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Positive 與 Negative 資料比例各站 50 %&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;從上面的介紹可以明顯地發現兩個任務的差異在於 negative 樣本的創造方式，而為什麼要這樣去修改呢？&lt;/p&gt;
&lt;p&gt;其實在 Liu et al., 2019&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; 中就有提到 NSP 對於 downstream tasks 的表現會有影響，所以在 RoBERTa 的預訓練任務中將其剔除(這部分值得深讀一下 RoBERTa)。而作者在這邊認為在學習句子之間的關係是很注重 &lt;code&gt;語句的連貫性(coherence)與銜接性(cohesion)&lt;/code&gt; ，假設說前一句與後一句所要描述的東西大不相同，那麼別說是機器，人類也很難理解想要表達的意思。&lt;/p&gt;
&lt;p&gt;此外，NSP 的學習融合了兩個主軸，一個是 &lt;strong&gt;topic prediction&lt;/strong&gt;，一個是 &lt;strong&gt;coherence prediction&lt;/strong&gt;，由上述的 NSP 資料，其實可以了解到，在不同文檔的句子主組合會很容易學習到 topic prediction，因為所講的東西完全不同，但卻很難學習到 coherence prediction，而這樣的情況其實與 MLM 的學習有同疊到，所以作者認為語句組合的建構是語言理解中很重要的一部分，所以採用了 SOP，避免產生 topic prediction 的問題，專注於學習 inter-sentence coherence，並且也提出了基於 coherence 的 loss，&lt;strong&gt;Inter-sentence coherence loss&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;
&lt;p&gt;講解完 ALBERT 的主要優化與改變的地方後，接下來看看作者對於 ALBERT 與其他模型的各種實驗比較。&lt;/p&gt;
&lt;h3 id=&#34;model-setup&#34;&gt;Model Setup&lt;/h3&gt;
&lt;p&gt;Tabel 1 列出 ALBERT 與 BERT 的對於模型框架不同的參數設定，在 paper 中的實驗結果大多以 12 層的網路架構為主，因為在相同配置下採用 24 層的網路架構所獲得的結果是相似的，但計算成本卻比較高。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_1.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;experimental-setup&#34;&gt;Experimental Setup&lt;/h3&gt;
&lt;p&gt;實驗設定方面，為了確保比較結果是具有可參考性的，這邊作者的設定是依據 BERT 所採用的 BOOK CORPUS (Zhu et al., 2015)8 與 English Wikipedia (Devlin et al., 2019)9 來作為預訓練任務資料集，總共大約 16 GB 的資料量。Input length 最大長度限制為 512， 並以 10% 的機率隨機產生長度小於 512 的語句，也與 BERT 依樣， vocabulary size 也是 30,000 個字詞，並且使用 SentencePiece (Kudo &amp;amp; Richardson, 2018)&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;作為斷詞的方法，XLNet 也有同樣有採用這個斷詞方法。&lt;/p&gt;
&lt;p&gt;對於 MLM 任務的輸入，BERT 採用的是 random masking 的方式，對 15% 的&lt;strong&gt;詞&lt;/strong&gt;作 mask，而 ALBET 採用 n-gram masking (Joshi et al., 2019)&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;的方式，$n$ 最大取 3，相對的比較保有語意的訊息。n- gram  masking 的機率如下：&lt;/p&gt;
&lt;p&gt;$$
p\left(n\right) = \frac{1/n}{\sum_{k=1}^{N} 1/k}
$$&lt;/p&gt;
&lt;p&gt;當 n 越大，被選擇 mask 的機率就相對較低，例如：1-gram 的機率為  6/11，2-gram 的機率 3/11，3-gram 的機率  2/11 這樣。&lt;/p&gt;
&lt;p&gt;其他參數設定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Batch size: 4096&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimizer: LAMB with learning rate 0.00176&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train steps: 125,000 steps&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Device: Gloud TPU V3&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上的設定都是用於實驗中的所有模型。&lt;/p&gt;
&lt;h3 id=&#34;comparison-between-bert-and-albert&#34;&gt;Comparison between BERT and ALBERT&lt;/h3&gt;
&lt;p&gt;底下展示 BERT 與 ALBERT 在不同參數下的模型表現，作者在文中將 BERT-large 當作比較的 baseline，ALBERT-xxlarge 在參數量上只有 BERT-large 的 70%，而且各項向下游任務都表現得較好，雖然訓練速度上慢了有約 3 倍，但仍顯示出 ALBERT 的設計方式對模型的表現有很大的改善。不過這邊有一點要注意的是這邊比較是訓練的時間，如果是在推論的狀況下，速度上應該是不會有太大的差異。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_2.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;factorized-embedding-parameterization&#34;&gt;Factorized Embedding Parameterization&lt;/h3&gt;
&lt;p&gt;Table 3 進行了不同 vocabulary embedding size 的設定對模型的節果影響，採用的模型為 ALBERT-base，其 configurations 的設定如 Table 1 所提的一樣，實驗的設定採用 not-shared parameters(BERT-style) 與 all-shared parameters(ALBERT-style) 的比較，從結果可以看得出 $E$  的大小對於實驗結果並不一定帶來正相關的反應，因為在後續模型在使用 shared parameters 的條件下 $E$ 都是設為 128，因為在 shared parameters 的條件下 $E = 128$  表現最好 。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_3.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;cross-layer-parameter-sharing&#34;&gt;Cross-layer parameter sharing&lt;/h3&gt;
&lt;p&gt;在這一部分的實驗，採用幾個不同的參數分享策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All-shared&lt;/li&gt;
&lt;li&gt;Not-shared&lt;/li&gt;
&lt;li&gt;Only the attention parameters arr shared&lt;/li&gt;
&lt;li&gt;Only the FFN parameters arr shared&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_4.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
在 paper 中作者還有提及另外一種參數分享策略，就是將模型的堆疊層數區分為 $N$ 組，每一組的大小為 $M$ 層做參數共享，實驗結果顯示當 $M$ 越小，其表現越好，但相對的較低 $M$ 的大小，也提升了整體參數的數量。
&lt;h3 id=&#34;sentence-order-prediction-sop&#34;&gt;Sentence Order Prediction (SOP)&lt;/h3&gt;
&lt;p&gt;Table 5 展示了在 &lt;code&gt;sentence-prediction loss&lt;/code&gt; 的影響下，對於固有任務(Intrinsic Tasks)與下游任務(Downstream Tasks)的模型表現：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_5.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
由上表可以看到在固有任務的表現，NSP 的訓練效果在 SOP 的預測上與隨機預測沒什麼差異，可以說是完全預測不出來，相反 SOP 的訓練下，在 NSP 的表現依然非常好(78.9% 的準確度)，由此可以確認 NSP 的任務學習只有對於 topic prediction 的部分有效。另外對於下游任務的表現也是基於 SOP 的訓練模型表現較好。
&lt;h3 id=&#34;train-with-same-amount-of-time&#34;&gt;Train with same amount of time&lt;/h3&gt;
&lt;p&gt;前面 Table 2 比較了在相同的訓練下 steps 下的模型表現差異，作者還比較了在同樣的訓練時間下模型的表現情況，如 Table 6 的結果，雖然 ALBERT-xxlarge 在差不多的同樣訓練時間內走的步數不如 BERT-large，但是在表現上卻優於  BERT-large，所以並非是訓練的步數越多或是訓練的時間越久就能夠得到更好的結果。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_6.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;additional-training-data-and-dropout-effects&#34;&gt;Additional training data and dropout effects&lt;/h3&gt;
&lt;p&gt;借鏡 XLNet (Yang et al., 2019)&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; 與 RoBERTa (Liu et al., 2019)&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; 都有透過增加而外的資料來提升模型的表現，作者也比較了增加額外的資料的結果。如下表 Table 7 結果，看起來並非全部的下游任務都有改善，除了 SQuAD 以外的任務都有提升。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_7.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;下圖展示了在採用額外資料與否的條件下，對於 MLM 任務有明顯的差異。此外也比較了 dropout 的效用，在訓練超過 1M 步後，模型仍然沒有發生 overfitting 的問題，所以直接刪除掉 dropout，比較結果於 Table 8。其實在 Szegedy et al., 2017&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt; 的實驗與 Li et al., 2019&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt; 就已經證明結合 batch normalization 與 dropout 會有害 Convolutional Neural Networks 的結果表現。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./figure_2.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_8.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;實驗的部分，作者還有測試了不同的 NLU tasks 的比較，這邊就留給讀者自己去看看，不再贅述。個人覺得在 Appenidx 的部分，值得好好看一下。&lt;/p&gt;
&lt;h3 id=&#34;effect-on-network-depth-and-width&#34;&gt;Effect on network depth and width&lt;/h3&gt;
&lt;p&gt;在 Appenidx 的部分，作者對於網路的寬度與深度進行了實驗，模型採用的是 ALBERT-large。實驗採用的方法是在前 2 層做 trained，而當超過 3 層(包含第 3 層)的深度網路則採用 fine-tuning 的方式來訓練，底下 Table 11 可以得知，在相同的參數下，當層數的堆疊從 $1 \rightarrow 3$ 層時，模型的表現相對提升；但是當層數堆疊持續增加時，模型的提升效益就開始減緩，可以看當層數從 $12 \rightarrow 24 \rightarrow 48$ 的模型表現，24 與 48 的表現已經差不多了，所以再堆疊上去對模型而言並不會有太多的提升。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_11.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
比較玩深度的狀況，來比較寬度的狀況，模型採用的是 3 層堆疊的 ALBERT-large。當增加 hidden size 的時候，也發生與深度堆疊的狀況，當增加到一定的程度後，效果開始減緩，寬度網路甚至還下降。
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_12.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;在 Tabel 11 比較了深度的影響，但是在 ALBERT-large 的 hidden size 為 1024，如果是更寬更深的堆疊網路層，是否能夠得到更好的效果。作者對此提出了疑問&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Do very wide ALBERT models need to be deep(er) too?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;採用 hidden size 為 4096 的 ALBERT-xxlarge 來進行實驗，在 Table 13 可以發現效果其實差不多，因此認為對於 ALBERT 來說不需要太深層的網路，只需要 12層就足夠了。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_13.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;這篇 paper 其實讀起來不難，只要有了解過 BERT 再來閱讀後，可以很快速地了解優化的部分，而且其優化的方法也並非創新，在其他的 paper 都有被討論過，比較新的部分，應該是預訓練任務的替換，SOP 的訓練確實帶來很大的改善，此外在 ERNIE 推出一個新的預訓練任務叫說 Sentence Reordering Task (SRT)，是對於學習句子的關聯性更加強的預訓練任務，可以好好看一下。另外，paper 中也不斷地提及 XLNet 與 RoBERTa 對於 ALBERT 的啟發與影響，所以這兩邊也值得好好閱讀一翻，有時間再來寫一下。&lt;/p&gt;
&lt;p&gt;題外話，在大幅減少參數的情況下，其實不只是訓練速度加快，另外來為平行運算的方式帶來很大的優化就是傳輸量的減少，我們都知道在做 multi-gpu 運算所採用到策略有 Central Storage Strategy 與 Parameter Server Startegy 兩種，前者需要將 gradient 的結果回傳到 Master 做全局的 gradient update，而後者則是各個 worker 之間互相溝通傳遞參數，直到所有 worker 都 update 完畢，所以減少了參數量，相對的在傳輸部分也縮短了時間。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Self Supervised Representation Learning in NLP, Amit Chaudhary, &lt;a href=&#34;https://amitness.com/2020/05/self-supervised-learning-nlp/&#34;&gt;https://amitness.com/2020/05/self-supervised-learning-nlp/&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017 &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Neural Information Processing Systems (NeurIPS), 2019. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL &lt;a href=&#34;https://www.aclweb.org/anthology/D18-2012&#34;&gt;https://www.aclweb.org/anthology/D18-2012&lt;/a&gt;. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529, 2019. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017. &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropout and batch normalization by variance shift. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2682–2690, 2019. &lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>[Paper-NLP] Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</title>
      <link>https://roymondliao.github.io/post/word_segmentation_in_chinese/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/word_segmentation_in_chinese/</guid>
      <description>&lt;h1 id=&#34;chinese-word-segmentation-cws&#34;&gt;Chinese word segmentation (CWS)&lt;/h1&gt;
&lt;p&gt;NLP 領域在近年的突破性發展，各項研究應用與相關論文不斷的推層出新，對於如何讓機器能更了解文章、句子所表達的內容，進而解決各項在 NLP 領域上的問題，在 &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt; 近期推出的 &lt;a href=&#34;https://github.com/openai/gpt-3&#34;&gt;GPT-3&lt;/a&gt; 已經將 NLP 研究推向高峰，但當中還是有一些 NLP 的相關議題是比較少被拿來討論的，尤其是對於&lt;strong&gt;中文&lt;/strong&gt;的處理，大部分的 NLP 都還是環繞在英文為主，畢竟大部分的新方法都是由歐美的學者所提出。&lt;/p&gt;
&lt;p&gt;字詞的&lt;strong&gt;斷詞方法&lt;/strong&gt;一直都是 NLP 相關任務的重點，尤其是目前為主流的 pretrain model 研究發展走向，好的斷詞可讓機器理解整句話所要表達的意思也可以推論語句的結構，反之不好的斷詞可能會讓機器理解錯誤也連帶影響後續的下游任務表現。&lt;/p&gt;
&lt;p&gt;在本次要節錄的論文是由&lt;a href=&#34;https://www.shannonai.com/&#34;&gt;香儂科技(Shannon AI)&lt;/a&gt; 發表在ACL上的一篇論文，主要是探討基於深度學習的中文 NLP 任務中對於中文字斷詞的處理方式，是要以&lt;strong&gt;詞(word-base)&lt;/strong&gt; 還是&lt;strong&gt;字(char-base)&lt;/strong&gt; 的處理方式比較。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Word-base 的處理方式有幾項缺點：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;由於字詞的組合過多，且有些詞出現的頻濾可能相當低，所以容易造成資料過於稀疏的狀況，所以無法避免 &lt;strong&gt;out-of-vocabulary(OOV)&lt;/strong&gt; 的問題產生，進而限制了模型的學習能力，根據 &lt;a href=&#34;https://en.wikipedia.org/wiki/Zipf%27s_law&#34;&gt;Zipf&amp;rsquo;s law&lt;/a&gt; 其表示詞的出現的頻率與語料中的排名成反比。&lt;/p&gt;
&lt;p&gt;在論文中以 Chinese Treebank dataset(CTB) 為例，使用 &lt;a href=&#34;https://github.com/fxsjy/jieba&#34;&gt;Jieba&lt;/a&gt; 作為斷詞的處理方式，最終總共有 615,194 個字詞，當中包含了 50,266 個不重複詞，而在其中又只有 24,458 個字詞只出現過一次，佔比 48.7%，對於整體字詞來說佔了 4% 。如果將字詞出現的次數提高到4 次以下來觀察，就佔了詞的 77.4%，佔整體字詞約 10.1%。其統計結果如下圖：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_1.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;由統計結果可以得知 word base 的處理在資料上是非常稀疏的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;資料稀疏(data sparsity)&lt;/strong&gt; 的問題，會造成 &lt;strong&gt;model overfitting&lt;/strong&gt; 而且也會產生大量的參數，所以如果要維護一個龐大的字詞語料庫，可想而知在現實狀況的處理上這是不洽當的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在斷詞上的錯誤會連帶給下游任務帶來 bias，尤其中文在斷詞的處理上有許多模糊的邊界地帶。例如：部分居民生活水平，正確的切分為 &lt;code&gt;部分/居民/生活/水平&lt;/code&gt;，但也被切成 &lt;code&gt; 部/分居/民生/活/水平&lt;/code&gt; 的狀況&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，在 neural network model 流行之前，&lt;code&gt;斷詞的必要性&lt;/code&gt;就已經有被拿來討論，是否斷詞能帶來模型的效能提升。在 2004 年 Schubert Foo and Hui Li.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; 討論在 Information retrieval(IR) 系統上 CWS 的影響，研究顯示在文本搜尋上使用 CWS 的 model 表現並不是每次都比沒有使用 CWS 的模型來的好。另外在 Xu et al. (2004)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; 、Zhao et al. (2013)&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; 、Liu et al. (2007)&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; 都提出了使用 CWS 的處理，並沒有能夠有效的提升模型的效能。&lt;/p&gt;
&lt;p&gt;綜合以上所提出的論點，本篇論文就以下面這句話來作為主軸，來探討其必要性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Whether word segmentation is necessary for deep learning-based Chinese natural language processing.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其實在 Yin et al., 2016&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;; Dong et al., 2016&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;; Yu et al., 2017&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; 都證明了基於 char-base 的處理方式對於模型的效能提升好過於使用 word-base，而且也比較了採用 char-base 與 word-base 混合的處理方式，也並沒有優於 char-base 的表現。&lt;/p&gt;
&lt;h2 id=&#34;analysis--experimental-results&#34;&gt;Analysis &amp;amp; Experimental Results&lt;/h2&gt;
&lt;p&gt;論文中將對四項任務進行 word-base 與 char-base 的比較分析。&lt;/p&gt;
&lt;h4 id=&#34;1-language-modeling&#34;&gt;1. Language modeling&lt;/h4&gt;
&lt;p&gt;Detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data
&lt;ul&gt;
&lt;li&gt;資料使用 Chinese Tree-Bank 6.0 (CTB6) 作為訓練資料，並切分 80%、10%、10% 分為 training、validation 與 testing set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model
&lt;ul&gt;
&lt;li&gt;文字的斷詞採用 Jieba 處理&lt;/li&gt;
&lt;li&gt;模型使用 LSTM，hyper-parameters 的 tuning 採用 grid search 的方式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其實驗結果如下：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_3.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;由上表可得知，在 language modeling 任務中 char-base 的表現遠優於其他的處理方式。另外作者也嘗試採用不同的套件 CWS package (Monroe et al., 2014)&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; 與 LTP package (Che et al., 2010)&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt; 對字詞做斷詞的處理，得到的結果與上述雷同。當中 hybird(char only) 的處理方式，是一個先用 word-base 的斷詞，然後在 embedding 的使用則是將 word 的組成拆成 char-base 作為輸入。&lt;/p&gt;
&lt;h4 id=&#34;2-machine-translation&#34;&gt;2. Machine Translation&lt;/h4&gt;
&lt;p&gt;Detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data
&lt;ul&gt;
&lt;li&gt;Training set: 使用 &lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;LDC corpora.&lt;/a&gt; 的 CH-EN/EN-CH當作訓練資料，當中包含 1.25M 的中翻英語句&lt;/li&gt;
&lt;li&gt;Validation set: &lt;a href=&#34;https://www.nist.gov/&#34;&gt;NIST&lt;/a&gt; 2002&lt;/li&gt;
&lt;li&gt;Testing set: NIST 2003, 2004, 2005, 2006 與 2008&lt;/li&gt;
&lt;li&gt;使用 top 30,000 個英文字與 27,500 的中文當作詞庫；對於 char-base 的部分，vocab size 只有 4,500 個字&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model
&lt;ul&gt;
&lt;li&gt;模型使用 SEQ2SEQ + Attention (Sutskever et al., 2014&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;; Luong et al., 2015&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;)&lt;/li&gt;
&lt;li&gt;採用 bag-of-words 的機制 Ma et al. (2018)&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;使用 BLEU 做為評分指標&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分別對 CH-EN/EN-CH 進行實驗，其結果如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CH-EN&lt;/strong&gt; 的實驗部分， char-based model 的表現平均高出 0.83 分，採用了 bag-of-words 方法後，雖然兩種處理方式整體表現都往上提升，但是 char-based model 還是高出 0.63 分的表現。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_4.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;EN-CH&lt;/strong&gt; 的實驗部分，char-based model 的表現則遠遠優於 word-based model，平均整整高出了 3.13 分。對比 &lt;strong&gt;CH-EN&lt;/strong&gt; 的實驗結果，會有如此大的差距，是因為在 CH-EN 的斷詞處理只有在 source 部分，而對於 EH-CH 的斷詞處理則在 source 與 target 都有。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_5.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;h4 id=&#34;3-sentence-matchingparaphrase&#34;&gt;3. Sentence Matching/Paraphrase&lt;/h4&gt;
&lt;p&gt;Detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/D18-1536/&#34;&gt;BQ dataset&lt;/a&gt;，包含 120,000 個中文語句配對，label 為兩個語句是否表達相似意思&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/C18-1166/&#34;&gt;LCQMC&lt;/a&gt; 與 BQ 的資料相似，一樣是語句的配對，但差異在於，不同的語意，但其背後想表達的意思是雷同的，例如：&lt;code&gt;My phone is lost&lt;/code&gt; 與 &lt;code&gt;I need a new phone&lt;/code&gt;的組合，表達不同的意思，但其隱含意思都是 &lt;code&gt;buying a new phone.&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文字的斷詞採用 Jieba 處理&lt;/li&gt;
&lt;li&gt;SOTA model: bilateral multiperspective matching model (BiMPM) (Wang et al., 2017&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_6.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;同樣的從實驗結果可以得知，在 char-based model 的表現優於 word-based model。&lt;/p&gt;
&lt;h4 id=&#34;4-text-classiﬁcation&#34;&gt;4. Text Classiﬁcation&lt;/h4&gt;
&lt;p&gt;Detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data
&lt;ul&gt;
&lt;li&gt;ChinaNews：包含7種不同類別的新聞文章&lt;/li&gt;
&lt;li&gt;Ifeng：只有新聞文章的第一個段落，包含5種不同類別&lt;/li&gt;
&lt;li&gt;JD_Full：從 JD.com 爬蟲下來的資料，為商品評價資訊，用戶對商品進行 1-5 分的評分，為多分類任務&lt;/li&gt;
&lt;li&gt;JD_binary：與 JD_Full 相同，但作者將 1-2 分分為 &lt;code&gt;negative&lt;/code&gt;、4-5 分分為 &lt;code&gt;positive&lt;/code&gt;、3 分則不參考，為二元分類任務&lt;/li&gt;
&lt;li&gt;Dianping：中式餐廳的用戶評價，1-3 分為 &lt;code&gt;negative&lt;/code&gt;、4-5 分為 &lt;code&gt;positive&lt;/code&gt;，資料從 Dazhong Dianping 上爬下來的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model
&lt;ul&gt;
&lt;li&gt;Bi-directional LSTM model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_7.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;實驗結果當中比較特別是在 &lt;code&gt;chinanews&lt;/code&gt; 這個 database 的結果是 word-based model 表現略好於 char-based model，其餘的部分還是跟其他任務相同，char-based model 表現較好。&lt;/p&gt;
&lt;h4 id=&#34;5-domain-adaptation-ability&#34;&gt;5. Domain Adaptation Ability&lt;/h4&gt;
&lt;p&gt;在進行完上述的實驗後，作者認為由於資料稀疏的問題，所以提出了以下的假設&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We hypothesize that char-based models have greater domain adaptation ability than word-based models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Detail:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data
&lt;ul&gt;
&lt;li&gt;train_dianping_test_jd： 訓練集來源為 Dianping(2M)，測試集來源為 JD_binary(0.25M)&lt;/li&gt;
&lt;li&gt;train_jb_test_dianping： 訓練集來源為 JD_binary，測試集來源為 Dianping&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;實驗結果如下：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./table_8.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;透過交換訓練集與測試集的實驗方式，可以得知 char-base 在領域適應(domain adaptation)上的表現比 word-base 來得要好。其中在 &lt;code&gt;train_dianping_test_jd&lt;/code&gt; 的訓練組合上也顯示出 test sentences 在 word-base  OOV 的比例佔了 11.79% ，而在 char-base 只佔了 0.56%，表示出 OOV 問題影響了 model 的表現，但在 &lt;code&gt;train_jd_test_dianping&lt;/code&gt; 卻是相反的結果，這邊認為有可能是作者寫反了。&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;透過各項的實驗結果來看，可以了解 word-based model 表現比較差的原因如下：&lt;/p&gt;
&lt;h3 id=&#34;data-sparsity&#34;&gt;Data Sparsity&lt;/h3&gt;
&lt;p&gt;為了避免 vocabulary 的資料量過大，通常會對字詞出現的頻次設定一個 threshold ，如果字詞出現的頻次低於 threshold 的門檻，其 token 就用 &lt;code&gt;UNK&lt;/code&gt; 來顯示，下圖表示了詞表大小與 threshold 設定的關係以及 threshold 的設定與 model 的表現：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./figure_2.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;從上圖的關係可以得知，當字詞頻次的 threshold 的值越高，model 的表現就會越來越接近，反之 threshold 越小，model 的表現越差。在這樣的結果下作者認為如果字詞出現的頻次太小，不管是 word/char-base 都難以學到語句的意義，就像人類對於不常看見的文字也是缺乏理解。(不過這邊 char-base 的表現並不是那麼的明顯)&lt;/p&gt;
&lt;p&gt;此外，在實驗中 char-based model 最好的結果是 &lt;code&gt;threshold=5&lt;/code&gt;，詞表大小為 1432，字詞的頻次的中位數為 72，而 word-based model  最好的結果是 &lt;code&gt;threshold=50&lt;/code&gt;，詞表大小為 1355，字詞的頻次的中位數為 83，可以發現兩個方法最好模型的結果所對應的詞表大小與字詞頻次中位數的數值都差不多，也就是說，為了讓 word/char-based model 能夠學習到更好的語義訊息，需要讓每個 word/char 能夠有足夠的曝光(表示在詞表中詞出現的頻次需求)，word-based model 由於詞表數據過多，特徵過多，且部分字詞出現的頻次不足，所以表現較差。&lt;/p&gt;
&lt;h3 id=&#34;out-of-vocabualary-words&#34;&gt;Out-of-Vocabualary Words&lt;/h3&gt;
&lt;p&gt;Word-based model 表現得不好的其中一個原因就是包含太多 OOV，但是在前面 &lt;strong&gt;data sparsity&lt;/strong&gt; 的部分也說明到，如果降低字詞頻次的門檻，那將會導致稀疏程度的增加，而影響模型的表現。於是作者進行了一項實驗，對於不同的 threshold，將包含OOV 的語句從 training、validation 與 testing 資料集中移除，下圖顯示實驗的結果：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./figure_4.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;可以看見當增加字詞出現的頻次後，兩個模型之間的差距正在逐漸地縮小；持續將 threshold 調高，word-based model 的表現持續往上提高，就算調高 50 也是持續上升，這就表示 word-based model 可以透過減少 OOV 語句數量來減緩模型的表現較差的狀況，不過這邊這樣的處理方式在現實的環境上是不太可能，除非在建構 hybird system 有專門的模型或是方法來處理 OOV 的句子。&lt;/p&gt;
&lt;h3 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Data sparsity&lt;/strong&gt; 的問題導致 word-based model 需要更多的參數來學習，因此更容易的 &lt;strong&gt;overfitting&lt;/strong&gt;。根據這個假設作者透過 BQ 的資料來進行實驗，其實驗結果如下：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./figure_1.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;為了達到最好的結果，word-based model 的 dropout 設置須為 0.5，而 char-based model 只需要設置為 0.5，這意味著 overfitting 對於 word-based model 來說是一個嚴重的問題。另外作者還注意到在不同的 dropout 設定上，word-based model 的表現都相當的接近，但 char-based model 卻有較明顯的差異，這顯示出 dropout 的方式並不足夠能解決 overfitting 的問題。&lt;/p&gt;
&lt;p&gt;最後藉由 BQ 資料集的資料展現在不同斷詞方法中模型對於語句 attention 的情況，如下圖：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./figure_3.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://arxiv.org/abs/1905.05526&#34;&gt;Paper&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;對於在論文的一開始作者給出的問題，從各項實驗的結果來評斷，char-based model 的表現始終優於 word-based model；而 word-based model 表現不好的原因在於字詞稀疏的問題，連帶導致 OOV、overfitting、lack of domain generalization ability 的狀況產生。&lt;/p&gt;
&lt;h1 id=&#34;other&#34;&gt;Other&lt;/h1&gt;
&lt;p&gt;雖然本篇的結論表示了 char-based model 的表現優於 word-based model，藉由&lt;strong&gt;苏剑林&lt;/strong&gt;大神的部落格文章 &lt;a href=&#34;https://kexue.fm/archives/7758&#34;&gt;提速不掉点：基于词颗粒度的中文WoBERT&lt;/a&gt; 這篇得知，近期&lt;a href=&#34;https://www.hit.edu.cn/&#34;&gt;哈爾濱工業大學&lt;/a&gt;開源了以 word-base 為主的 BERT 模型，並且確保了模型效果，稱之為 &lt;a href=&#34;https://github.com/ZhuiyiTechnology/WoBERT&#34;&gt;&lt;strong&gt;WoBERT&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;其切入角度以目前主流的 pretrain model 的方式來處理 word-base model 表現不好的問題，為何這麼說呢？其原因是在目前的 NLP 任務上，大多都採用 pretrain model 為基底，之後再做 fine-tuning 或是 transfer learning 的處理，所以在 embedding layer 的部分並不會是以隨機初始化的情況下去做建模，想當然爾如果是從頭開訓練，word-base model 的 embedding layer 參數量一定更多，自然就容易 overfitting，模型的效果也會變差。所以，在基於這樣的問題狀況下，採用 pretrain model 預訓練好的詞項量，之後再進行 word-base 的處理，就會是比較好的手段。&lt;/p&gt;
&lt;p&gt;文章中說明了 word 與 char 的優點：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Char base
&lt;ul&gt;
&lt;li&gt;模型參數量較少，不容易 overfitting&lt;/li&gt;
&lt;li&gt;不依賴斷詞的算法，避免斷詞邊界的模糊地帶&lt;/li&gt;
&lt;li&gt;資料稀疏程度不嚴重，較少出現 OOV 的狀況&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Word base
&lt;ul&gt;
&lt;li&gt;序列較短，處理速度較快&lt;/li&gt;
&lt;li&gt;在文本生成任務上能夠降低 &lt;a href=&#34;(https://kexue.fm/archives/7259)&#34;&gt;Expousure Bias&lt;/a&gt; 問題
&lt;ul&gt;
&lt;li&gt;由於序列較短，所以 Expousure bias 的問題並不明顯&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;詞義的不確性較低，降低建模複雜度
&lt;ul&gt;
&lt;li&gt;雖然有多義詞的存在，但多數多義詞的意義都是比較明確的，比起字義更加明確，這樣只需要一個 embedding 就能把詞表達準確，而不用像 char-base model 需要多層膜型才能把自組合成詞&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外一些在 char-base 為優點的，在 word-base 被視為缺點的，可以通過一些技巧來處理，例如：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;參數量過多，但可以通過預訓練的模型來緩解 overfitting 的問題&lt;/li&gt;
&lt;li&gt;Word-base 依賴斷詞演算法，但如果只保留最常見的一部詞(出現頻次較高的)，不管哪種斷詞演算法其結果都是差不多的，差異性不大&lt;/li&gt;
&lt;li&gt;斷詞邊界的模糊地帶，這較難以避免，但除非是序列標類任務，否則文本分類、文本生成都不太需要準確的邊界&lt;/li&gt;
&lt;li&gt;可以把常出現的用字加入到詞表中，避免 OOV 的問題，這點李宏毅也有提到，中文常見的用字大約 3000 個左右&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;wobert&#34;&gt;WoBERT&lt;/h2&gt;
&lt;p&gt;概念相當簡單，在 &lt;a href=&#34;https://github.com/ymcui/Chinese-BERT-wwm&#34;&gt;RoBERTa-wwm-ext&lt;/a&gt; 的基礎上再進行預訓練，預訓練任務為 MLM。在詞的 embeding layer 初始化階段，將每個詞用 BERT 所提供的 tokenizer 切分為字，然後在用字所得到的 embedding 取平均作為詞的 embedding 初始化，剩下的就是訓練模型。模型的訓練結果可以直接參考一開始所提供的文章連結，裡面有詳細的訓練結果展示。&lt;/p&gt;
&lt;p&gt;此外在今年 ACL 2020 的會議，&lt;a href=&#34;https://www.chuangxin.com/&#34;&gt;創新工場&lt;/a&gt;有兩篇關於中文斷詞方法的論文入選，分別是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.734/&#34;&gt;Improving Chinese Word Segmentation with Wordhood Memory Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.735/&#34;&gt;Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果不想直接看論文也可以看看機器之心所撰寫的簡介&lt;a href=&#34;https://www.jiqizhixin.com/articles/2020-07-09-2&#34;&gt;创新工场两篇论文入选ACL 2020，将中文分词数据刷至新高&lt;/a&gt;，關於中文斷詞的分享就到這邊結束。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Schubert Foo and Hui Li. 2004. Chinese word segmentation and its effect on information retrieval. Information processing &amp;amp; management, 40(1):161–190. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang. 2018. Lcqmc: A large-scale chinese question matching corpus. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1952–1962. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Hai Zhao, Masao Utiyama, Eiichiro Sumita, and BaoLiang Lu. 2013. An empirical study on word segmentation for chinese machine translation. In International Conference on Intelligent Text Processing and Computational Linguistics, pages 248–263. Springer. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Wei Liu, Ben Allison, David Guthrie, and Louise Guthrie. 2007. Chinese text classiﬁcation without automatic word segmentation. In Sixth International Conference on Advanced Language Processing and Web Information Technology (ALPIT 2007), pages 45–50. IEEE. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Rongchao Yin, Quan Wang, Peng Li, Rui Li, and Bin Wang. 2016. Multi-granularity chinese word embedding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 981–986. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Chuanhai Dong, Jiajun Zhang, Chengqing Zong, Masanori Hattori, and Hui Di. 2016. Characterbased lstm-crf with radical-level features for chinese named entity recognition. In Natural Language Understanding and Intelligent Applications, pages 239250. Springer. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Jinxing Yu, Xun Jian, Hao Xin, and Yangqiu Song. 2017. Joint embeddings of chinese words, characters, and ﬁne-grained subcharacter components. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 286–291. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Will Monroe, Spence Green, and Christopher D Manning. 2014. Word segmentation of informal arabic with domain adaptation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 206–211. &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:A chinese language technology platform. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 13–16. Association for Computational Linguistics. &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112. &lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attentionbased neural machine translation. ACL. &lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Shuming Ma, Xu Sun, Yizhong Wang, and Junyang Lin. 2018. Bag-of-words as target for neural machine translation. arXiv preprint arXiv:1805.04871. &lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.Bilateral multi-perspective matching for natural language sentences. IJCAI. &lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Transformer Part 3 - Transformer</title>
      <link>https://roymondliao.github.io/post/transformer_part3/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/transformer_part3/</guid>
      <description>&lt;h1 id=&#34;transformer&#34;&gt;Transformer&lt;/h1&gt;
&lt;p&gt;拖延了很久，終於有時間拉把欠的部分補齊了，來講在 NLP 已經廣為人知的 Transformer。在開始介紹前先來附上 transformer 的結構，如下圖：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt; 
&lt;center&gt;
  &lt;img src=&#34;./transformer_inside_detail.png&#34; style=&#34;zoom:60%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&#34;&gt;Lil&#39;s Log&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
在這裡我會把 transformer 拆解如下面幾個主題來解說：
&lt;ol&gt;
&lt;li&gt;Embedding and Softmax&lt;/li&gt;
&lt;li&gt;Positional Encoding&lt;/li&gt;
&lt;li&gt;Scaled dot-product attention&lt;/li&gt;
&lt;li&gt;Multi-head attention&lt;/li&gt;
&lt;li&gt;Postition-wise Feed-Forward Networks&lt;/li&gt;
&lt;li&gt;Encoder &amp;amp; Decoder&lt;/li&gt;
&lt;li&gt;Optimizeer&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;embedding-and-softmax&#34;&gt;Embedding and Softmax&lt;/h2&gt;
&lt;p&gt;Embedding and softmax 這個部分應該比較少看到有人在文章中提出，其實光看字面的意思應該不難理解主要要做些什麼事情，但在論文中提到的主要差異點是說，在模型中在 embedding layer 與 pre-softmax linear transformation layer 使用共享參數矩陣的方式，而在 embedding layer 乘上一個縮放權重 $\sqrt{d_{model}}$，共享參數矩陣的想法可以參考 &lt;a href=&#34;https://arxiv.org/pdf/1608.05859.pdf&#34;&gt;Using the Output Embedding to Improve Language Models&lt;/a&gt; 篇所提出的概念&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;補充說明：&lt;/p&gt;
&lt;p&gt;在 transformer 中借鏡了 &lt;strong&gt;Using the Output Embedding to Improve Language Models&lt;/strong&gt; 的 &lt;code&gt;weight tying&lt;/code&gt; 的想法，把 embedding and pre-softmax 的 weight 共享，因為在 input 的 sequence 經過 embedding 會得到 word vector，在經過 decoder 計算後，也得到一樣維度的 vector，而在要進入 softmax 前，再乗上一個 weight (在 paper 稱這樣的 weight 為 output embedding)，這樣的操作方式有效的提升 language models。&lt;/p&gt;
&lt;p&gt;後續在 &lt;code&gt;Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling&lt;/code&gt; 與 &lt;code&gt;How to represent a word and predict it, too: improving tied architectures for language modelling&lt;/code&gt; 這兩篇 paper 也用上 weight tying 的技巧。那麼為什麼這樣的技巧有用，來自於當 model 在做 backpropagation 的時候，在 output embedding 會得到較大的受益，而在 input embedding 所得到的改善，經過模型中多層的 backpropagate 回來後，改變的 weight 的程度就不如 output embedding 來得好。&lt;/p&gt;
&lt;p&gt;在 Using the Output Embedding to Improve Language Models 這篇 paper 有提到他們最好的做法，叫做 &lt;code&gt;three-way weight tying (TWWT)&lt;/code&gt;，意思就是 input embedding (encoder)、input embedding (decoder)、output embedding (decoder) 都是共享，但如果要達到能夠都共享，在字詞的處理上就需要用到 subwoard (子字詞) 概念的處理方式，目前的處理方式有，Byte Pair Encoding (BPE)、WordPiece 與 Unigram Language Model 這三種方法。&lt;/p&gt;
&lt;p&gt;另外有一個重點，雖然 weight 是共享，但是各層的 bias 是獨立的。那為什麼 transformer 的 embedding 要乘上 $\sqrt{d_{model}}$，我的猜測可能是為了放大 embedding 空間，放讓在最後過 softmax 的時候，讓真正的答案可以得到較高的 probability。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;positional-encoding&#34;&gt;Positional Encoding&lt;/h2&gt;
&lt;p&gt;一般在做 NLP 的處理上，都會透過 word embedding 的方式將文字轉為向量的方式讓模型可讀，但在 transformer 模型中並非使用 recurrence 或是 convolution 的結構，所以在 self-attention 裡的文字訊息是沒有順序的，而為了讓 Transformer 能夠學習到文字序列中的字詞的順序關係，所以需要額外加入一些「位置資訊」給 Transformer，而 &lt;strong&gt;positional encoding&lt;/strong&gt; 就是扮演這個重要的角色。&lt;/p&gt;
&lt;p&gt;位置編碼(Positional Encoding) 實作上是直接加到最一開始的詞嵌入向量(word embedding) 裡頭，其直觀的想法是想辦法讓被加入位置編碼的詞嵌入向量在 $d_{model}$ 維度的空間裡頭不只會因為語義相近而靠近，也會因為位置靠近而在該空間裡頭靠近，所以位置編碼的維度與詞嵌入向量的維度是相同的。&lt;/p&gt;
&lt;p&gt;論文裡頭使用的位置編碼的公式如下：&lt;/p&gt;
&lt;p&gt;$$
PE_(pos, 2i) = sin\left(pos/10000^{2i/d_{model}}\right)
$$
$$
PE_(pos, 2i+1) = cos\left(pos/10000^{2i/d_{model}}\right)
$$&lt;/p&gt;
&lt;p&gt;在這裏 pos 指的是編碼的位置，$i$ 則是表示第 $i$ 個維度，就是第 n 個詞的第 i 個維度。Positional embedding 在模型中其實可以被學習的，但這是邊作者會選擇這樣的模型是因為為了使模型可以輕鬆學習相對位置，因為透過固定的公式來表達。&lt;/p&gt;
&lt;p&gt;下圖來解釋為什麼要用 sin 與 cos 的函數來處理 positional encoding:&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./positional_encoding.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646
&#34;&gt;Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;舉例來說，如果只有用單一的 sin 函數(藍色線)的話，那以 $p = 22$ 與 $p = 35$ 來說，就無法方便出這兩個詞的位置差異$(i = 100)$，但如果多增加了cos 函數(紅線) 的話，那就可以清楚的區分。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;scaled-dot-product-attention&#34;&gt;Scaled dot-product attention&lt;/h2&gt;
&lt;p&gt;接下來將會一步一步從模型的內部的細節結構慢慢往外部的結構做講解，首先要講的是 Multi-head attention 中的所使用到的注意力機制 &lt;strong&gt;scale dot-product attention&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果閱讀過之前的 &lt;a href=&#34;https://roymondliao.github.io/post/2019-12-16_transformer_part2/&#34;&gt; Transformer part2 - Attention&lt;/a&gt; 就能明白注意力機制的運作原理。這邊我取用 &lt;a href=&#34;leemeng.tw&#34;&gt;leemeng&lt;/a&gt; 在&lt;a href=&#34;https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html&#34;&gt;淺談神經機器翻譯&amp;amp;用 Transformer 與 TensorFlow 2 英翻中&lt;/a&gt; 的解說來複習一下。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注意力機制(或稱注意函式，attention function) 在概念上就是拿一個查詢(query) 去跟一組 key-values 做運算，最後產生一個輸出。只是這邊會利用矩陣運算同時讓多個查詢跟一組 key-values 做運算，最大化計算效率，而不管是查詢(query)、鍵值(keys) 還是值(values) 或是輸出，全部都是向量(vectors)。該輸出是 values 的加權平均，而每個 value 獲得的權重則是由當初 value 對應的 key 跟 query 計算匹配程度所得來的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./scale_dot_product_attention.jpg&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://youtu.be/ugWDIIOHtPA&#34;&gt;Hung-yi Lee professor - Transformer&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;用一個抽象的例子來說：
假設輸入的文字序列為 They played chess.，&amp;ldquo;They&amp;rdquo; 是 subject，&amp;ldquo;played&amp;rdquo; 是 verb，在經過 encoder 後，將這樣的訊息轉變編碼向量，假設 decoder 已經翻譯出 subject 的字詞，那接下來需要的就是 verb 詞性的文字，這就像是 encoder 創建了一個 lookup dictionary {&amp;lsquo;subject&amp;rsquo;: &amp;lsquo;They&amp;rsquo;, &amp;lsquo;verb&amp;rsquo;: &amp;lsquo;played&amp;rsquo;, …}，而 decoder 想要從這個 dictionary 中找到下一個跟 verb 有關的字詞，但是在模型裡面並不會有這樣的資訊去告訴你接下來你要找的 key 是一個 subject 或是 verb，所以在模型中，就利用要尋找這個 verb 的方式當作一個 query，而與每個在 dictionary 中 key 計算相似度，然後透過 softmax 得到相對應的 weight，如果 key 的向量表示與 query 越相似，則 weight 就越接近 1。&lt;/p&gt;
&lt;p&gt;相似度的計算公式如下：&lt;/p&gt;
&lt;p&gt;$$
Attnetion\left(Q, K, V \right) = softmax\left(\frac{QK^{T}}{\sqrt{d_{keys}}}\right)V \&lt;br&gt;
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Q$ 是一個 matrix，每一列包含 query 的資訊，維度 $(n_{queries}, d_{keys})$&lt;/li&gt;
&lt;li&gt;$K$ 是一個 matrix，每一列包含 key 的資訊，維度 $(n_{keys}, d_{keys})$&lt;/li&gt;
&lt;li&gt;$V$ 是一個 matrix，每一列包含 value 的資訊，維度 $(n_{keys}, d_{values})$&lt;/li&gt;
&lt;li&gt;$QK^T$ 維度 $(n_{queries}, n_{keys})$ 為相似度的分數(similarity score)&lt;/li&gt;
&lt;li&gt;$\sqrt{d_{keys}}$ 是一個縮放因子(scaling factors)，為了避免 softmax function 的結果過飽和，而造成 gradients 的結果很小。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;就我個人的看法是認為，如果從 attention 慢慢演化到 self-attention 的過程來看，$K, Q, V$ 其實不帶任何意義，從過往對 attention 的理解，$QK^T$ 的計算只是為了符合在 attention 中，decoder 的 output 去關注 encoder 的 output 來得到 similarity score，所以就如同 self-attention 的字面意義一樣，為了要得到類似於 attention 的結果。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;multi-head-attention&#34;&gt;Multi-head attention&lt;/h2&gt;
&lt;p&gt;在 Multo-head attention 中，會將 $Q、K$ 以及 $V$ 這三個張量先個別轉換到 $d_{model}$ 維空間，再將其拆成多個比較低維的 depth 維度 $h$ 次以後，將這些產生的 $sub \space q、sub \space k$ 以及 $sub \space v$ 分別丟入前面講解的注意函式得到 $h$ 個結果。接著將這 $h$ 個 $heads$ 的結果串接起來，最後通過一個線性轉換就能得到 multi-head attention 的輸出。如下圖所示：&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./multi_head_attention.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://www.tensorflow.org/tutorials/text/transformer#scaled_dot_product_attention&#34;&gt;Tensorflow Tutorials&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
然而為何要那麽要把本來 $d_{model}$ 維的空間投影到多個維度較小的子空間(subspace) 以後才各自進行注意力機制呢？
&lt;p&gt;這是因為這樣的處理方式給予模型更大的彈性，讓它可以同時關注不同位置的子詞在不同子空間下的 representation，而不是本來 $d_{model}$ 維度下的一個 representation，而且這樣的做法也相對地降低了所需要的計算量。&lt;/p&gt;
&lt;p&gt;$$
MultiHead\left(Q, K, V\right) = Concat\left(head_{1}, \dots, head_{h}\right)W^O
$$&lt;/p&gt;
&lt;p&gt;$$
where \space head_i = Attention\left(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V}\right)
$$&lt;/p&gt;
&lt;p&gt;Where the projections are parameter matrices:
$$
W_{i}^{Q} \in R^{d_{model} \times d_k}, W_{i}^{K} \in R^{d_{model} \times d_k}, W_{i}^{V} \in R^{d_{model} \times d_v} and \space W^o \in R^{hd_v \times d_{model}}
$$&lt;/p&gt;
&lt;p&gt;論文中的設定 $h = 8$，$d_k = d_v = d_{model} / h = 512 / 8 = 64$。&lt;/p&gt;
&lt;p&gt;Multi-head attention 運用在 Transformer 三個不同的地方：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在 &lt;strong&gt;encoder-decoder attention layers&lt;/strong&gt;，當中 queries 是從 decoder 前一層的所 output 的值，而 keys 與 values 則是從 encoder 所 output 的值。這樣的運作方式，可以讓每個位置的 decoder 結果去關注所有 input sequence。&lt;/li&gt;
&lt;li&gt;在 encoder 中的 &lt;strong&gt;self-attention layers&lt;/strong&gt;，對所有的 keys、values 與 queries 都是來至前一層的 output 結果，這讓每個位置的詞都能去關注與自身有關係的詞&lt;/li&gt;
&lt;li&gt;在 decoder 中的 &lt;strong&gt;self-attention layers&lt;/strong&gt;，其概念與上述的 encoder 相同，唯一的差異就是在 decoder 這邊的輸入是 target sequence，所以為了避免偷看答案的情形出現，並保持 auto-regressive 的特性，所以必須要加上遮罩(mask)，來避免模型得知句子的結果。而實作上 mask 的處理是在 &lt;strong&gt;scale dot-product attention&lt;/strong&gt; 裡面來執行的。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;postition-wise-feed-forward-networks&#34;&gt;Postition-wise Feed-Forward Networks&lt;/h2&gt;
&lt;p&gt;在 Encoder 跟 Decoder 裡頭都各自有一個 Feed Forward Networks，其組成是由兩個 linear transformations 所組成，使用的 activation function 是 ReLU 。公式如下：
$$
FFN(x)=max(0,xW_1+b_1)W_2+b_2
$$&lt;/p&gt;
&lt;p&gt;Linear transformations 被應用在相同的位置上，但在不同層之間的參數就不相同。在兩層的線性轉換中，論文中設計了一個鑽石模型，讓輸入的維度與輸出的維度相同皆為 $d_{model} = 512$，但在 FFN 的中間層維度則為較大的維度為 2048。其目的是為了讓模型能夠學習到到高層次的抽象理解與減少直接 fully-connected 的計算量。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;encoder--decoder&#34;&gt;Encoder &amp;amp; Decoder&lt;/h2&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./transformer.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://www.tensorflow.org/tutorials/text/transformer#encoder_and_decoder&#34;&gt;Tensorflow Tutorials&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;由了上述各個細部結構的理解後，再來看大結構 Encoder 與 decoder 就很容易了久，因為只是將前面的小結構做組合而已。在講解的過程中，我們都只是以一個 layer 來看講解，如上圖就是一個 transformer layer，而實際上論文是堆疊了 6 層的 transformer layers。底下附上兩層的組合，讓讀者可以比較有概念。&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./transformer_2_layers.png&#34; style=&#34;zoom:60%&#34; /&gt;
  &lt;figcaption&gt;
  Image credit: &lt;a href=&#34;https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#Position-wise-Feed-Forward-Networks&#34;&gt; leemeng : 淺談神經機器翻譯&amp;用 Transformer 與 TensorFlow 2 英翻中&lt;/a&gt;
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由 6 個 encoder layers 堆疊組成&lt;/li&gt;
&lt;li&gt;每個 encoder layer 都包含兩個 sublayers:
&lt;ol&gt;
&lt;li&gt;Multi-head attention (with padding mask)&lt;/li&gt;
&lt;li&gt;Postition-wise Feed-Forward Networks&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;每個 sublayer 都有一個殘差連結(residual connection) 來緩解梯度消失(Gradien vanishing) 的狀況&lt;/li&gt;
&lt;li&gt;在每個 sublayer 後的 output 都會再經過針對最後一維 $d_{model}$ 做 layer normalization 的處理
$$
LayerNorm(x + Sublayer(x))
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;與 Encoder 相同，都是由 6 個 decoder layers 堆疊組成&lt;/li&gt;
&lt;li&gt;每個 decoder layer 都包含三個 sublayers:
&lt;ol&gt;
&lt;li&gt;Masked multi-head attention (with look ahead mask and padding mask)&lt;/li&gt;
&lt;li&gt;Multi-head attention (with padding mask). $V (value)$ 與 $K(key)$ 的輸入來至於 encoder output，$Q(query)$ 的輸入來至於 Masked multi-head attention sublayer output&lt;/li&gt;
&lt;li&gt;Postition-wise Feed-Forward Networks&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;與 Encoder 相同，在每個 sublayer 之後都 residual connection 與 layer normalization 的運作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;optimizer&#34;&gt;Optimizer&lt;/h2&gt;
&lt;p&gt;模型所使用的 optimizer 是 adam 加上 warmup 的設計，但這邊的 warmup 與我們一般理解的 learning rate warmup 有些許不同，這邊採用的是用 $d_{model}^{-0.5}$ 來當作每一步要增加的 learing rate 比例，這邊就直接擷取論文中的段落做呈現：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We used the Adam optimizer with $\beta_1 = 0.9, \beta_2 = 0.98$ and $\epsilon \in 10^{-9}$. We varied the learning rate over the course of training, according to the formula:$$
lrate = d_{model}^{-0.5} \cdot min\left(step\_num^{-0.5}, warmup\_steps^{-1.5} \right)
$$This corresponds to increasing the learning rate linearly for the first $warmup\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup_steps = 4000$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;除了這邊些的優化改動外，還有使用了 Residual Dropout $P_{drop} = 0.1$ 與 Label Smoothing $\epsilon_{ls} = 0.1$ 的技巧來優化模型結果。&lt;/p&gt;
&lt;p&gt;最後的模型表現就請各位直接翻翻論文看一下，這邊就不再多做解說了。在經歷 transformer 三部曲後，應該都對 transformer 有更深的了解，之後將會往 transformer 的延伸應用，也就是目前在 NLP 領域最廣為被大家使用或是被拿來優化的˙ &lt;strong&gt;BERT&lt;/strong&gt; 做解說。&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;The IIIustrated Transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#top&#34;&gt;w淺談神經機器翻譯 &amp;amp; 用 Transformer 與 Tensorflow2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/34781297&#34;&gt;Attention is all you need 解讀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/text/transformer&#34;&gt;Transformer model for language understanding by google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a&#34;&gt;How Self-Attention with Relative Position Representations works&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Transformer Part 2 - Attention</title>
      <link>https://roymondliao.github.io/post/transformer_part2/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/transformer_part2/</guid>
      <description>&lt;h1 id=&#34;attention-mechanism&#34;&gt;Attention Mechanism&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt; 的概念在 2014 年被 Bahdanau et al. [Paper 1] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 &lt;span  class=&#34;math&#34;&gt;\({y_1, \cdots, y_{t-1}}\)&lt;/span&gt; 的條件下字詞 $y_{t}$ 的機率，所以 decoder  的定義是在有序的條件下所有預測字詞的聯合機率：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
p(\mathrm{y}) &amp; = \prod_{t=1}^T p(y_t | \{y_1, \cdots, y_{t-1}\}, c) \tag 1 \\
\mathrm{y} &amp; = (y_1, \cdots, y_T)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;在第 $t$ 時間，字詞 $y_t$ 的條件機率：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
p(y_t | \{y_1, \cdots, y_{t-1}\}, c) = g(y_{t-1}, s_t, c) \tag 2
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;當中 $g$ 唯一個 nonlinear function，可以為多層的架構，$s_t$ 為 hidden state，c 為 context vector。&lt;/p&gt;

&lt;p&gt;而在 Attention model 中，作者將 decoder 預測下一個字詞的的條件機率重新定義為：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
p(y_i | \{y_1, \cdots, y_{i-1}\}, \mathrm{x}) = g(y_{i-1}, s_t, c_i) \tag 3
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;當中 $s_i$ 表示 RNN 在 $i$ 時間的 hiddent state。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
s_i = f\left(s_{i-1}, y_{i-1}, c_i\right) \tag 4
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;將式子 (3) 與 (2) 相比就可以發現，每一個預測字詞 $y_i$ 對於 context vector 的取得，由原本都是固定的 C  轉變成 每個字詞預測都會取得不同的 $C_i$。&lt;/p&gt;

&lt;p&gt;Bahdanau Attention model 的架構如圖1：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_bahdanau.png&#34; style=&#34;zoom:60%&#34; /&gt;
  &lt;figcaption&gt;
  圖1 (Image credit:[Paper 1])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Context vector $c_i$ 是取決於 sequence of annotations &lt;span  class=&#34;math&#34;&gt;\((h_1, h_2, \cdots, h_{T_x})\)&lt;/span&gt; 的訊息，annotation $h_i$ 包含了在第 $i$ 步下， input sequence 輸入到 econder 的訊息。計算方法是透過序列權重加總 annotation $h_i$，公式如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{equation}
c_i = \displaystyle\sum_{j=1}^{T_x}\alpha_{ij}h_j \tag5
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $i$ 表示 decoder 在第 $i$ 個字詞，$j$ 表示 encoder 中第 $j$ 個詞。&lt;/p&gt;

&lt;p&gt;$\alpha_{ij} $ 則稱之為 attention distribution，可以用來衡量 input sequence 中的每個文字對 output sequence 中的每個文字所帶來重要性的程度，計算方式如下
：
&lt;span  class=&#34;math&#34;&gt;\(
\begin{align}
\alpha_{ij} &amp; = softmax(e_{ij}) \\
&amp; = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} \tag6 \\
\end{align}
\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
e_{ij} = a(s_{i-1}, h_j) \tag7
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;計算 attention  score $e_{ij}$ 中 $a$ 表示為 alignment model (對齊模型)，是衡量 input sequence 在位置 $j$ 與 output sequence 位置 $i$ 這兩者之間的關係&lt;/strong&gt;。
這邊作者為了解決在計算上需要 $T_{x} \times T_{y}$ 的計算量，所以採用了 singlelayer multilayer perceptron 的方式來減少計算量，其計算公式：
&lt;span  class=&#34;math&#34;&gt;\(
\begin{align}
a(s_{i-1}, h_j) = v_a^Ttanh(W_aS_{i-1} + U_ah_j) \tag8
\end{align}
\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $W_a \in R^{n\times n}，U_a \in R^{n \times 2n}，v_a \in R^n$ 都是 weight。&lt;/p&gt;

&lt;p&gt;另外作者在此採用了 BiRNN(Bi-directional RNN) 的 forward 與 backward 的架構，由圖一可以得知&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Forward hidden state 為 &lt;span  class=&#34;math&#34;&gt;\((\overrightarrow{h_1}, \cdots, \overrightarrow{h_{T_x}})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Backward hidden state 為 &lt;span  class=&#34;math&#34;&gt;\((\overleftarrow{h_1}, \cdots, \overleftarrow{h_{T_x}})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Concatenate forward 與 backward 的 hidden state，所以 annotation $h_j$ 為 &lt;span  class=&#34;math&#34;&gt;\(\left[\overrightarrow{h_j^T};\overleftarrow{h_j^T}\right]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這樣的方式更能理解句子所要表達的意思，並得到更好的預測結果。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;例如以下兩個句子的比較：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;我喜歡蘋果，因為它很好吃。&lt;/li&gt;
&lt;li&gt;我喜歡蘋果，因為它很潮。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;下圖為 Bahdanau Attention model 的解析可以與圖1對照理解，這樣更能了解圖一的結構：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;需要注意的一點是在最一開始的 decoder hidden state $S_0$ 是採用 encoder 最後一層的 output&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_bahdanau_example.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  圖2
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;下圖為論文中英文翻譯成法語的 attention distribution：&lt;/p&gt;

&lt;p&gt;在圖中 $[European \space Economic \space Area]$ 翻譯成$ [zone \space \acute{a}conomique \space europ\acute{e}enne] $ 的注意力分數上，模型成功地專注在對應的字詞上。&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_bahdanau_output.png&#34; style=&#34;zoom:90%&#34; /&gt;
  &lt;figcaption&gt;
  圖3 (Image credit:[Paper 1])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;最後作者後續還有實驗了採用 LSTM 來替換 Vainlla RNN 進行實驗，詳細的公式都有列出在論文中，有興趣的可以看一下。&lt;/p&gt;

&lt;h1 id=&#34;attention-mechanism-family&#34;&gt;Attention Mechanism Family&lt;/h1&gt;

&lt;h3 id=&#34;hard-attention--soft-attention&#34;&gt;Hard Attention &amp;amp; Soft Attention&lt;/h3&gt;

&lt;p&gt;Xu et al. [Paper 2] 對於圖像標題(caption)的生成研究中提出了 hard attention 與 soft attention 的方法，作者希望透過 attention mechanism 的方法能夠讓 caption 的生成從圖像中獲得更多有幫助的訊息。下圖為作者所提出的模型架構：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./nic_figure1.jpg&#34; style=&#34;zoom:70%&#34; /&gt;
  &lt;figcaption&gt;
  圖4 (Image credit:[Paper 2])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;模型結構&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 encoder 端模型使用 CNN 來提取 low-level 的卷積層特徵，每一個特徵都對應圖像的一個區域&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ a = \{a_1, \dots, a_L\}, a_i \in R^D \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;總共有 $L$ 個特徵，特徵向量維度為 $D$。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Decoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;採用 LSTM 模型來生成字詞，而因應圖片的內容不同，所以標題的長度是不相同的，作者將標題 $y $ encoded 成一個 one-hot encoding 的方式來表示&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ y = \{y_1, \dots, y_C\}, y_i \in R^K \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;K 為字詞的數量，C 為標題的長度。下圖為作者這本篇論文所採用的 LSTM 架構：&lt;/p&gt;

&lt;p&gt;&lt;figure class=&#34;image&#34;&gt;
  &lt;center&gt;
  &lt;img src=&#34;./attention_soft_and_hard.png&#34; style=&#34;zoom:50%&#34; /&gt;
  &lt;figcaption&gt;
  圖5 (Image credit:[Paper 2])
  &lt;/figcaption&gt;
  &lt;/center&gt;
  &lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;利用 affine transformation 的方式  &lt;span  class=&#34;math&#34;&gt;\(T_{s, t} : R^s \rightarrow R^t\)&lt;/span&gt; 來表達 LSTM 的公式：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{pmatrix}
  i_t \\
  f_t \\
  o_t \\
  g_t 
  \end{pmatrix}
  = 
  \begin{pmatrix}
  \sigma \\
  \sigma \\
  \sigma \\
  tanh 
  \end{pmatrix}
  T_{D+m+n, n}
  \begin{pmatrix}
  Ey_{t-1} \\
  h_{t-1} \\
  \hat{Z_t}
  \end{pmatrix}  \tag1 \\
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{align}
  c_t &amp; = f_t \odot c_{t-1} + i_t \odot g_t \tag2 \\
  h_t &amp; = o_t \odot tanh(c_t) \tag3
  \end{align}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(i_t\)&lt;/span&gt; : input gate&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(f_t\)&lt;/span&gt; : forget gate&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(o_t\)&lt;/span&gt; : ouput gate&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(g_t\)&lt;/span&gt; : canaidate cell&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(c_t\)&lt;/span&gt; : memory cell&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(h_t\)&lt;/span&gt; : hidden state&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(Ey_{t-1}\)&lt;/span&gt; 是詞 &lt;span  class=&#34;math&#34;&gt;\(y_{t-1}\)&lt;/span&gt; 的 embedding vector，&lt;span  class=&#34;math&#34;&gt;\(E \in R^{m \times k}\)&lt;/span&gt; 為 embedding matrix，m 為 embedding dimention&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(\hat{Z} \in R^D\)&lt;/span&gt; 是 context vector，代表捕捉特定區域視覺訊息的上下文向量，與時間 $t$ 有關，所以是一個動態變化的量
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;特別注意的是作者在給定 memory state 與 hidden state 的初始值的計算方式使用了兩個獨立的多層感知器(MLP)，其輸入是各個圖像區域特徵的平均，計算公式如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{align}
  c_0 = f_{init, c}( \frac{1}{L} \sum_{i}^L a_i) \\
  h_0 = f_{init, h}( \frac{1}{L} \sum_{i}^L a_i)
  \end{align}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;以及作者為了計算在 $t$ 時間下所關注的 context vector &lt;span  class=&#34;math&#34;&gt;\(\hat{Z_t}\)&lt;/span&gt; &lt;strong&gt;定義了 attention machansim $\phi$ 為在 $t$ 時間，對於每個區域 $i$ 計算出一個權重 &lt;span  class=&#34;math&#34;&gt;\(\alpha_{ti}\)&lt;/span&gt; 來表示產生字詞 $y_t$ 需要關注哪個圖像區域  annotation vectors $a_i, i=1, \dots, L$ 的訊息。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;權重 &lt;span  class=&#34;math&#34;&gt;\(\alpha_i\)&lt;/span&gt; 的產生是透過輸入 annotation vector &lt;span  class=&#34;math&#34;&gt;\(a_i\)&lt;/span&gt; 與前一個時間的 hidden state  $h_{t-1}$ 經由 attention model $f_{att}$ 計算所產生。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{align}
  e_{ti} = f_{att}(a_i, h_{t-1}) \tag4 \\
  \alpha_{ti} = \frac{exp(e_{ti})}{\sum_{k=1}^{L}exp{e_{tk}}} \tag5 \\
  \hat{Z_t} = \phi(\{a_i\}, \{\alpha_{i}\}) \tag6
  \end{align}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;有了上述的資訊，在生成下一個 $t$ 時間的字詞機率可以定義為：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  p(y_t | a, y_1, y_2, \dots, y_{t-1}) \propto exp(L_o(Ey_{t-1} + L_hh_t + L_z\hat{Z_t})) \tag7
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 &lt;span  class=&#34;math&#34;&gt;\(L_o \in R^{K \times m}, L_h \in R^{m \times n}, L_z \in R^{m \times D}\)&lt;/span&gt;，m 與 n 分別為 embedding dimension 與 LSTM dimension。&lt;/p&gt;

&lt;p&gt;對於函數 $\phi$ 作者提出了兩種 attention  machansim，對應於將權重附加到圖像區域的兩個不同策略。根據上述的講解，搭配下圖為 Xu et al. [Paper 2] 的模型架構解析，更能了解整篇論文模型的細節：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_soft_and_hard_example.png&#34; style=&#34;zoom:90%&#34; /&gt;
  &lt;figcaption&gt;
  圖6
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h4 id=&#34;hard-attention-stochastic-hard-attention&#34;&gt;Hard attention (Stochastic Hard Attention)&lt;/h4&gt;

&lt;p&gt;在 hard attention 中定義區域變數(location variables) $s_{t, i}$ 為在 t 時間下，模型決定要關注的圖像區域，用 one-hot 的方式來表示，要關注的區域 $i$ 為 1，否則為 0。&lt;/p&gt;

&lt;p&gt;$s_{t, i}$ 被定為一個淺在變數(latent variables)，並且以 &lt;strong&gt;multinoulli distriubtion&lt;/strong&gt; 作為參數 $\alpha_{t, i}$ 的分佈，而 $\hat{Z_t}$ 則被視為一個隨機變數，公式如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
p(s_{t, i} = 1 | s_{j, t}, a) = \alpha_{t, i} \tag8 
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\hat{Z_t} = \sum_{i} s_{t, i}a_i \tag9
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;定義新的 objective function $L_s$ 為 marginal log-likelihood $\text{log }p(y|a)$ 的下界(lower bound)&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
L_s &amp; = \sum_s p(s|a)\text{log }p(y|s,a) \\
&amp; \leq \text{log } \sum_s p(s|a)p(y|s,a) \\
&amp; = \text{log }p(y|a)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;在後續的 $L_s$ 推導求解的過程，作者利用了&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Monte Carlo 方法來估計梯度，利用 moving average 的方式來減小梯度的變異數&lt;/li&gt;
&lt;li&gt;加入了 multinouilli distriubtion 的 entropy term $H[s]$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;透過這兩個方法提升隨機算法的學習，作者在文中也提到，最終的公式其實等價於 &lt;strong&gt;Reinforce learing&lt;/strong&gt;。作者在論文中有列出推導的公式，有興趣的可以直接參考論文。&lt;/p&gt;

&lt;h4 id=&#34;soft-attention-deterministic-soft-attention&#34;&gt;Soft attention (Deterministic Soft Attention)&lt;/h4&gt;

&lt;p&gt;Soft attention 所關注的圖像區域並不像 hard attention 在特定時間只關注特定的區域，在 soft attention 中則是每一個區域都關注，只是關注的程度不同。透過對每個圖像區域 $a_{i}$ 與對應的 weight $\alpha_{t,i}$ ，$\hat{Z}_t$ 就可以直接對權重做加總求和，從 hard attention  轉換到 soft attention 的 context vector：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\hat{Z_t} = \sum_{i} s_{t, i}a_i \implies \mathbb{E}{p(s_t|a)}[\hat{Z_t}] = \sum_{i=1}^L \alpha_{t,i}a_i
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;這計算方式將 weight vector $\alpha_i$ 參數化，讓公式是可微的，可以透過 backpropagation 做到 end-to-end 的學習。其方法是參考前面所介紹的 Bahdanau attention 而來。&lt;/p&gt;

&lt;p&gt;作者在這邊提出三個理論：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(\mathbb{E}{p(s_t|a)}[h_t]\)&lt;/span&gt; 等同於透過 context vector &lt;span  class=&#34;math&#34;&gt;\(\mathbb{E}{p(s_t|a)}[\hat{Z_t}]\)&lt;/span&gt; 使用 forward propagation 的方法計算 $h_t$&lt;/li&gt;
&lt;li&gt;Normalized weighted geometric mean approximation&lt;/li&gt;
&lt;li&gt;根據公式(7)定義 &lt;span  class=&#34;math&#34;&gt;\(n_t = L_o(Ey_{t-1} + L_hh_t + L_z\hat{Z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以 soft attention 在最後做文字的預測時作者定義了 softmax $k^{th}$ 的 normalized weighted geometric mean。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
NWGM[p(y_t=k|a)] &amp; = \frac{\prod_i exp(n_{t,k,i})^{p(s_{t,i} = 1 | a)}}{\sum_j\prod_i exp(n_{t,j,i})^{p(s_{t,i} = 1 | a)}} \\
&amp; = \frac{exp\left(\mathbb{E_{p(s_t) | a}[n_{t,k}]}\right)}{\sum_j exp\left(\mathbb{E_{p(s_t) | a}[n_{t,j}]}\right)}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\mathbb{E}[n_t] =  L_o(Ey_{t-1} + L_h\mathbb{E}[h_t] + L_z\mathbb{E}[\hat{Z_t}])
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;這邊的部分就是 soft attention  與 Bahdanau attention 的主要差異，在 Bahdanau attention 最後的 output 是透過 softmax 來取得下一次詞的機率，而作者在這邊採用了 NWGM 的方式。這邊並不是很清楚作者怎麼來證明這樣的論述，日後有理解出來或是有找到相關的參考再補上來。&lt;/p&gt;

&lt;p&gt;最後來看看 soft attention 與 hard attention 的圖像視覺化結果，下圖是兩種 attention 對於圖像區域注意程度，可以看得出 hard attention 都會專注在很小的區域，而 soft attention 的注意力相對發散，這也是因為 soft 與 hard 在關注圖像區域上的一個是注意全部的圖像區域，一個是注意特定的區域。&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_soft_and_hard_visualization.png&#34; style=&#34;zoom:90%&#34; /&gt;
  &lt;figcaption&gt;
  圖7 (Image credit:[Paper 2])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;global-attention--local-attention&#34;&gt;Global Attention &amp;amp; Local Attention&lt;/h3&gt;

&lt;p&gt;Loung et al. [Paper 3] 在 2015 年所發表了 global / local attention 來提升 NMT 任務上的準確度，global attention 類似於 soft attention，而 local attention 則介於 hard 與 soft attention 的混合。&lt;/p&gt;

&lt;p&gt;Global / local attention 相同之處：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;採用的 target hidden state $h_t$ 是 stacking LSTM 最後一層的 output&lt;/li&gt;
&lt;li&gt;Context vector $c_t$ 都是將 $h_t$ 與 source-side $\bar{h_s}$ 作為 input 計算&lt;/li&gt;
&lt;li&gt;結合 $c_t$ 與 $h_t$ 的訊息計算 &lt;span  class=&#34;math&#34;&gt;\(\tilde{h_t} = tanh\left(W_c[c_t;h_t]\right)\)&lt;/span&gt;，稱為 attentional vector&lt;/li&gt;
&lt;li&gt;預測 $t$ 時間下的生成字詞的機率 &lt;span  class=&#34;math&#34;&gt;\( p(y_t|y_{\text{&lt;}t}, x) = softmax(W_s\tilde{h_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Global / local attention 不同之處：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Context vector $c_t$ 的計算方式不同&lt;/li&gt;
&lt;li&gt;採用 source side  $\bar{h_s}$ 的數量不同&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下來分別介紹 global attention 與 local attention 當中的細節。&lt;/p&gt;

&lt;h4 id=&#34;global-attention&#34;&gt;Global attention&lt;/h4&gt;

&lt;p&gt;作者將 alignment vector $a_t$ 定義是一個可變長度向量，所以在 global attention 中 $a_t$ 將全部時間的 source side 的資訊當作 input ，公式如下：
&lt;span  class=&#34;math&#34;&gt;\(
\begin{align}
a_t(s) &amp;= align(h_t, \bar{h_s}) \tag 1 \\ 
&amp;= \frac{exp(score(h_t,\bar{h_s}))}{\sum_{s&#39;}exp\left(score(h_t,\bar{h_{s&#39;}})\right)}
\end{align}
\)&lt;/span&gt;
在 score function 的部分，這邊定義了 &lt;strong&gt;content-based function&lt;/strong&gt;，可以有以下三種形式：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
score(h_t, \bar{h_s}) = 
\begin{cases}
h_{t}^T\bar{h_s} &amp; \text{dot} \\
h_{t}^TW_a\bar{h_s} &amp; \text{general} \\
v_a^Ttanh\left(W_a[h_t;\bar{h_s}]\right) &amp; \text{concat}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $W_a, v_a$ 都是透過訓練所得到的參數。&lt;/p&gt;

&lt;p&gt;另外還有一種 &lt;strong&gt;local-based function&lt;/strong&gt;，score只單存參考 target hidden state $h_t$ 的結果&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
a_t = softmax(W_ah_t) &amp;&amp; \text{location} 
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;下圖為 global attention 的模型架構：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_global.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖8 Global Attention (Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Global attention 與 Bahdanau attention 對比，差異的地方如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在 encoder 與 decoder 都採用最後一層的 LSTM output 作為 hidden state&lt;/li&gt;
&lt;li&gt;計算順序為 &lt;span  class=&#34;math&#34;&gt;\(h_t \rightarrow a_t \rightarrow c_t \rightarrow \tilde{h_t}\)&lt;/span&gt;，而 bahdanau attention 是 &lt;span  class=&#34;math&#34;&gt;\(h_{t-1} \rightarrow a_t \rightarrow c_t \rightarrow h_t\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;local-attention&#34;&gt;Local attention&lt;/h4&gt;

&lt;p&gt;相較於 global attention 採用的所有 soucre side 的字詞，在計算上可能較為龐大，而且面對較長的句子可能無法翻譯正確的狀況，提出了 local attention，只專注關心一小部分的字詞來替代關注全部的字詞。&lt;/p&gt;

&lt;p&gt;前面提到 local attention 是 hard 與 soft attention 的混合，選擇性地關注一個數量較小的上下文窗口，並且是可以微分的，減少了 soft attention 的計算量以及避免了 hard attention 不可微分的問題，更容易的訓練。&lt;/p&gt;

&lt;p&gt;注意的重點：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在每個 $t$ 時間下生成 aligned position $p_t$&lt;/li&gt;
&lt;li&gt;Context vector $c_t$ 則是計算 $[p_t - D, p_t + D]$ 之間的 source hidden 做加權平均，$D$ 是可調的參數。如果所選擇的範圍超過句子本身的長度，則忽略掉多出來的部分，只考慮有存在的部分。&lt;/li&gt;
&lt;li&gt;Alignment vector $a_t \in R^{2D+1}$ 是固定的維度&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;作者針對模型提出了兩遍變形：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Monotonic aligment (&lt;strong&gt;local-m&lt;/strong&gt;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;假設 source 與 target sequences 是單調對齊，就是指 source 與  target 長度相同：
&lt;span  class=&#34;math&#34;&gt;\(
p_t = t
\)&lt;/span&gt;
Alignment vector $a_t$ 的計算就跟公式(1)一致。&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Predictive alignment (&lt;strong&gt;local-p&lt;/strong&gt;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;認為 source 與 target sequences 是並非單調對齊，就是長度不相同：
&lt;span  class=&#34;math&#34;&gt;\(
p_t = S \cdot sigmoid\left( v_p^Ttanh(W_ph_t)\right)
\)&lt;/span&gt;
$W_p, v_p$ 都是可訓練的模型參數，$S$ 則表示 source sequence 的長度，$p_t \in [0, S]$。&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alignment vector $a_t$ 的計算採用的 Gaussian distribution 來賦予 alignment 的權重：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
a_t(s) = align(h_t,\bar{h_s})exp\left(-\frac{(s-p_t)^2}{2\sigma^2}\right)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;$align(h_t,\bar{h_s})$ 與公式(1)一致，標準差設定為 $\sigma = \frac{D}{2}$，這是透過實驗所得來。&lt;/p&gt;

&lt;p&gt;下圖為 local attention 的模型架構：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_local.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖9 Local Attention (Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Local attention 可能的會遇到的問題：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;當 encoder 的 input sequence 長度不長時，計算量並不會減少&lt;/li&gt;
&lt;li&gt;當 Aligned position $p_t$ 不準確時，會直接影響到 local attention 的準確度&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;inputfeeding-approach&#34;&gt;Input-feeding Approach&lt;/h4&gt;

&lt;p&gt;作者認為在 global 與 local attention 的方法中，模型的注意力機制是獨立的，但是在整個翻譯的過程中，必須要去了解哪些資訊已經被翻譯了，所以在預測下一個翻譯字詞時，應該結合過去 attentional vectors $\tilde{h_t}$ 的資訊，也就是說在 deocder 這邊多考慮了 alignment model 的結果，如下圖所示：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_global_and_local_input_feeding.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖10 Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;來看看論文中的針對各個模型在 WMT&#39;14 英文翻譯成德文資料集的訓練結果：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;這邊的實驗限制了字詞的數量，只取在資料集中最常出現的 50k 字詞當作 corpos&lt;/li&gt;
&lt;li&gt;如果出現字詞沒有在 corpos 中，則用 &lt;strong&gt;&amp;lt;unk&amp;gt;&lt;/strong&gt; 來取代&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_global_and_local_result.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖11 (Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Global attention 與 local attention 都有自己的優勢，如果說要選用哪個方式來當作模型，認為因應不同的任務可能表現都會有所差異，所以建議兩種都實驗看看結果來比較優劣，而在實際上大多數採用的都是以 Global attention 為主。&lt;/p&gt;

&lt;p&gt;底下列出上面各篇論文所提到的 &lt;strong&gt;Attention score function&lt;/strong&gt;：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Attention score function&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Dot-product&lt;/td&gt;
&lt;td&gt;$score(s_t, h_i) = S_t^Th_i$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;General&lt;/td&gt;
&lt;td&gt;$score(s_t, h_i) = S_t^TW_ah_i$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Additive&lt;/td&gt;
&lt;td&gt;$score(s_t, h&lt;em&gt;i) = v^Ttanh(WS&lt;/em&gt;{t-1} + Uh_i) $&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Scaled Dot-product&lt;/td&gt;
&lt;td&gt;$score(s_t, h_i) = \frac{S_t^Th_i}{\sqrt{d}}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Loocation&lt;/td&gt;
&lt;td&gt;$a_{t} = softmax(W_aS_t)$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;總結來說：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Attention 的目的就是要實現的就是在 decoder 的不同時刻可以關注不同的圖像區域或是句子中的文字，進而可以生成更合理的詞或是結果。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;refenece&#34;&gt;Refenece&lt;/h2&gt;

&lt;p&gt;Paper:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34;&gt;Dzmitry Bahdanau, KyungHyun Cho Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.03044.pdf&#34;&gt;Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.04025.pdf&#34;&gt;Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.02874&#34;&gt;Sneha Chaudhari, Gungor Polatkan , Rohan Ramanath , Varun Mithal, An Attentive Survey of Attention Models(2019)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Illustrate:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/37601161h&#34;&gt;https://zhuanlan.zhihu.com/p/37601161h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/31547842&#34;&gt;https://zhuanlan.zhihu.com/p/31547842&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.floydhub.com/attention-mechanism/#bahdanau-atth&#34;&gt;https://blog.floydhub.com/attention-mechanism/#bahdanau-atth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf&#34;&gt;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/Determined22/p/6914926.html&#34;&gt;https://www.cnblogs.com/Determined22/p/6914926.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&#34;&gt;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf&#34;&gt;http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jiqizhixin.com/articles/2018-06-11-16&#34;&gt;https://www.jiqizhixin.com/articles/2018-06-11-16&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@joealato/attention-in-nlp-734c6fa9d983&#34;&gt;https://medium.com/@joealato/attention-in-nlp-734c6fa9d983&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3&#34;&gt;https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms&#34;&gt;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/80692530&#34;&gt;https://zhuanlan.zhihu.com/p/80692530&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tutorial:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/nmt#background-on-the-attention-mechanism&#34;&gt;Neural Machine Translation (seq2seq) Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/text/transformerG&#34;&gt;https://www.tensorflow.org/tutorials/text/transformerG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;Guide annotating the paper with PyTorch implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Visualization:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jessevig/bertviz&#34;&gt;https://github.com/jessevig/bertviz&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Transformer Part 1 - Seq2Seq</title>
      <link>https://roymondliao.github.io/post/transformer_part1/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/transformer_part1/</guid>
      <description>&lt;p&gt;一開始接觸 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇論文是從 &lt;a href=&#34;https://www.youtube.com/playlist?list=PLqFaTIg4myu8t5ycqvp7I07jTjol3RCl9&#34;&gt;Kaggle Reading Group&lt;/a&gt; 這個 channel 開始，非常推薦可以跟著一起讀!!&lt;/p&gt;

&lt;p&gt;主持人 &lt;a href=&#34;https://www.kaggle.com/rtatman&#34;&gt;Rachael Atman&lt;/a&gt; 本身是 Kaggle 的 Data Scientist，她的導讀我覺得是流暢的，但她自己本身有說過並非是 NLP 領域的專家，所以在 kaggle reading group 裡閱讀的論文也有可能是她完全沒接觸過的，整個 channel 帶給你的就是一個啟發，讓你覺得有人跟你一起閱讀的感覺，然後過程中也些人會在 channel 的 chat room 提出一些看法或是連結，Rachael 本身也會提出自己的見解，可以多方面參考。&lt;/p&gt;

&lt;p&gt;在跟完整個 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 的影片後，還是有太多細節是不清楚的，因為自己本身也不是這個領域的，所以開始追論文中所提到的一些關鍵名詞，就開始從 $seq2seq \rightarrow attention \rightarrow self-attention$。這中間 有太多知識需要記錄下來，所以將論文的內容分成三部曲，來記錄閱讀下來的點點滴滴:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Part 1: Sequence to sequence model 起源&lt;/li&gt;
&lt;li&gt;Part 2: Attention 與 Self-attention 的理解&lt;/li&gt;
&lt;li&gt;Part 3: Transformer 的架構探討與深入理解&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要談論 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇 paper 就必須從 seq2seq 講起，seq2seq 全名為 Sequence to Sequence[1]，是一個 Encoder - Decoder 架構的模型，在 2014 年被提出，被廣泛的應用於 Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.&lt;/p&gt;

&lt;h3 id=&#34;sequence-to-sequence-model&#34;&gt;Sequence to sequence model&lt;/h3&gt;

&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;簡單來說，期望輸入一串序列(source)，輸出一串序列(target)，而這個 source 與 target 可以是什麼呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果以 machine translation 來說，任務是中翻英，輸入是一句中文，而輸出則會是一句英文。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果是 text summarization，輸入則會是一段文章，而輸出則會是一段摘要&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這就是 seq2seq model 所要解決的問題，在輸入一些資訊後，經過 encoder-decoder 的訓練，可以得到相對應的回答或是其他資訊。&lt;/p&gt;

&lt;h4 id=&#34;model&#34;&gt;Model&lt;/h4&gt;

&lt;p&gt;模型的架構也非常簡單，就如下圖所示：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./seq2seq.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;圖一&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;假設今天我們執行中翻英的任務，輸入(source: X)是一句中文，可以是 &amp;quot;我愛機器學習&amp;quot;，而輸出(target: Y)則會是 &amp;quot;I love machine learning&amp;quot;，所以整個訓練的步驟如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Input sequence $(x_1, x_2, x_3, \dots, x_s)$  經過 embedding layer 的轉換，得到每個 word 的 embedding vector&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Encoder 把所有輸入序列 embedding vector 消化後，將資訊壓縮轉換為一個向量 $C$，稱之為 context vector&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
   \begin{align}
   h_s^{e} &amp; = f_{enc}\left(h_{s-1}^{e}, e_{x_{s-1}}, W_{enc}\right) \\
   C &amp; = h_s^e \text {，最後一步的 hidden state} \\
   C &amp; = q(h_s^e) \text {，最後一步的 hidden state 做 transform } \\
   C &amp; = q\left(h_1^e, h_2^e, \dots, h_s^e\right) \text {，每一步的 hidden state 做 transform }
   \end{align}
   \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $f_{enc}(\cdot)$ 表示 Encoder 中的 RNN function，參數為 $W_{enc}$。$e_{x_s}$ 表示 $x_s$ 的 embedding vector，$h_s^e$ 表示在時間 $s$ 的 hidden state，$C$ 可以表示為 Encoder 最後的 hidden state 或是經過函數 $q(\cdot)$ 的轉換。&lt;/p&gt;

&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Decoder 則根據 context vector 的資訊來生成文字，output sequence $(y_1, y_2, y_3, \dots, y_t)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
   \begin{align}
   h_0^{d} &amp; = C \\
   h_{t}^{d} &amp; = f_{dec}\left(h_{t-1}^{d}, e_{y_{t-1}}, W_{dec}\right) \\
   O_{t} &amp; = g\left(h_t^d\right)
   \end{align}
   \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $h_{0}^{d}$ 為 context vector 傳進來當作 Decoder 的初始 hidden state，$f_{dec}(\cdot)$ 表示 Decoder 中的 RNN function，參數為 $W_{dec}$。$h_{t}^{d}$ 表示 Decoder在時間 $t$ 的 hidden state，$e_{y_{t}}$ 表示前一步的所得到的 $y_{t-1}$ 結果當作輸入，$y_0$ 都是以特殊索引 &amp;lt;BOS&amp;gt; 當作輸入。
   $g(\cdot)$ 為 output layer，一般都是 softmax function。&lt;/p&gt;

&lt;p&gt;過程就只是簡單的三個步驟，雖然看起來簡單，但當中有些細節是需要注意的。&lt;/p&gt;

&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;特殊索引&lt;/p&gt;

&lt;p&gt;在每個句子做 one-hot-encoder 的轉換時，會在句子的前後加上 &amp;lt;BOS&amp;gt; 與 &amp;lt;EOS&amp;gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BOS: Begin of sequence，在預測的時候我們並沒有對應的答案，所以會先以 &amp;lt;BOS&amp;gt; 當作 $Y_0$ 的 target input&lt;/li&gt;
&lt;li&gt;EOS: End of sequence，用意是要告訴 model 當出現這個詞的時候就是停止生成文字，如果沒有這個詞，模型會無限迴圈的一直生成下去&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了上述的 &amp;lt;BOS&amp;gt; 與 &amp;lt;EOS&amp;gt; 外，還有 &amp;lt;PAD&amp;gt; 與 &amp;lt;UNK&amp;gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PAD: 由於 RNN 的 parameters 是共享的，所以在 input 的維度就需要保持相同，但並不是每個句子的長度都是同樣的，有的可能長度是 3 ，有的長度可能是 5，所以為了處理不同 input sequence 長度不同的狀況，增加了 &amp;lt;PAD&amp;gt; 的字詞，來讓每次 &lt;strong&gt;batch&lt;/strong&gt; 的 input sequence 的長度都是相同的&lt;/li&gt;
&lt;li&gt;UNK: 如果輸入的字詞在 corpus 是沒有出現過的，就會用 &amp;lt;UNK&amp;gt; 索引來代替&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Encoder layer 與 Decoder layer 的選擇&lt;/p&gt;

&lt;p&gt;Encoder 與 Decoder 中的 RNN function 可以是 simple RNN / LSTM / GRU，又或者是一個 bidirectional LSTM 的架構在裡面，也可以是一個 multi layer LSTM&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Teacher forcing&lt;/p&gt;

&lt;p&gt;在 training model 時 ，為了提高 model 的準確度與訓練速度，採用了 &lt;a href=&#34;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&#34;&gt;Teacher forcing training&lt;/a&gt; 方法，圖二就是 teacher foring 的概念，在 training 的時候直接告訴 model 實際的答案，省去 model 自己去尋找到正確的答案。另外也有提出 &lt;a href=&#34;https://arxiv.org/abs/1610.09038&#34;&gt;Professor Forcing&lt;/a&gt; 的做法，尚未理解這方法的概念，提供當作參考。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./teacher_forcing.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;
  圖二 (Image credit: &lt;a href=&#34;https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c&#34;&gt;LINK&lt;/a&gt;)
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h4 id=&#34;prediction&#34;&gt;Prediction&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Beam search&lt;/p&gt;

&lt;p&gt;在 prediction  model 時，每一步的 output 都是要計算出在 corpus 中生成最可能的那個字詞&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
\hat{y_t} = argmax\space p_{\theta}(y | \hat{y}_{1:(t-1)}) 
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 &lt;span  class=&#34;math&#34;&gt;\(\hat{y}_{1:(t-1)} = \hat{y}_1,\dots,\hat{y}_{t-1}\)&lt;/span&gt; 為前面 $t-1$ 步所生成的字詞。以一個簡單的概念來思考，每一步的 output 都是該步所得到的最大條件機率，那這樣的 greedy search 所得到的結果對於我們的目標並非是最優的，得到的是每個字詞的最大條件機率，而並非是整個句子的最大條件機率，所以這樣的狀況下你所得的的翻譯可能不會是最適合的。&lt;/p&gt;

&lt;p&gt;Beam search 就是為了解決這樣的問題而提出的，在每一步的生成過程中，生成 $B$ 的最可能的文字序列作為約束，其中 $B$ 的大小為 beam width，是一個 hyperparamter。$B$ 值越大可以得到更好的結果，但相對的計算量也增加。&lt;/p&gt;

&lt;p&gt;過程就如同圖三所示:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;建立一個 search tree，該 root 為一個開始符號 (非序列的第一個詞)&lt;/li&gt;
&lt;li&gt;由序列的左到右開始，順序生成目標語言序列，同時成長對應的搜尋樹&lt;/li&gt;
&lt;li&gt;在生成序列的每一步，對  search tree 的每個 leaf node，選取 $B$ 個擁有最高條件機率的生成單詞，並生成 B 個子節點。&lt;/li&gt;
&lt;li&gt;在成長搜尋樹後，進行剪枝的工作，只留下 B 個最高條件機率的葉節點後，再進行下一個位置的序列生成。
&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Beam search 的實作可以參考 Blog:[4]。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./beam_search.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;圖三 (Image credit: &lt;a href=&#34;https://distill.pub/2017/ctc/&#34;&gt; LINK&lt;/a&gt;)
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;另外要注意在使用 beam search 所謂遇到的問題，在 Andrew Ng 大師的&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;課程&lt;/a&gt;中提到&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;消除長度對計算機率影響（Length Normalization）&lt;/li&gt;
&lt;li&gt;如何選擇 Beam Width 參數（The Choice of Beam Width）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;詳細的解說可以參考&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;課程&lt;/a&gt;或是 Blog:[3]。&lt;/p&gt;

&lt;h4 id=&#34;problems&#34;&gt;Problems&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;所有資訊只壓縮成一個 context vector，input sequence 的資訊很難全部保存在一個固定維度的向量裡&lt;/li&gt;
&lt;li&gt;當 sequence 的長度很長時，在 decoder 解碼時，由於 Recurrent neural network 的依賴問題，容易丟失 input sequence 的訊息&lt;/li&gt;
&lt;/ol&gt;

&lt;hr&gt;

&lt;p&gt;在理解完 Seq2Seq model 後，所遇到的問題該如何解決? 那就是要靠 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇 paper 的主要重點之一 &lt;code&gt;Attention mechanism(注意力機制)&lt;/code&gt;，
這部分將會在 part 2 的時候介紹。&lt;/p&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34;&gt;Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, Sequence to Sequence Learning with Neural Networks(2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/pdf/1610.09038.pdf&#34;&gt;Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio, Professor Forcing: A New Algorithm for Training Recurrent Networks(2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2014)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&#34;https://arxiv.org/abs/1508.04025&#34;&gt;Minh-Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Blog:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&#34;&gt;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdfh&#34;&gt;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdfh&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10208587&#34;&gt;https://ithelp.ithome.com.tw/articles/10208587&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&#34;https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/&#34;&gt;https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&#34;https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-%E4%B8%AD%E6%96%87%E7%89%88-2714bbd92727&#34;&gt;Seq2seq pay Attention to Self Attention: Part 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&#34;https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-%E4%B8%AD%E6%96%87%E7%89%88-ef2ddf8597a4&#34;&gt;Seq2seq pay Attention to Self Attention: Part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;&gt;Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&#34;http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/&#34;&gt;http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&#34;https://www.zhihu.com/question/54356960&#34;&gt;https://www.zhihu.com/question/54356960&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adversarial validation</title>
      <link>https://roymondliao.github.io/post/adversarial_validation/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/adversarial_validation/</guid>
      <description>&lt;p&gt;在重新回顧 Kaggle 近期的 IEEE-CIS Fraud Detection 的比賽中，發現有人提到一個 Features selection 的方法 &lt;strong&gt;Adversarial validation&lt;/strong&gt;。&lt;/p&gt;

&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;

&lt;p&gt;在建立模型時常常都會遇到 training set 與 testing set 的分佈存在明顯的差異的，而在分佈不相同的狀況下，即使我們使用 Kfold 的方法來驗證 model，也不會得到較好的結果，因為在驗證所取得的 validation set 也會與 testing set 有著分佈上的差異。&lt;/p&gt;

&lt;p&gt;在現實的處理方法，可以透過重新收集數據或是一些處理手段，來取得 training set 與 testing set 分佈相同的，但在資料的比賽中， training set 與 testing set 都是給定好的數據，並無法做其他跟改，而面對這樣的狀況， Adversarial validation 就是一個很好來處理這樣的問題。&lt;/p&gt;

&lt;h1 id=&#34;mothed&#34;&gt;Mothed&lt;/h1&gt;

&lt;p&gt;其實 Adversarial validation 的概念非常簡單，只需要幾個步驟：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;將 training set 與 testing set 合併，並標注新的 target column &lt;code&gt;is_train&lt;/code&gt; ($training = 1, testing = 0$)&lt;/li&gt;
&lt;li&gt;建立一個 classifier&lt;/li&gt;
&lt;li&gt;將 training set 的預測機率按照 Ascending 的方式排序，由小排到大。&lt;/li&gt;
&lt;li&gt;取 Top $n\%$ 的數據當作 validation set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;藉由這樣的方式所取得的 validation set 在分佈上就與 testing set 相似，如果 model 在 validation 上取得好的預測結果，那相對地也能反映在 testing set。&lt;/p&gt;

&lt;h1 id=&#34;understanding&#34;&gt;Understanding&lt;/h1&gt;

&lt;p&gt;依據 $(2)$ 建模的結果：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Model 的 AUC 大約等於 0.5，表示 training set 與 testing set 來自相同的分佈&lt;/li&gt;
&lt;li&gt;Model 的 AUC 非常高時，表示 training set 與 testing set 來自不相同的分佈，可以明顯地分開&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;other&#34;&gt;Other&lt;/h1&gt;

&lt;p&gt;這邊提一下另一個 trick 的 features selection 方法，稱為 &lt;strong&gt;time consistency&lt;/strong&gt;。在 IEEE-CIS Fraud Detection 比賽第一名的隊伍中，&lt;a href=&#34;https://www.kaggle.com/cdeotte&#34;&gt;Chris Deotte&lt;/a&gt; 提出用了這個方法來去除掉對模型沒有影響力的 features。&lt;/p&gt;

&lt;h3 id=&#34;problem-1&#34;&gt;Problem&lt;/h3&gt;

&lt;p&gt;不管在現實的資料或是比賽的資料，部分資料都有可能因為時間的改變而分佈有所改變，這是我們在建立模型上不太希望發生的事情。因為如果 features 會因為時間的因素而分佈有明顯變化的話，在建模的過程中，受時間影響的 features 可能就會傷害模型本身，可能在時間相近的資料驗證有好的表現，但當預測時間間隔較長的資料時就會發生 overfitting。在處理上述的情況，我們期望 features 的分佈是穩定的，不希望因為時間的影響而有所改變，所以可以使用 time consistency 的方法來剔除這些受時間影響的 features。&lt;/p&gt;

&lt;h3 id=&#34;mothed-1&#34;&gt;Mothed&lt;/h3&gt;

&lt;p&gt;Time consistency 的步驟，這邊以 IEEE-CIS Fraud Detection 的比賽資料為例：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;將 training set 依據&lt;code&gt;月&lt;/code&gt;為單位切分資料&lt;/li&gt;

&lt;li&gt;&lt;p&gt;training data 與 validation data 策略，這邊的策略可以自由調整改變，以下只舉幾個例子&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;選擇前 n 個月的資料為 training data，最後一個月的資料為 validation data&lt;/li&gt;
&lt;li&gt;選擇前 n 個月的資料為 training data，中間跳過 m 個月份，最後一個月的資料為 validation data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;選擇一個 feature，進行模型建立，分別查看模型的 AUC 在 training 與 validation 是否有差異&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;understanding-1&#34;&gt;Understanding&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;如果 training 的 AUC 與 validation 的 AUC 差不多，表示這 feature 不受時間的變化影響&lt;/li&gt;
&lt;li&gt;如果 training 的 AUC 與 validation 的 AUC 有明顯差異，表示這 feature 時間的變化影響，會影響模型本身，可以考慮移除&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;

&lt;p&gt;以下是 Chris Deotte 所提供的簡單的程式碼：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ADD MONTH FEATURE&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datetime
START_DATE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strptime(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017-11-30&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;%Y-%m-&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
train[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;DT_M&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;TransactionDT&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;apply(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; x: (START_DATE &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;timedelta(seconds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x)))
train[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;DT_M&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (train[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;DT_M&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;year&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2017&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; train[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;DT_M&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;month 

&lt;span style=&#34;color:#75715e&#34;&gt;# SPLIT DATA INTO FIRST MONTH AND LAST MONTH&lt;/span&gt;
train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train[train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DT_M&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()
validate &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train[train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DT_M&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;17&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy()

&lt;span style=&#34;color:#75715e&#34;&gt;# TRAIN AND VALIDATE&lt;/span&gt;
lgbm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lgb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;LGBMClassifier(n_estimators&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;, objective&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;binary&amp;#39;&lt;/span&gt;, num_leaves&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, learning_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.02&lt;/span&gt;)
h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lgbm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(train[[col]], 
             train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isFraud, 
             eval_metric&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auc&amp;#39;&lt;/span&gt;, 
             eval_names&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;valid&amp;#39;&lt;/span&gt;],
             eval_set&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[(train[[col]],train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isFraud),(validate[[col]],validate&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isFraud)],
             verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)
auc_train &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(h&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_best_score[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auc&amp;#39;&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
auc_val &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;round(h&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_best_score[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;valid&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;auc&amp;#39;&lt;/span&gt;], &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Best score in trian:{}, valid:{}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(auc_train, auc_val))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Btw，最近有看到一個驗證的方法叫做 &lt;code&gt;Double Cross-Validation&lt;/code&gt;，這邊紀錄一下，有機會再來講講這方法的概念與應用。&lt;/p&gt;

&lt;h1 id=&#34;refenece&#34;&gt;Refenece&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://fastml.com/adversarial-validation-part-one/&#34;&gt;http://fastml.com/adversarial-validation-part-one/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fastml.com/adversarial-validation-part-two/&#34;&gt;http://fastml.com/adversarial-validation-part-two/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43896398/article/details/84762922&#34;&gt;https://blog.csdn.net/weixin_43896398/article/details/84762922&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308&#34;&gt;https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Lookahead Optimizer: k steps forward, 1 step back</title>
      <link>https://roymondliao.github.io/post/lookahead/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/lookahead/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;在目前的 optimizer 分為兩個主要發展方向：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Adaptive learning rate, such as AdaGrad and Adam&lt;/li&gt;
&lt;li&gt;Accelerated schema (momentum), such as Polyak heavyball and Nesterov momentum&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上都是透過累積過往梯度下降所得到的結果來達到收斂，然而要獲得好的結果，都需要一些超參數的調整。&lt;/p&gt;

&lt;p&gt;Lookahead method：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;是一種新的優化方法，採用兩個不同的權重，分別為 fast weights 與 slow weights。fast weights 是使用一般常見的 optimizer 當作 inner optimizer 先進行 &lt;code&gt;k&lt;/code&gt; 次的計算後得到的結果與預先保留的 slow weights 進行線性插值(linearly interpolating)來更新權重 ，更新後的 wieight 為新的 slow weights 並推動之前的 fast weights 往前探索，以這樣的方式進行迭代。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在使用不同的 inner optimizer 下，像是 SGD 或是 Adam，減少了對超參數調整的需求，並且可以以最小的計算需求確保在不同的深度學習任務中加快收斂速度。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;lookahead_figure_1.png&#34; alt=&#34;lookahead_figure_1&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;演算過程 :&lt;/p&gt;

&lt;p&gt;Step 1 : 先設定 $\phi$ 的初始值，以及選定 objective function $L$ &lt;br&gt;
Step 2 : 確定更新週期 $k$ 值、slow weight 的更新步伐 $\alpha $ 以及 optimizer $A$ &lt;br&gt;
Step 3 : 更新 fast weight $\theta$ ，$ \space \theta_{t,0} \leftarrow \phi_{t-1}, t=1,2,\dots $ &lt;br&gt;
Step 4 : 利用 optimizer $A$ 迭代 $k$ 次更新，由 $\theta_{t, i}$ 更新到 $\theta_{t, k}, i=1, 2, \dots, k$ &lt;br&gt;
Step 5 : 更新 slow weight $\phi_{k} \leftarrow \phi_{k-1} + \alpha\left(\theta_{t, k} - \phi_{t-1}\right)$ &lt;br&gt;
重複 Step 3 - Step 5 直至收斂。&lt;/p&gt;

&lt;p&gt;其可以想像身處在山脈的頂端，而周邊都是山頭林立，有高有低，其中一座山可通往山腳下，其他都只是在山中繞來繞去，無法走下山。如果親自探索是非常困難，因為在選定一條路線的同時，必須要放棄其他路線，直到最終找到正確的通路，但是如果我們在山頂留下一位夥伴，在其狀況看起來不妙時及時把我們叫回，這樣能幫助我們在尋找出路的時候得到快速的進展，因此全部地形的探索速度將更快，而發生迷路的狀況也更低。&lt;/p&gt;

&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;

&lt;p&gt;如同 Algorithm 1 所表示的內循環(inner loop)的 optimizer A 在迭代 $k$ 次後，在 weight space 中，slow weights 的更新為與 fast weights k的線性插值(linearly interpolating)，$\theta - \phi$. 我們將 slow weights learning rate 表示為 $\alpha$, 在 slow weights 更新後，fast weights 會重新設定為 slow weights 的位置。&lt;/p&gt;

&lt;p&gt;Standard optimization method typically require carefully tuned learning rate to prevent &lt;strong&gt;oscillation&lt;/strong&gt; and &lt;strong&gt;slow converagence&lt;/strong&gt;. However, lookahead benefits from a larger learning rate in the inner loop. When oscillation in  the high curvature direction, the fast weights updates make rapid progress along the low curvature direction. The slow weights help smooth out the oscillation throught the parameter interpolation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slow weights trajectory&lt;/strong&gt; We can characterize the trajectory of the slow weights as an exponential moving average (EMA) of the final fast weights within each inner-loop, regardless of the inner optimizer. After k inner-loop steps we have:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
\phi_{t+1} &amp;= \phi_{t} + \alpha\left(\theta_{t, k} - \phi_{t}\right) \\
&amp;= \left(1-\alpha\right)\phi_{t} + \alpha\theta_{t, k} \\
&amp;= \left(1-\alpha\right)\left(\phi_{t-1} + \alpha\left(\theta_{t-1, k} - \phi_{t-1}\right) \right) +  \alpha\theta_{t, k} \\
&amp; \vdots \\
&amp;= \alpha\left[\theta_{t, k} + (1 - \alpha)\theta_{t-1, k} + \dots + (1 - \alpha)^{t-1}\theta_{0, k} \right]  + (1- \alpha)^{t}\theta_{0}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fast weights trajectory&lt;/strong&gt; Within each inner-loop, the trajectory of the fast weight depends on the choice of underlying optimizer. Given an optimization algorithm A that takes in an objective function $L$ and the current mini-batch training examples $d$, we have the update rule for the fast weights:
&lt;span  class=&#34;math&#34;&gt;\(
\theta_{t, i+1} = \theta_{t, i} + A\left(L, \theta_{t, i-1}, d\right)
\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We have the choice of maintaining, interpolating, or resetting the internal state (e.g. momentum) of the inner optimizer. Every choice improves convergence of the inner optimizer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computational complexity&lt;/strong&gt; Lookahead has a constant computational overhead due to parameter copying and basic arithmetic operations that is amortized across the k inner loop updates. The number of operations is $O\left(\frac{k+1}{k}\right)$ times that of the inner optimizer. Lookahead maintains a single additional copy of the number of learnable parameters in the model.&lt;/p&gt;

&lt;h2 id=&#34;empirical-analysis&#34;&gt;Empirical Analysis&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Robustness to inner optimization algorithm $k$ and $\alpha$&lt;/strong&gt; 在論文中使用 &lt;strong&gt;CIFAR&lt;/strong&gt; 的資料測試，Lookahead 能夠在不同的超參數設定下保有快速收斂的結果。在實驗中固定 slow weight step size $\alpha = 0.5$ 與 $k=5$，inner optimizer 選擇使用 SGD optimizer，測試不同的 learning rate 與 momentum 參數，結果顯示如下:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;lookahead_figure_8.png&#34; alt=&#34;lookahead_figure_8&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;同時實驗了在超參數固定的狀況下，inner optimizer 的 fast weights 在歷經不同 $k$ 與 $\alpha$ 的設定，結果如下圖:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;lookahead_figure_9.png&#34; alt=&#34;lookahead_figure_9&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inner loop and outer loop evalation&lt;/strong&gt; 為了更了解 Lookahead 的在 fast weights 與 slow weights 的更新狀況，透過 test accuracy 的結果來了解 weights 變化的趨勢。如下圖，在每次 inner loop 更新 fast weights 的情況下，對 test accuracy 造成大幅的下降，反映了在每次 inner loop 的更新都具有 high variance 的情況產生。然而，在 slow weights 的更新階段，降低了 variance 的影響，並且慢慢調整 test accuracy 的準確度。&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;lookahead_figure_10.png&#34; alt=&#34;lookahead_figure_10&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;code-implement&#34;&gt;Code implement&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/bojone/keras_lookahead&#34;&gt;https://github.com/bojone/keras_lookahead&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/lifeiteng/Optimizers&#34;&gt;https://github.com/lifeiteng/Optimizers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1907.08610v1.pdf&#34;&gt;Lookahead Optimizer: k steps forward, 1 step back&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.infoq.cn/article/Q7gBMEHNrd2rkjqV6CM3?utm_source=rss&amp;amp;utm_medium=article&#34;&gt;https://www.infoq.cn/article/Q7gBMEHNrd2rkjqV6CM3?utm_source=rss&amp;amp;utm_medium=article&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.infoq.cn/article/Q7gBMEHNrd2rkjqV6CM3&#34;&gt;https://www.infoq.cn/article/Q7gBMEHNrd2rkjqV6CM3&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://roymondliao.github.io/post/faster_rcnn/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/faster_rcnn/</guid>
      <description>&lt;p&gt;Faster R-CNN 是由 object detection 的大神 &lt;a href=&#34;https://www.rossgirshick.info/&#34;&gt;&lt;strong&gt;&lt;em&gt;Ross Girshick&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; 於 2015 年所提出的一個非常經典的目標檢測(object detection)的方法，當中提出了 &lt;strong&gt;Region Proposal Networks&lt;/strong&gt; 的方法應用在提取候選區域(reigon proposals) 取代了傳統的 Selective Search 的方式，大幅提升了目標檢測的精準度，也提升了整體計算的速度，另外 &lt;a href=&#34;http://kaiminghe.com/&#34;&gt;&lt;strong&gt;&lt;em&gt;Kaiming He&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; 博士也是共同作者。&lt;/p&gt;

&lt;p&gt;在介紹 Faster R-CNN 之前需要先了解何為 one stage 與 two stage，目前 object detection 的主流都是以 one stage 為基礎的演算法，建議可以參考下列兩篇很棒的文章:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@chih.sheng.huang821/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E4%BB%80%E9%BA%BC%E6%98%AFone-stage-%E4%BB%80%E9%BA%BC%E6%98%AFtwo-stage-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-fc3ce505390f&#34;&gt;什麼是one stage，什麼是two stage 物件偵測&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@chih.sheng.huang821/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC%E4%B8%8A%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%B5%90%E6%A7%8B%E8%AE%8A%E5%8C%96-e23fd928ee59&#34;&gt;物件偵測上的模型結構變化&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Faster R-CNN 主要是由四個部分來完成:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feature Extractor&lt;/li&gt;
&lt;li&gt;Region Proposal Network (RPN)&lt;/li&gt;
&lt;li&gt;Regoin Proposal Filter&lt;/li&gt;
&lt;li&gt;ROI Pooling&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;下圖為 Faster R-CNN 的簡易架構圖:&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;faster_rcnn_figure_2.png&#34;/&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: original paper&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;下圖是我參考了許多相關的部落格文章後，覺得在呈現 Faster R-CNN 的架構上最容易讓人了解的一張圖，可以搭配著上圖來對照一下！&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;faster_rcnn_arch.png&#34;/&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: https://zhuanlan.zhihu.com/p/44599606&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;1-feature-extractor&#34;&gt;1. Feature Extractor&lt;/h2&gt;

&lt;p&gt;透過五層的 conv layer 來取得 feature maps，作為後續的共享的 input。 作者採用了 &lt;strong&gt;ZF Net&lt;/strong&gt; 以及 &lt;strong&gt;VGG-16&lt;/strong&gt; 為主，依據 input size 的不同，最後 feature maps 的 W x H 也有所不同，但 channel 數是相同的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ZF Net 取第五層的 Feature maps，output 為 13 x 13 x 256&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ZFNet.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VGG-16 取第五層的 Feature maps，output 為 7 x 7 x 256&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;VGG16.png&#34; width=&#34;750px&#34; height=&#34;300px&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-region-proposal-networks&#34;&gt;2. Region Proposal Networks&lt;/h2&gt;

&lt;p&gt;在解析 RPN 的內容前，先來談談 RPN 與 Anchors 之所會被提出來，是來解決什麼樣的問題。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;簡略說明兩個方法面向的問題:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RPN&lt;/strong&gt; 主要的目的是為了產生只具有 foreground 的候選區域(region proposals)， 所以在 RPN 的輸出會有所謂的 foreground 與 background 的分類機率(此處的foreground 與 background 可以理解成是否含有 objects )。相較原本 Selective Search 用比較相鄰區域的相似度(顏色, 紋理, …等)合併再一起的方式，加快了運算的速度，同時也加強了物件檢測的精準度。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Anchors&lt;/strong&gt; 主要的目的是要來解決由於圖片大小不同，所以導致每張圖片在最後要將結果還原成原圖的座標時，會產生複雜的計算。固定 bounding boxes 的大小，可以加快計算的效率，也可以減少過多不必要的後選區域的產生。&lt;/p&gt;

&lt;p&gt;接下來進入 RPN 生成 region proposals 的解析:&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;faster_rcnn_figure_3.png&#34; width=&#34;500px&#34; height=&#34;350px&#34; /&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: original paper&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;對最後一層 conv layer 的 feature maps 用一個 n x n 的 spatial window 做 sliding window (stride=1, pad=1)，在論文中 &lt;strong&gt;n=3&lt;/strong&gt; 是因為對於較大的圖片在計算上比較有效率。此處可以將 sliding windows 這個過程想成是做一個 convolution 的過程來理解，output 則會是 ( feature map width, feature map height, channel )。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sliding windows 的結果 mapping 到 lower-dimensional feature，此處將帶入關鍵的 &lt;strong&gt;anchors&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Anchors&lt;/strong&gt; 是事先設定好的多組 bounding boxes，設定的組成是透過 image 對於 &lt;strong&gt;scale&lt;/strong&gt; 與 &lt;strong&gt;aspect ratio&lt;/strong&gt; 的參數設定來決定的。如下圖的右圖，論文中選擇 3 個 scale x 3 個 aspect ratio，所以共產生 9 個 archors。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;scale: the size of image, ex: $(128^2, 256^2, 512^2)$&lt;/li&gt;
&lt;li&gt;aspect ratio: width of image / height of image, ex: (1:1, 1:2, 2:1)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;表示在對 feature maps 進行 sliding windows 時，每個 sliding windows 對應原圖區域中的 9 個anchors，而 sliding windows 的中心點就對應 anchors 的中心點位置，藉由中心點與圖片的大小，就可以得到 sliding windows 的位置和原圖位置的映射關係(這邊可以用 receptive field 來理解)，就可以由原圖的位置與 ground truth 計算 &lt;a href=&#34;https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&#34;&gt;&lt;strong&gt;Intersection over Union(IOU)&lt;/strong&gt;&lt;/a&gt;，並且判斷是否有 objects。&lt;/p&gt;

&lt;p&gt;下面左圖示的紅點就是表示每個 sliding window 的中心點對應原圖的位置，而右圖是在表示 9 種不同大小的 anchor 在原圖的呈現。&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;anchors-centers.png&#34; width=&#34;400px&#34; height=&#34;350px&#34; /&gt; &lt;img src=&#34;anchors-boxes.png&#34; width=&#34;370px&#34; height=&#34;350px&#34; /&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;下圖呈現出當 9 個不同 anchor 映射到 sliding windows 的中心點，在原圖上的呈現，由這樣的步驟可以理解這 9 個 anchor 剛好足夠可以框出圖片上的所有 object。這邊要注意，如果 anchor boxes 超出原圖的邊框就要被忽略掉。&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;anchors-progress.png&#34; /&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;p&gt;在上圖可以看到，每個 sliding windows 映射到原圖，原圖上每個 sliding windows 的中心點對應 9 個 anchors，所以將 intermediate layer 所得到的 features 輸入給 兩個 sliding fully-connected layers。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;box-regression layer (reg layer): 輸出 4 x k個 boxes 的 encoding 座標值。&lt;/li&gt;
&lt;li&gt;box-classification layer (cls layer): 輸出 2 x k 個關於 forground / background 的機率&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;此方法有效的原因:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The anchors 與 ground truth 的 intersection-over-union (IOU) 重疊率很高&lt;/li&gt;
&lt;li&gt;IOU &amp;gt; 0.7 為 positive，IOU &amp;lt; 0.3 為 negative，介於 0.7 &amp;gt;= IOU &amp;gt;= 0.3 則忽略，期望 positive 的 proposal 包含前景的機率高，negative 包含背景的機率高。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Faster R-CNN 的缺點:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在單一 scale 的 feature map 做 object localization and Classification，而且還是 scale=1/32 下，在小物件偵測效果相對不佳，有可能在down-scale時小物件的特徵就消失了&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&#34;&gt;https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@tanaykarmarkar/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9&#34;&gt;https://medium.com/@tanaykarmarkar/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/baderange/article/details/79643478&#34;&gt;https://blog.csdn.net/baderange/article/details/79643478&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/lanran2/article/details/54376126&#34;&gt;https://blog.csdn.net/lanran2/article/details/54376126&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Word2Vec</title>
      <link>https://roymondliao.github.io/post/word2vec/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/word2vec/</guid>
      <description>&lt;p&gt;最近剛好看到一篇關於 &lt;a href=&#34;https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb&#34;&gt;Skip-gram word2vec&lt;/a&gt;的介紹，內文寫的淺顯易懂，衍生的閱讀也十分詳細，決定動手寫篇記錄下來。&lt;/p&gt;

&lt;p&gt;人對於文字的理解，可以很簡單的就能了解字面的意義，但是對於機器來說，要如何理解文字是一個很困難的問題。
要如何讓機器來理解文字的意義？ 透過將文字轉換成向量，來讓機器能夠讀的懂，所以其實文字對於機器來說只是數字，而我們在做的就只是數字的遊戲。&lt;/p&gt;

&lt;h2 id=&#34;word-embeddings&#34;&gt;Word embeddings&lt;/h2&gt;

&lt;p&gt;在將字詞轉換成向量的實作中，大家常用的方法肯定是 &lt;strong&gt;one-hot-encoding&lt;/strong&gt;，但是 one-hot-encoding 在計算上卻是非常沒有效率的方式，如果一篇文章中總共有50,000的單詞，用 one-hot-encoding 來表示某個單詞的話，將會變成1與49999個0的向量表示。就如同下圖表示，如果要做 matrix multiplication 的話，那將會浪費許多的計算資源。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;one_hot_encoding.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;透過 &lt;strong&gt;Word Embedding&lt;/strong&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; 可以有效的解決上述的問題。 Embedding 可以想成與 full connected layer 一樣，將這個 layer 稱做為 embedding layer ， weight 則稱為 embedding weights。藉由這樣的概念，可以省略掉 multiplication 的過程，直接透過 hidden layer 的 weigth matrix 來當作輸入字詞的 word vector。之所以可以這樣來執行是因為在處理 one-hot-encoding 與 weight matrix 相乘的結果，其實就是 matrix 所對應&amp;quot;詞&amp;quot;的索引值所得到的結果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;lookup_matrix.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;舉例來說： &amp;quot;heart&amp;quot; 的在 one-hot-encoding 的索引位置為958，我們直接拿取 heart 所對應 hidden layer 的值，也就是 embedding weights 的第958列(row)，這樣的過程叫做 &lt;strong&gt;embedding lookup&lt;/strong&gt;，而 hidden layer 的神經元數量則為 &lt;strong&gt;embedding dimension&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;tokenize_lookup.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;另一個解釋是在於 word2vec 是一個三層架構，分別是 input layer、hidden layer、output layer，但是在 hidden layer 並沒有非線性的 activation function，由於 input layer 是經由 one-hot-encoding 過的資訊，所以在 hidden layer 所取得的值，其實就是對應輸入層得值；另外一提 output layer 的 activation function 是 sigmoid。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;word2vec_weight_matrix_lookup_table.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;原文中最後提到的三個主要重點：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The embedding lookup table is just a weight matrix.&lt;/li&gt;
&lt;li&gt;The embedding layer is just a hidden layer.&lt;/li&gt;
&lt;li&gt;The lookup is just a shortcut for the matrix multiplication.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;

&lt;p&gt;介紹完 word embedding 後，要來介紹 word2vec algorithm 中的兩個 model：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Skip-gram&lt;/li&gt;
&lt;li&gt;CBOW(Continous Bag-Of-Words)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#39;word2vec_architectures.png&#39; width=500&gt;&lt;/p&gt;

&lt;h3 id=&#34;skipgram-model&#34;&gt;Skip-gram model&lt;/h3&gt;

&lt;p&gt;用下列兩張圖來解釋 skip-gram model 的結構，假設model是一個simple logistic regression(softmax)，左邊的圖表示為概念上的架構(conceptual architecture)，右邊的圖則為實作上的架構(implemented architectures)，雖然圖的架構有些微不同，但是實際上是一樣的，並沒有任何的改變。
首先定義參數：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;V - Vocabulary Size (Number of unique words in the corpora)&lt;/li&gt;
&lt;li&gt;P - The Projection or the Embedding Layer&lt;/li&gt;
&lt;li&gt;D - Dimensionality of the Embedding Space&lt;/li&gt;
&lt;li&gt;b - Size of a single Batch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#39;skip_gram.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;由左圖可以了解到，Skip-gram model 的 &lt;strong&gt;input(X)&lt;/strong&gt; 為一個單詞，而你的目標，也就是你的  &lt;strong&gt;output(Y)&lt;/strong&gt; 為相鄰的單詞。換句話就是在一句話中，選定句子當中的任意詞作為 input word，而與 input word 相鄰的字詞則為 model 的所要預測的目標(labels)，最後會得到相鄰字詞與 input word 相對應的機率。&lt;/p&gt;

&lt;p&gt;但是上述的想法會出現一個問題，就是你只提供一個字詞的訊息，然而要得到相鄰字詞出現的機率，這是很困難的一件事，效果也不佳。所以這邊提出兩個方法:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;針對&amp;quot;相鄰字詞&amp;quot;這部分，加入了 &lt;strong&gt;window size&lt;/strong&gt; 的參數做調整&lt;/li&gt;
&lt;li&gt;將輸出所有字詞的方式轉成一對一成對的方式&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;舉例來說：&lt;strong&gt;&amp;quot;The dog barked at the mailman.&amp;quot;&lt;/strong&gt; 這樣一句話，選定 dog 做為 input，設定window size = 2，則 &amp;quot;dag&amp;quot; 下上兩個相鄰字詞為 &lt;strong&gt;[&#39;the&#39;, &#39;barked&#39;, &#39;at&#39;]&lt;/strong&gt; 就會是我們的 output。此外將原本的(input: &#39;dag&#39;, output: &#39;[&#39;the&#39;, &#39;barked&#39;, &#39;at&#39;]) 轉換成 (input: &#39;dag&#39;, output: &#39;the&#39;), (input: &#39;dag&#39;, output: &#39;barked&#39;), (input: &#39;dag&#39;, output: &#39;at&#39;) 這樣一對一的方式。這樣的過程就如同右圖 implemented architectures。&lt;/p&gt;

&lt;p&gt;下圖解釋一個語句的training samples產生:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;training_data.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;所以當training samples: (brown, fox)的數量越多時，輸入&lt;code&gt;brown&lt;/code&gt;得到&lt;code&gt;fox&lt;/code&gt;的機率越高。&lt;/p&gt;

&lt;h4 id=&#34;model-details&#34;&gt;Model Details&lt;/h4&gt;

&lt;p&gt;Input layer: 字詞經過 one-hot-encoding 的向量表示。
hidden layer: no activation function，上述介紹 embedding layer 已經解釋過。
output layer: use softmax regression classifier，output 的結果介於0與1之間，且加總所有的值和為1。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;skip_gram_net_arch.png&#39; width=750&gt;&lt;/p&gt;

&lt;p&gt;假設輸入的 word pair 為(ants, able)，則模型的目標是 &lt;span  class=&#34;math&#34;&gt;\(max P\left(able | ants \right)\)&lt;/span&gt;，同時也需要滿足 &lt;span  class=&#34;math&#34;&gt;\(min P\left(other \space words | ants \right)\)&lt;/span&gt;，這裡利用 log-likehood function 作為目標函數。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
P\left(able | ants \right) = softmax\left( X_{ants 1\times 10000} \cdot W_{10000 \times 300}\right) 
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
Y = sotfmax(Y) =\frac{exp(X_{1 \times 300} \cdot W_{300 \times 1})}{\sum_{i=1}^{10000} exp(X_{1 \times 300}^i \cdot W_{300 \times 1})}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;log-likehood function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
L(W) = P\left(able \mid ants \right)^{y=able} \times P\left(other \space words | ants \right)^{y=other \space words}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Objective function可以表示如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
LogL\left(W\right) 
&amp; = \{y = target \space word\} \{logP\left(able | ants \right) + logP\left(other \space words | ants \right)\}\\
&amp; = \sum_{i}^{10000}\{ y = target \space word\}logP\left( word_{i} | ants \right)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;之後就是 Maxmium log-likehood function。
由上述的介紹，會發現一個問題，這是一個非常巨大的 NN model。假設 word vectors 為300維的向量，具有10,000個字詞時，總共會有 &lt;span  class=&#34;math&#34;&gt;\(300 \times10000 = 3\)&lt;/span&gt; 百萬的 weight 需要訓練!! 這樣的計算 gradient descent 時造成模型的訓練時間會非常的久。&lt;/p&gt;

&lt;p&gt;對於這問題，Word2Vec 的作者在&lt;a href=&#34;https://arxiv.org/pdf/1310.4546.pdf&#34;&gt;paper&lt;/a&gt;第二部分有提出以下的解決方法:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Treating common word pairs or phrases as single words in their model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subsampling frequent words&lt;/strong&gt; to decrease the number of training examples.&lt;/li&gt;
&lt;li&gt;Modifying the optimization objective with a technique they called &lt;strong&gt;Negative Sampling&lt;/strong&gt;, which causes each training sample to update only a small percentage of the model’s weights&lt;/li&gt;
&lt;li&gt;A computationally efficient approximation of the full softmax is the &lt;strong&gt;hierarchical softmax&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Subsampling 與 Negative Sampling 這兩個實作方法不只加速了模型的訓練速度，同時也提升模型的準確率。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Words pairs and phrases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如說 New York City 在字詞訓練時，會拆成 New、York、City 三個字詞，但是這樣分開來無法表達出原意，所以將&amp;quot;New York City&amp;quot;組合為一個單詞做訓練。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Subsampling frequent words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在剛剛透過下圖解釋了相關的原理，但是這會發現兩個問題，一是像是出現(the, fox)這樣的 pair，並沒有告訴我們有用的資訊，並且&amp;quot;the&amp;quot;是常出現的字詞；二是有大量像是&amp;quot;the&amp;quot;這類的字詞出現在文章，要如何有意義地學習&amp;quot;the&amp;quot;字詞表示的意思。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;training_data.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;subsamplig 針對這樣的狀況，透過一個機率值來判斷詞是否應該保留。機率值計算公式如下:
&lt;span  class=&#34;math&#34;&gt;\(
P\left( w_{i} \right) = \left( \sqrt{\frac{Z(w_{i})}{0.001} + 1} \right) \cdot \frac{0.001}{Z(w_{i})}
\)&lt;/span&gt;
其中$P\left( w_{i} \right)$表示$w_{i}$的出現機率，0.001為默認值。具體結果如下圖，字詞出現的頻率越高，相對被採用的機率越低。
&lt;img src=&#39;subsample_func_plot.png&#39; width=600&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Negative Sampling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此方法目的是希望只透過正確的目標字詞來小改動 weight。比如說，ward pair (fox, qiuck)，在這個例子中&amp;quot;qiuck&amp;quot;為目標字詞，所以標記為1，而其他與 fox 無相關的字詞標記為0，就稱之為 negative sampling，這樣的 output 就有像是 one-hot vector，只有正確的目標字詞為1(positive word)，其他為0(negative word)。
  至於 Negative sampling size 需要多少，底下是Word2Vec的作者給出的建議:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以假設是以上面描述的狀況，qiuck 則為 postive word，另外加上5個 negative word，output 值為6個值，總共有 &lt;span  class=&#34;math&#34;&gt;\(300 \times 6 = 1800\)&lt;/span&gt; 個 weight 需要更新，這樣只佔了原本300萬的 weight 0.06%而已!&lt;/p&gt;

&lt;p&gt;該如何挑選 negative sampling? 則是透過 &lt;code&gt;unigram distribution&lt;/code&gt; 的機率來挑選， 在C語言實作 word2vec 的程式碼中得到以下公式&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  P\left( w_{i} \right) = \frac{f\left( w_{i} \right)^{\frac{3}{4}} }{\sum_{j=0}^{n} \left( f\left( w_{j}\right)^{\frac{3}{4}} \right)}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;$\frac{3}{4}$次方的選擇是來至於實驗測試的結果。&lt;/p&gt;

&lt;p&gt;Define Objective function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(
  log \space \sigma \left( v_{I}^{T} \cdot v_{o} \right) - \sum_{i=1}^{k} E_{w_{i} -&gt; P_{v}}[\sigma\left( -v_{w_{i}}^{T}v_{wI} \right)]
  \)&lt;/span&gt;
  $ Note \space \sigma(-x) = 1 - \sigma(x)$&lt;/p&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;An implementation &lt;a href=&#34;http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/&#34;&gt;skip-gram of word2vec&lt;/a&gt; from Thushan Ganegedara&lt;/li&gt;
&lt;li&gt;An implementation &lt;a href=&#34;http://www.thushv.com/natural_language_processing/word2vec-part-2-nlp-with-deep-learning-with-tensorflow-cbow/&#34;&gt;CBOW of word2vec&lt;/a&gt; from Thushan Ganegedara&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&#34;&gt;Word2Vec Tutorial Part1&lt;/a&gt; and &lt;a href=&#34;http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/&#34;&gt;Part2&lt;/a&gt; from Chris McCormick&lt;/li&gt;
&lt;li&gt;Deep understand with word2vec form &lt;a href=&#34;http://cpmarkchang.logdown.com/posts/773062-neural-network-word2vec-part-1&#34;&gt;Mark Chang&#39;s Blog (Chinese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;plus-reference&#34;&gt;Plus reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/fastText&#34;&gt;FastText&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;word embedding: 將單詞word映射到另一個空間，其中這個映射具有injective和structure-preserving的特性。
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>DropBlock</title>
      <link>https://roymondliao.github.io/post/dropblock/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/dropblock/</guid>
      <description>&lt;p&gt;Dropout 相關方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Dropout: 完全隨機丟棄 neuron&lt;/li&gt;
&lt;li&gt;Sparital Dropout: 按 channel 隨機丟棄&lt;/li&gt;
&lt;li&gt;Stochastic Depth: 按 res block 隨機丟棄&lt;/li&gt;
&lt;li&gt;DropBlock: 每個 feature map 上按 spatial square 隨機丟棄&lt;/li&gt;
&lt;li&gt;Cutout: 在 input layer 按 spatial square 隨機丟棄&lt;/li&gt;
&lt;li&gt;DropConnect: 只在連接處丟，不丟 neuron&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650751601&amp;amp;idx=5&amp;amp;sn=6ba09bea3acb116eb9f4902af5261e72&amp;amp;chksm=871a860fb06d0f194c4c0452e53d21cc6c537b4e33a5aea4e3c0a067db0c46d41168afabcc0c&amp;amp;scene=21#wechat_redirect&#34;&gt;DropBlock&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Idea&lt;/li&gt;
&lt;li&gt;一般的 Dropout 都是用在 fully connection layer，而在 convolutional network 上使用 dropout 的意義並不大，該文章則認為因為在每一個 feature maps 的位置都具有一個 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/28492837&#34;&gt;receptive field&lt;/a&gt;，僅對單一像素位置進行 dropout 並不能降低 feature maps 學習特徵範圍，也就是說，network 能夠特過&lt;strong&gt;相鄰位置&lt;/strong&gt;的特徵值去學習，也不會特別加強去學習保留下來的訊息。既然對於單獨的對每個位置進行 dropout 並無法提高 network 本身的泛化能力，那就以區塊的概念來進行 dropout，反而更能讓 network 去學習保留下來的訊息，而加重特徵的權重。&lt;/li&gt;
&lt;li&gt;Method

&lt;ul&gt;
&lt;li&gt;不同 feature maps 共享相同的 dropblock mask，在相同的位置丟棄訊息&lt;/li&gt;
&lt;li&gt;每一層的 feature maps 使用各自的 dropblock mask&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Parameters&lt;/li&gt;
&lt;li&gt;block size: 控制要讓 value of feature maps 歸為 0 的區塊大小&lt;/li&gt;
&lt;li&gt;$ \gamma $: 用來控制要丟棄特徵的數量&lt;/li&gt;
&lt;li&gt;keep_prob: 與 dropout 的參數相同&lt;/li&gt;
&lt;li&gt;Code implement&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/DHZS/tf-dropblock/blob/master/nets/dropblock.py&#34;&gt;https://github.com/DHZS/tf-dropblock/blob/master/nets/dropblock.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shenmbsw/tensorflow-dropblock/blob/master/dropblock.py&#34;&gt;https://github.com/shenmbsw/tensorflow-dropblock/blob/master/dropblock.py&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bernoulli distrubtion:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset_default_graph()
&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Graph()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;as_default() &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; g: 
    mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;placeholder(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32, [None])
    input_shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;placeholder(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32, [None, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
    shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape(input_shape))
    &lt;span style=&#34;color:#75715e&#34;&gt;# method 1&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 用 uniform distributions 產生值，再透過 sign 轉為 [-1, 1], 最後透過 relu 將 -1 轉換為 0&lt;/span&gt;
    uniform_dist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random_uniform(shape, minval&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, maxval&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
    sign_dist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sign(mean &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; uniform_dist)
    bernoulli &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;relu(sign_dist)
    &lt;span style=&#34;color:#75715e&#34;&gt;# method 2&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# probs 可以為多個 p, 對應 shape, 產生 n of p 的 bernoulli distributions&lt;/span&gt;
    noise_dist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;distributions&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Bernoulli(probs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;])
    mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; noise_dist&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sample(shape)
&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Session(graph&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;g) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; sess:
    tmp_array &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros([&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uint8) 
    tmp_array[:,:&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#75715e&#34;&gt;#Orange left side array[:,100:] = [0, 0, 255] #Blue right side&lt;/span&gt;
    batch_array &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array([tmp_array]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
    uniform, sign, bern &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sess&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;run([uniform_dist, sign_dist, bernoulli], feed_dict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{mean: [&lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;], input_shape:batch_array})    &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;DropBlock implement:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; tensorflow &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; tf
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; tensorflow.python.keras &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; backend &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; K
&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;DropBlock&lt;/span&gt;(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keras&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;layers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Layer) :
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, keep_prob, block_size, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        super(DropBlock, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__(&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keep_prob &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; float(keep_prob) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(keep_prob, int) &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; keep_prob
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(block_size)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compute_output_shape&lt;/span&gt;(self, input_shape):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; input_shape
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;build&lt;/span&gt;(self, input_shape):
        _, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;h, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;w, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;channel &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input_shape&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;as_list()
        &lt;span style=&#34;color:#75715e&#34;&gt;# pad the mask&lt;/span&gt;
        bottom &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; right &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
        top &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; left &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; bottom
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;padding &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], [top, bottom], [left, right], [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]]
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_keep_prob()
        super(DropBlock, self)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;build(input_shape)
        
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;set_keep_prob&lt;/span&gt;(self, keep_prob&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;This method only support Eager Execution&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; keep_prob &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; None:
            self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keep_prob &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; keep_prob
        w, h &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_float(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;w), tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_float(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;h)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gamma &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keep_prob) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (w &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; h) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; ((w &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (h &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_create_mask&lt;/span&gt;(self, input_shape):
        sampling_mask_shape &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack([input_shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], 
                                        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;h &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, 
                                        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;w &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,
                                        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;channel])
        mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DropBlock&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_bernoulli(sampling_mask_shape, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gamma)
        &lt;span style=&#34;color:#75715e&#34;&gt;# 擴充行列，並給予0值，依據 paddings 參數給予的上下左右值來做擴充，mode有三種模式可選，可參考 document&lt;/span&gt;
        mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pad(tensor&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mask, paddings&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;padding, mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;CONSTANT&amp;#39;&lt;/span&gt;) 
        mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max_pool(value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mask, 
                              ksize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;block_size, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], 
                              strides&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], 
                              padding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SAME&amp;#39;&lt;/span&gt;)
        mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mask
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mask
        
    &lt;span style=&#34;color:#a6e22e&#34;&gt;@staticmethod&lt;/span&gt;    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_bernoulli&lt;/span&gt;(shape, mean):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;relu(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sign(mean &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;random_uniform(shape, minval&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, maxval&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)))
    
    &lt;span style=&#34;color:#75715e&#34;&gt;# The call function is a built-in function in &amp;#39;tf.keras&amp;#39;.&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;call&lt;/span&gt;(self, inputs, training&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None, scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;drop&lt;/span&gt;():
            mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_create_mask(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape(inputs))
            output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inputs &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; mask
            output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cond(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;constant(scale, dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bool) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(scale, bool) &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; scale,
                             true_fn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt;: output &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_float(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size(mask)) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reduce_sum(mask),
                             false_fn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt;: output)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; output
        
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; training &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None:
            training &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; K&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;learning_phase()
        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cond(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;logical_or(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;logical_not(training), tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;equal(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keep_prob, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;)),
                         true_fn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt;: inputs,
                         false_fn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;drop)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; output&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Testing&lt;/span&gt;
a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;placeholder(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32, [None, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;])
keep_prob &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;placeholder(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float32)
training &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;placeholder(tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bool)

drop_block &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DropBlock(keep_prob&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;keep_prob, block_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; drop_block(inputs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;a, training&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;training)

sess &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Session()
feed_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {a: np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones([&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]), keep_prob: &lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, training: True}
c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sess&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;run(b, feed_dict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;feed_dict)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(c[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, :, :, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650752571&amp;amp;idx=1&amp;amp;sn=8417645148afd8eebdb79c91b37a7409&amp;amp;chksm=871a8245b06d0b53115d79f1ce42bc5a03aad5d038fe51c2f237c5848c41c51c5b756aaa8937&amp;amp;scene=21#wechat_redirect&#34;&gt;Targeted Dropout&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/1367373&#34;&gt;https://cloud.tencent.com/developer/article/1367373&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>XGBoost</title>
      <link>https://roymondliao.github.io/post/xgboost/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/xgboost/</guid>
      <description>&lt;h2 id=&#34;review-note&#34;&gt;Review note&lt;/h2&gt;

&lt;p&gt;Bagging&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Concept&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bagging involves creating mulitple copies of the original training data set using the boostrap, fitting a seperate decision tree to each copy, and then combining all of the trees in order to create a single predcitive model. &lt;font color=&#34;#F44336&#34;&gt;Notably, each tree is built on a bootstrap data set, independent of the other trees.&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Random Forest&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Boosting&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Concept&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Boosting works in a similar way with bagging, except that the trees are grown &lt;font color=&#34;#F44336&#34;&gt;sequentially&lt;/font&gt;. Each tree is grown using information from previous grown trees.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Adaboost - &lt;a href=&#34;https://en.wikipedia.org/wiki/AdaBoost&#34;&gt;Yoav Freund and Robert Schapire&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;根據樣本的誤差來調整樣本的權重，誤差較大的樣本給予較高的權重，反之亦然。藉此著重訓練分類錯誤的資料，進而來增進模型的準確度。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient boosting - &lt;a href=&#34;https://statweb.stanford.edu/~jhf/&#34;&gt;Friedman, J.H.&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;根據當前模型的殘差來調整權重的大小，其目的是為了降低殘差。通過迭代的方式，使損失函數(loss function)達到最小值(局部最小)。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Method&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GBDT(Grandien Boosting Decision Tree)&lt;/li&gt;
&lt;li&gt;XGBoost(eXtreme Gradient Boosting)](&lt;a href=&#34;https://github.com/dmlc/xgboost&#34;&gt;https://github.com/dmlc/xgboost&lt;/a&gt;) - &lt;a href=&#34;http://homes.cs.washington.edu/~tqchen/&#34;&gt;Tianqi Chen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LightGBM(Light Gradient Boosting Machine)](&lt;a href=&#34;https://github.com/Microsoft/LightGBM&#34;&gt;https://github.com/Microsoft/LightGBM&lt;/a&gt;) - Microsoft Research Asia&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;advantages-of-xgboost&#34;&gt;Advantages of XGBoost&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;傳統 GBDT 是以 CART 作為分類器的基礎，但是XGBoost還可以支援線性分類器，另外在 objective function 可以加入 L1 regularization 和 L2 regularization 的方式來優化，降低了 model 的 variance，避免 overfitting 的狀況。&lt;/li&gt;
&lt;li&gt;GBDT 在優化部分只使用到泰勒展開式的一階導數，但 XGBoost 則使用到二階導數，所以在預測準確度上提供更多的訊息。&lt;/li&gt;
&lt;li&gt;XGBoost 支援平行運算與分布式運算，所以相較傳統的GBDT在計算速度上有大幅的提升。XGBoost 的平行並非是在 tree 的維度做平行化處理，而是在 features 的維度上做平行化處理，因為 tree 的生長是需要前一次迭代的結果的來進行 tree 的生長。&lt;/li&gt;
&lt;li&gt;對 features 進行預排序的處理，然後保存排序的結構，以利後續再 tree 的分裂上能夠快速的計算每個 features 的 gain 的結果，最終選擇 gain 最大的 feature 進行分裂，這樣的方式就可以平行化處理。&lt;/li&gt;
&lt;li&gt;加入 shrinkage 和 column subsampling 的優化技術。&lt;/li&gt;
&lt;li&gt;有效地處理 missing value 的問題。&lt;/li&gt;
&lt;li&gt;先從頭到尾建立所有可能的 sub trees，再從底到頭的方式進行剪枝(pruning)。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;disadvantages-of-xgboost&#34;&gt;Disadvantages of XGBoost&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;在每次的迭代過程中，都需要掃過整個訓練集合多次。如果把整個訓練集合存到 memory 會限制數據的大小;如果不存到 memory 中，反覆的讀寫訓練集合也會消耗非常多的時間。&lt;/li&gt;
&lt;li&gt;預排序方法(pre-sorted): 由於需要先針對 feature 內的 value 進行排序並且保存排序的結果，以利於後續的 gain 的計算，但在這個計算上就需要消耗兩倍的 memory 空間，來執行。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf&#34;&gt;http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mlnote.com/2016/10/05/a-guide-to-xgboost-A-Scalable-Tree-Boosting-System/&#34;&gt;http://mlnote.com/2016/10/05/a-guide-to-xgboost-A-Scalable-Tree-Boosting-System/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zybuluo.com/yxd/note/611571#机器学习的关键元素&#34;&gt;https://www.zybuluo.com/yxd/note/611571#机器学习的关键元素&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting#Shrinkage&#34;&gt;https://en.wikipedia.org/wiki/Gradient_boosting#Shrinkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhanpengfang.github.io/418home.html&#34;&gt;http://zhanpengfang.github.io/418home.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;paper&#34;&gt;Paper&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Fridman J.H. (1999). &lt;a href=&#34;http://statweb.stanford.edu/~jhf/ftp/trebst.pdf&#34;&gt;Greedy Function Approximation: A Gradient Boosting Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tianqi Chen, Carlos Gusetrin (2016). &lt;a href=&#34;http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf&#34;&gt;XGBoost: A Scalable Tree Boosting System&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;doing&#34;&gt;Doing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/&#34;&gt;https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://adeshpande3.github.io/adeshpande3.github.io/Applying-Machine-Learning-to-March-Madness&#34;&gt;https://adeshpande3.github.io/adeshpande3.github.io/Applying-Machine-Learning-to-March-Madness&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
