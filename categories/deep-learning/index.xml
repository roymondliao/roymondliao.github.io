<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Roymond Liao</title>
    <link>https://roymondliao.github.io/categories/deep-learning/</link>
      <atom:link href="https://roymondliao.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - © 2020</copyright><lastBuildDate>Fri, 28 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://roymondliao.github.io/img/icon-192.png</url>
      <title>Deep Learning</title>
      <link>https://roymondliao.github.io/categories/deep-learning/</link>
    </image>
    
    <item>
      <title>Transformer Part 2 - Attention</title>
      <link>https://roymondliao.github.io/post/2019-12-16_transformer_part2/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/2019-12-16_transformer_part2/</guid>
      <description>&lt;h1 id=&#34;attention-mechanism&#34;&gt;Attention Mechanism&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt; 的概念在 2014 年被 Bahdanau et al. [Paper 1] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 $${y_1, \cdots, y_{t-1}}$$ 的條件下字詞 $y_{t}$ 的機率，所以 decoder  的定義是在有序的條件下所有預測字詞的聯合機率：&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
p(\mathrm{y}) &amp;amp; = \prod_{t=1}^T p(y_t | {y_1, \cdots, y_{t-1}}, c) \tag 1 \&lt;br&gt;
\mathrm{y} &amp;amp; = (y_1, \cdots, y_T)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;在第 $t$ 個字詞，字詞 $y_t$ 的條件機率：&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
p(y_t | {y_1, \cdots, y_{t-1}}, c) = g(y_{t-1}, s_t, c) \tag 2
\end{align}
$$&lt;/p&gt;
&lt;p&gt;當中 $g$ 唯一個 nonlinear function，$s_t$ 為 hidden state，c 為 context vector。&lt;/p&gt;
&lt;p&gt;而在 Attention model 中，作者將 decoder 預測下一個字詞的的條件機率重新定義為：&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
p(y_i | {y_1, \cdots, y_{i-1}}, \mathrm{x}) = g(y_{i-1}, s_t, c_i) \tag 3
\end{align}
$$&lt;/p&gt;
&lt;p&gt;當中 $s_i$ 表示 RNN 在 $i$ 時間的 hiddent state。&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
s_i = f\left(s_{i-1}, y_{i-1}, c_i\right) \tag 4
\end{align}
$$&lt;/p&gt;
&lt;p&gt;將式子 (3) 與 (2) 相比就可以發現，每一個預測字詞 $y_i$ 對於 context vector 的取得，由原本都是固定的 C  轉變成 每個字詞預測都會取得不同的 $C_i$。&lt;/p&gt;
&lt;p&gt;Bahdanau Attention model 的架構如圖一：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Context vector $c_i$ 是取決於 sequence of annotations $$(h_1, h_2, \cdots, h_{T_x})$$ 的訊息，annotation $h_i$ 包含了在第 $i$ 步下， input sequence 輸入到 econder 的訊息。計算方法是透過序列權重加總 annotation $h_i$，公式如下：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
c_i = \displaystyle\sum_{j=1}^{T_x}\alpha_{ij}h_j \tag5
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;其中 $i$ 表示 decoder 在第 $i$ 個字詞，$j$ 表示 encoder 中第 $j$ 個詞。&lt;/p&gt;
&lt;p&gt;$\alpha_{ij} $ 則稱之為 attention distribution，可以用來衡量 input sequence 中的每個文字對 output sequence 中的每個文字所帶來重要性的程度，計算方式如下
：
$$
\begin{align}
\alpha_{ij} &amp;amp; = softmax(e_{ij}) \&lt;br&gt;
&amp;amp; = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} \tag6 \&lt;br&gt;
\end{align}
$$&lt;/p&gt;
&lt;p&gt;$$
e_{ij} = a(s_{i-1}, h_j) \tag7
$$&lt;/p&gt;
&lt;p&gt;**計算 attention  score $e_{ij}$ 中 $a$ 表示為 alignment model (對齊模型)，是衡量 input sequence 在位置 $j$ 與 output sequence 位置 $i$ 這兩者之間的關係**。
這邊作者為了解決在計算上需要 $T_{x} \times T_{y}$ 的計算量，所以採用了 singlelayer multilayer perceptron 的方式來減少計算量，其計算公式：&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
a(s_{i-1}, h_j) = v_a^Ttanh(W_aS_{i-1} + U_ah_j) \tag8
\end{align}
$$&lt;/p&gt;
&lt;p&gt;其中 $W_a \in R^{n\times n}，U_a \in R^{n \times 2n}，v_a \in R^n$ 都是 weight。&lt;/p&gt;
&lt;p&gt;另外作者在此採用了 BiRNN(Bi-directional RNN) 的 forward 與 backward 的架構，由圖一可以得知&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Forward hidden state 為 $$(\overrightarrow{h_1}, \cdots, \overrightarrow{h_{T_x}})$$&lt;/li&gt;
&lt;li&gt;Backward hidden state 為 $$(\overleftarrow{h_1}, \cdots, \overleftarrow{h_{T_x}})$$&lt;/li&gt;
&lt;li&gt;Concatenate forward 與 backward 的 hidden state，所以 annotation $h_j$ 為 $$\left[\overrightarrow{h_j^T};\overleftarrow{h_j^T}\right]^T$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這樣的方式更能理解句子所要表達的意思，並得到更好的預測結果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;例如以下兩個句子的比較：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;我喜歡蘋果，因為它很好吃。&lt;/li&gt;
&lt;li&gt;我喜歡蘋果，因為它很潮。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下圖為 Bahdanau Attention model 的解析可以與圖一對照理解，這樣更能了解圖一的結構：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;需要注意的一點是在最一開始的 decoder hidden state $S_0$ 是採用 encoder 最後一層的 output&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;下圖為論文中英文翻譯成法語的 attention distribution：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;在上圖中 $[European \space Economic \space Area]$ 翻譯成$ [zone \space \acute{a}conomique \space europ\acute{e}enne] $ 的注意力分數上，模型成功地專注在對應的字詞上。&lt;/p&gt;
&lt;h1 id=&#34;attention-mechanism-family&#34;&gt;Attention Mechanism Family&lt;/h1&gt;
&lt;p&gt;Attention score function:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Attention score function&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Dot-product&lt;/td&gt;
&lt;td&gt;$e_{ij} = S_i^Th_j$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;General&lt;/td&gt;
&lt;td&gt;$e_{ij} = S_i^TWh_j$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Additive&lt;/td&gt;
&lt;td&gt;$e_{ij} = v^Ttanh(WS_{i-1} + Uh_j) $&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scaled Dot-product&lt;/td&gt;
&lt;td&gt;$e_{ij} = \frac{S_i^Th_j}{\sqrt{d}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;hard-attention--soft-attention&#34;&gt;Hard Attention &amp;amp; Soft Attention&lt;/h3&gt;
&lt;p&gt;Xu et al. [Paper 2] 對於圖像標題(caption)的生成研究中提出了 hard attention 與 soft attention 的方法，作者希望透過 attention mechanism 的方法能夠讓 caption 的生成從圖像中獲得更多有幫助的訊息。下圖為作者所提出的模型架構：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;strong&gt;模型結構&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Encoder&lt;/p&gt;
&lt;p&gt;在 encoder 端模型使用 CNN 來提取 low-level 的卷積層特徵，每一個特徵都對應圖像的一個區域&lt;/p&gt;
&lt;p&gt;$$
a = {a_1, \dots, a_L}, a_i \in R^D
$$
總共有 $L$ 個特徵，特徵向量維度為 $D$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder&lt;/p&gt;
&lt;p&gt;採用 LSTM 模型來生成字詞，而因應圖片的內容不同，所以標題的長度是不相同的，作者將標題 $y$ encoded 成一個 one-hot encoding 的方式來表示
$$
y = {y_1, \dots, y_C}, y_i \in R^K
$$
K 為字詞的數量，C 為標題的長度。下圖為作者這本篇論文所採用的 LSTM 架構：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;endpmatrix&#34;&gt;利用 affine transformation 的方式  $$T_{s, t} : R^s \rightarrow R^t$$ 來表達 LSTM 的公式：
$$
\begin{pmatrix}
i_t \&lt;br&gt;
f_t \&lt;br&gt;
o_t \&lt;br&gt;
g_t
\end{pmatrix}&lt;/h1&gt;
&lt;p&gt;\begin{pmatrix}
\sigma \&lt;br&gt;
\sigma \&lt;br&gt;
\sigma \&lt;br&gt;
tanh
\end{pmatrix}
T_{D+m+n, n}
\begin{pmatrix}
Ey_{t-1} \&lt;br&gt;
h_{t-1} \&lt;br&gt;
\hat{Z_t}
\end{pmatrix}  \tag1 \&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
c_t &amp;amp; = f_t \odot c_{t-1} + i_t \odot g_t \tag2 \&lt;br&gt;
h_t &amp;amp; = o_t \odot tanh(c_t) \tag3
\end{align}
$$&lt;/p&gt;
&lt;p&gt;其中&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$$i_t$$ : input gate&lt;/li&gt;
&lt;li&gt;$$f_t$$ : forget gate&lt;/li&gt;
&lt;li&gt;$$o_t$$ : ouput gate&lt;/li&gt;
&lt;li&gt;$$g_t$$ : canaidate cell&lt;/li&gt;
&lt;li&gt;$$c_t$$ : memory cell&lt;/li&gt;
&lt;li&gt;$$h_t$$ : hidden state&lt;/li&gt;
&lt;li&gt;$$Ey_{t-1}$$ 是詞 $$y_{t-1}$$ 的 embedding vector，$$E \in R^{m \times k}$$ 為 embedding matrix，m 為 embedding dimention&lt;/li&gt;
&lt;li&gt;$$\hat{Z} \in R^D$$ 是 context vector，代表捕捉特定區域視覺訊息的上下文向量，與時間 $t$ 有關，所以是一個動態變化的量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;特別注意的是作者在給定 memory state 與 hidden state 的初始值的計算方式使用了兩個獨立的多層感知器(MLP)，其輸入是各個圖像區域特徵的平均，計算公式如下：
$$
c_0 = f_{init, c}( \frac{1}{L} \sum_{i}^L a_i) \&lt;br&gt;
h_0 = f_{init, h}( \frac{1}{L} \sum_{i}^L a_i) \&lt;br&gt;
$$
以及作者為了計算在 $t$ 時間下所關注的 context vector $\hat{Z_t}$ **定義了 attention machansim $\phi$ 為在 $t$ 時間，對於每個區域 $i$ 計算出一個權重 $$\alpha_{ti}$$ 來表示產生字詞 $y_t$ 需要關注哪個圖像區域  annotation vectors $a_i, i=1, \dots, L$ 的訊息。**權重 $$\alpha_i$$ 的產生是透過輸入 annotation vector $a_i$ 與前一個時間的 hidden state  $h_{t-1}$ 經由 attention model $f_{att}$ 計算所產生。&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
e_{ti} = f_{att}(a_i, h_{t-1}) \tag4 \&lt;br&gt;
\alpha_{ti} = \frac{exp(e_{ti})}{\sum_{k=1}^{L}exp{e_{tk}}} \tag5 \&lt;br&gt;
\hat{Z_t} = \phi({a_i}, {\alpha_{i}}) \tag6
\end{align}
$$&lt;/p&gt;
&lt;p&gt;有了上述的資訊，在生成下一個 $t$ 時間的字詞機率可以定義為：
$$
p(y_t | a, y_1, y_2, \dots, y_{t-1}) \propto exp(L_o(Ey_{t-1} + L_hh_t + L_z\hat{Z_t})) \tag7
$$
其中 $$L_o \in R^{K \times m}, L_h \in R^{m \times n}, L_z \in R^{m \times D}$$，m 與 n 分別為 embedding dimension 與 LSTM dimension。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;對於函數 $\phi$ 作者提出了兩種 attention  machansim，對應於將權重附加到圖像區域的兩個不同策略。根據上述的講解，搭配下圖為 Xu et al. [Paper 2] 的模型架構解析，更能了解整篇論文模型的細節：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h4 id=&#34;hard-attention-stochastic-hard-attention&#34;&gt;Hard attention (Stochastic Hard Attention)&lt;/h4&gt;
&lt;p&gt;在 hard attention 中定義區域變數(location variables) $s_{t, i}$ 為在 t 時間下，模型決定要關注的圖像區域，用 one-hot 的方式來表示，要關注的區域 $i$ 為 1，否則為 0。&lt;/p&gt;
&lt;p&gt;$s_{t, i}$ 被定為一個淺在變數(latent variables)，並且以 **multinoulli distriubtion** 作為參數 $\alpha_{t, i}$ 的分佈，而 $\hat{Z_t}$ 則被視為一個隨機變數，公式如下：
$$
p(s_{t, i} = 1 | s_{j, t}, a) = \alpha_{t, i} \tag8\&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;$$
\hat{Z_t} = \sum_{i} s_{t, i}a_i \tag9
$$&lt;/p&gt;
&lt;p&gt;定義新的 objective functipn $L_s$ 為 marginal log-likelihood $\text{log }p(y|a)$ 的下界(lower bound)
$$
\begin{align}
L_s &amp;amp; = \sum_s p(s|a)\text{log }p(y|s,a) \&lt;br&gt;
&amp;amp; \leq \text{log } \sum_s p(s|a)p(y|s,a) \&lt;br&gt;
&amp;amp; = \text{log }p(y|a)
\end{align}
$$
在後續的 $L_s$ 推導求解的過程，作者利用了&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Monte Carlo 方法來估計梯度，利用 moving average 的方式來減小梯度的變異數&lt;/li&gt;
&lt;li&gt;加入了 multinouilli distriubtion 的 entropy term $H[s]$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;透過這兩個方法提升隨機算法的學習，作者在文中也提到，最終的公式其實等價於 &lt;strong&gt;Reinforce learing&lt;/strong&gt;。作者在論文中有列出推導的公式，有興趣的可以直接參考論文。&lt;/p&gt;
&lt;h4 id=&#34;soft-attention-deterministic-soft-attention&#34;&gt;Soft attention (Deterministic Soft Attention)&lt;/h4&gt;
&lt;p&gt;Soft attention 所關注的圖像區域並不像 hard attention 在特定時間只關注特定的區域，在 soft attention 中則是每一個區域都關注，只是關注的程度不同。透過對每個圖像區域 $a_{i}$ 與對應的 weight $\alpha_{t,i}$ ，$\hat{Z}_t$ 就可以直接對權重做加總求和，公式如下：
$$
\mathbb{E}_{p(s_t|a)}[\hat{Z_t}] = \sum_{i=1}^L \alpha_{t,i}a_i
$$
這計算方式將 weight vector $\alpha_i$ 參數化，讓公式是可微的，可以透過 backpropagation 做到 end-to-end 的學習。其方法是參考前面所介紹的 Bahdanau attention 而來。&lt;/p&gt;
&lt;p&gt;由於公式(7)的定義了生成下一個 $t$ 時間的字詞機率，所以在這邊作者定義了 $$n_t = L_o(Ey_{t-1} + L_hh_t + L_z\hat{Z_t})$$，透過 expect context vector&lt;/p&gt;
&lt;p&gt;另外 soft attention 在最後做文字的預測時作者定義了 softmax $k^{th}$ 的 normalized weighted geometric mean。
$$
\begin{align}
NWGM[p(y_t=k|a)] &amp;amp; = \frac{\prod_i exp(n_{t,k,i})^{p(s_{t,i} = 1 | a)}}{\sum_j\prod_i exp(n_{t,j,i})^{p(s_{t,i} = 1 | a)}} \&lt;br&gt;
&amp;amp; = \frac{exp\left(\mathbb{E_{p(s_t) | a}[n_{t,k}]}\right)}{\sum_j exp\left(\mathbb{E_{p(s_t) | a}[n_{t,j}]}\right)}
\end{align}
$$&lt;/p&gt;
&lt;h3 id=&#34;global-attention--local-attention&#34;&gt;Global Attention &amp;amp; Local Attention&lt;/h3&gt;
&lt;p&gt;總結來說：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Attention 要實現的就是在 decoder 的不同時刻可以關注不同的圖像區域或是句子中的文字，進而可以生成更合理的詞。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;refenece&#34;&gt;Refenece&lt;/h2&gt;
&lt;p&gt;Paper:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34;&gt;Dzmitry Bahdanau, KyungHyun Cho Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.03044.pdf&#34;&gt;Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.04025.pdf&#34;&gt;Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.02874&#34;&gt;Sneha Chaudhari, Gungor Polatkan , Rohan Ramanath , Varun Mithal, An Attentive Survey of Attention Models(2019)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Illustrate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/37601161h&#34;&gt;https://zhuanlan.zhihu.com/p/37601161h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/31547842&#34;&gt;https://zhuanlan.zhihu.com/p/31547842&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.floydhub.com/attention-mechanism/#bahdanau-atth&#34;&gt;https://blog.floydhub.com/attention-mechanism/#bahdanau-atth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf&#34;&gt;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/Determined22/p/6914926.html&#34;&gt;https://www.cnblogs.com/Determined22/p/6914926.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&#34;&gt;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf&#34;&gt;http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Tutorial:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/nmt#background-on-the-attention-mechanism&#34;&gt;Neural Machine Translation (seq2seq) Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/text/transformerG&#34;&gt;https://www.tensorflow.org/tutorials/text/transformerG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;Guide annotating the paper with PyTorch implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Visualization:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jessevig/bertviz&#34;&gt;https://github.com/jessevig/bertviz&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Transformer Part 1 - Seq2Seq</title>
      <link>https://roymondliao.github.io/post/2019-12-16_transformer_part1/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/2019-12-16_transformer_part1/</guid>
      <description>&lt;p&gt;一開始接觸 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇論文是從 &lt;a href=&#34;https://www.youtube.com/playlist?list=PLqFaTIg4myu8t5ycqvp7I07jTjol3RCl9&#34;&gt;Kaggle Reading Group&lt;/a&gt; 這個 channel 開始，非常推薦可以跟著一起讀!!&lt;/p&gt;

&lt;p&gt;主持人 &lt;a href=&#34;https://www.kaggle.com/rtatman&#34;&gt;Rachael Atman&lt;/a&gt; 本身是 Kaggle 的 Data Scientist，她的導讀我覺得是流暢的，但她自己本身有說過並非是 NLP 領域的專家，所以在 kaggle reading group 裡閱讀的論文也有可能是她完全沒接觸過的，整個 channel 帶給你的就是一個啟發，讓你覺得有人跟你一起閱讀的感覺，然後過程中也些人會在 channel 的 chat room 提出一些看法或是連結，Rachael 本身也會提出自己的見解，可以多方面參考。&lt;/p&gt;

&lt;p&gt;在跟完整個 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 的影片後，還是有太多細節是不清楚的，因為自己本身也不是這個領域的，所以開始追論文中所提到的一些關鍵名詞，就開始從 $seq2seq \rightarrow attention \rightarrow self-attention$。這中間 有太多知識需要記錄下來，所以將論文的內容分成三部曲，來記錄閱讀下來的點點滴滴:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Part 1: Sequence to sequence model 起源&lt;/li&gt;
&lt;li&gt;Part 2: Attention 與 Self-attention 的理解&lt;/li&gt;
&lt;li&gt;Part 3: Transformer 的架構探討與深入理解&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要談論 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇 paper 就必須從 seq2seq 講起，seq2seq 全名為 Sequence to Sequence[1]，是一個 Encoder - Decoder 架構的模型，在 2014 年被提出，被廣泛的應用於 Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.&lt;/p&gt;

&lt;h3 id=&#34;sequence-to-sequence-model&#34;&gt;Sequence to sequence model&lt;/h3&gt;

&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;簡單來說，期望輸入一串序列(source)，輸出一串序列(target)，而這個 source 與 target 可以是什麼呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果以 machine translation 來說，任務是中翻英，輸入是一句中文，而輸出則會是一句英文。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果是 text summarization，輸入則會是一段文章，而輸出則會是一段摘要&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這就是 seq2seq model 所要解決的問題，在輸入一些資訊後，經過 encoder-decoder 的訓練，可以得到相對應的回答或是其他資訊。&lt;/p&gt;

&lt;h4 id=&#34;model&#34;&gt;Model&lt;/h4&gt;

&lt;p&gt;模型的架構也非常簡單，就如下圖所示：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./seq2seq.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;圖一&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;假設今天我們執行中翻英的任務，輸入(source: X)是一句中文，可以是 &amp;quot;我愛機器學習&amp;quot;，而輸出(target: Y)則會是 &amp;quot;I love machine learning&amp;quot;，所以整個訓練的步驟如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Input sequence $(x_1, x_2, x_3, \dots, x_s)$  經過 embedding layer 的轉換，得到每個 word 的 embedding vector&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Encoder 把所有輸入序列 embedding vector 消化後，將資訊壓縮轉換為一個向量 $C$，稱之為 context vector&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
   \begin{align}
   h_s^{e} &amp; = f_{enc}\left(h_{s-1}^{e}, e_{x_{s-1}}, W_{enc}\right) \\
   C &amp; = h_s^e \text {，最後一步的 hidden state} \\
   C &amp; = q(h_s^e) \text {，最後一步的 hidden state 做 transform } \\
   C &amp; = q\left(h_1^e, h_2^e, \dots, h_s^e\right) \text {，每一步的 hidden state 做 transform }
   \end{align}
   \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $f_{enc}(\cdot)$ 表示 Encoder 中的 RNN function，參數為 $W_{enc}$。$e_{x_s}$ 表示 $x_s$ 的 embedding vector，$h_s^e$ 表示在時間 $s$ 的 hidden state，$C$ 可以表示為 Encoder 最後的 hidden state 或是經過函數 $q(\cdot)$ 的轉換。&lt;/p&gt;

&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Decoder 則根據 context vector 的資訊來生成文字，output sequence $(y_1, y_2, y_3, \dots, y_t)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
   \begin{align}
   h_0^{d} &amp; = C \\
   h_{t}^{d} &amp; = f_{dec}\left(h_{t-1}^{d}, e_{y_{t-1}}, W_{dec}\right) \\
   O_{t} &amp; = g\left(h_t^d\right)
   \end{align}
   \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $h_{0}^{d}$ 為 context vector 傳進來當作 Decoder 的初始 hidden state，$f_{dec}(\cdot)$ 表示 Decoder 中的 RNN function，參數為 $W_{dec}$。$h_{t}^{d}$ 表示 Decoder在時間 $t$ 的 hidden state，$e_{y_{t}}$ 表示前一步的所得到的 $y_{t-1}$ 結果當作輸入，$y_0$ 都是以特殊索引 &amp;lt;BOS&amp;gt; 當作輸入。
   $g(\cdot)$ 為 output layer，一般都是 softmax function。&lt;/p&gt;

&lt;p&gt;過程就只是簡單的三個步驟，雖然看起來簡單，但當中有些細節是需要注意的。&lt;/p&gt;

&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;特殊索引&lt;/p&gt;

&lt;p&gt;在每個句子做 one-hot-encoder 的轉換時，會在句子的前後加上 &amp;lt;BOS&amp;gt; 與 &amp;lt;EOS&amp;gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BOS: Begin of sequence，在預測的時候我們並沒有對應的答案，所以會先以 &amp;lt;BOS&amp;gt; 當作 $Y_0$ 的 target input&lt;/li&gt;
&lt;li&gt;EOS: End of sequence，用意是要告訴 model 當出現這個詞的時候就是停止生成文字，如果沒有這個詞，模型會無限迴圈的一直生成下去&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了上述的 &amp;lt;BOS&amp;gt; 與 &amp;lt;EOS&amp;gt; 外，還有 &amp;lt;PAD&amp;gt; 與 &amp;lt;UNK&amp;gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PAD: 由於 RNN 的 parameters 是共享的，所以在 input 的維度就需要保持相同，但並不是每個句子的長度都是同樣的，有的可能長度是 3 ，有的長度可能是 5，所以為了處理不同 input sequence 長度不同的狀況，增加了 &amp;lt;PAD&amp;gt; 的字詞，來讓每次 &lt;strong&gt;batch&lt;/strong&gt; 的 input sequence 的長度都是相同的&lt;/li&gt;
&lt;li&gt;UNK: 如果輸入的字詞在 corpus 是沒有出現過的，就會用 &amp;lt;UNK&amp;gt; 索引來代替&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Encoder layer 與 Decoder layer 的選擇&lt;/p&gt;

&lt;p&gt;Encoder 與 Decoder 中的 RNN function 可以是 simple RNN / LSTM / GRU，又或者是一個 bidirectional LSTM 的架構在裡面，也可以是一個 multi layer LSTM&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Teacher forcing&lt;/p&gt;

&lt;p&gt;在 training model 時 ，為了提高 model 的準確度與訓練速度，採用了 &lt;a href=&#34;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&#34;&gt;Teacher forcing training&lt;/a&gt; 方法，圖二就是 teacher foring 的概念，在 training 的時候直接告訴 model 實際的答案，省去 model 自己去尋找到正確的答案。另外也有提出 &lt;a href=&#34;https://arxiv.org/abs/1610.09038&#34;&gt;Professor Forcing&lt;/a&gt; 的做法，尚未理解這方法的概念，提供當作參考。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./teacher_forcing.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;
  圖二 (Image credit: &lt;a href=&#34;https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c&#34;&gt;LINK&lt;/a&gt;)
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h4 id=&#34;prediction&#34;&gt;Prediction&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Beam search&lt;/p&gt;

&lt;p&gt;在 prediction  model 時，每一步的 output 都是要計算出在 corpus 中生成最可能的那個字詞&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
\hat{y_t} = argmax\space p_{\theta}(y | \hat{y}_{1:(t-1)}) 
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 &lt;span  class=&#34;math&#34;&gt;\(\hat{y}_{1:(t-1)} = \hat{y}_1,\dots,\hat{y}_{t-1}\)&lt;/span&gt; 為前面 $t-1$ 步所生成的字詞。以一個簡單的概念來思考，每一步的 output 都是該步所得到的最大條件機率，那這樣的 greedy search 所得到的結果對於我們的目標並非是最優的，得到的是每個字詞的最大條件機率，而並非是整個句子的最大條件機率，所以這樣的狀況下你所得的的翻譯可能不會是最適合的。&lt;/p&gt;

&lt;p&gt;Beam search 就是為了解決這樣的問題而提出的，在每一步的生成過程中，生成 $B$ 的最可能的文字序列作為約束，其中 $B$ 的大小為 beam width，是一個 hyperparamter。$B$ 值越大可以得到更好的結果，但相對的計算量也增加。&lt;/p&gt;

&lt;p&gt;過程就如同圖三所示:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;建立一個 search tree，該 root 為一個開始符號 (非序列的第一個詞)&lt;/li&gt;
&lt;li&gt;由序列的左到右開始，順序生成目標語言序列，同時成長對應的搜尋樹&lt;/li&gt;
&lt;li&gt;在生成序列的每一步，對  search tree 的每個 leaf node，選取 $B$ 個擁有最高條件機率的生成單詞，並生成 B 個子節點。&lt;/li&gt;
&lt;li&gt;在成長搜尋樹後，進行剪枝的工作，只留下 B 個最高條件機率的葉節點後，再進行下一個位置的序列生成。
&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Beam search 的實作可以參考 Blog:[4]。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./beam_search.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;圖三 (Image credit: &lt;a href=&#34;https://distill.pub/2017/ctc/&#34;&gt; LINK&lt;/a&gt;)
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;另外要注意在使用 beam search 所謂遇到的問題，在 Andrew Ng 大師的&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;課程&lt;/a&gt;中提到&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;消除長度對計算機率影響（Length Normalization）&lt;/li&gt;
&lt;li&gt;如何選擇 Beam Width 參數（The Choice of Beam Width）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;詳細的解說可以參考&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;課程&lt;/a&gt;或是 Blog:[3]。&lt;/p&gt;

&lt;h4 id=&#34;problems&#34;&gt;Problems&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;所有資訊只壓縮成一個 context vector，input sequence 的資訊很難全部保存在一個固定維度的向量裡&lt;/li&gt;
&lt;li&gt;當 sequence 的長度很長時，在 decoder 解碼時，由於 Recurrent neural network 的依賴問題，容易丟失 input sequence 的訊息&lt;/li&gt;
&lt;/ol&gt;

&lt;hr&gt;

&lt;p&gt;在理解完 Seq2Seq model 後，所遇到的問題該如何解決? 那就是要靠 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇 paper 的主要重點之一 &lt;code&gt;Attention mechanism(注意力機制)&lt;/code&gt;，
這部分將會在 part 2 的時候介紹。&lt;/p&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34;&gt;Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, Sequence to Sequence Learning with Neural Networks(2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/pdf/1610.09038.pdf&#34;&gt;Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio, Professor Forcing: A New Algorithm for Training Recurrent Networks(2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2014)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&#34;https://arxiv.org/abs/1508.04025&#34;&gt;Minh-Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Blog:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&#34;&gt;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdfh&#34;&gt;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdfh&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10208587&#34;&gt;https://ithelp.ithome.com.tw/articles/10208587&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&#34;https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/&#34;&gt;https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&#34;https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-%E4%B8%AD%E6%96%87%E7%89%88-2714bbd92727&#34;&gt;Seq2seq pay Attention to Self Attention: Part 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&#34;https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-%E4%B8%AD%E6%96%87%E7%89%88-ef2ddf8597a4&#34;&gt;Seq2seq pay Attention to Self Attention: Part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;&gt;Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&#34;http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/&#34;&gt;http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&#34;https://www.zhihu.com/question/54356960&#34;&gt;https://www.zhihu.com/question/54356960&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://roymondliao.github.io/post/2019-05-14_faster_rcnn/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/2019-05-14_faster_rcnn/</guid>
      <description>&lt;p&gt;Faster R-CNN 是由 object detection 的大神 &lt;a href=&#34;https://www.rossgirshick.info/&#34;&gt;&lt;strong&gt;&lt;em&gt;Ross Girshick&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; 於 2015 年所提出的一個非常經典的目標檢測(object detection)的方法，當中提出了 &lt;strong&gt;Region Proposal Networks&lt;/strong&gt; 的方法應用在提取候選區域(reigon proposals) 取代了傳統的 Selective Search 的方式，大幅提升了目標檢測的精準度，也提升了整體計算的速度，另外 &lt;a href=&#34;http://kaiminghe.com/&#34;&gt;&lt;strong&gt;&lt;em&gt;Kaiming He&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; 博士也是共同作者。&lt;/p&gt;

&lt;p&gt;在介紹 Faster R-CNN 之前需要先了解何為 one stage 與 two stage，目前 object detection 的主流都是以 one stage 為基礎的演算法，建議可以參考下列兩篇很棒的文章:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@chih.sheng.huang821/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E4%BB%80%E9%BA%BC%E6%98%AFone-stage-%E4%BB%80%E9%BA%BC%E6%98%AFtwo-stage-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-fc3ce505390f&#34;&gt;什麼是one stage，什麼是two stage 物件偵測&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@chih.sheng.huang821/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC%E4%B8%8A%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%B5%90%E6%A7%8B%E8%AE%8A%E5%8C%96-e23fd928ee59&#34;&gt;物件偵測上的模型結構變化&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Faster R-CNN 主要是由四個部分來完成:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feature Extractor&lt;/li&gt;
&lt;li&gt;Region Proposal Network (RPN)&lt;/li&gt;
&lt;li&gt;Regoin Proposal Filter&lt;/li&gt;
&lt;li&gt;ROI Pooling&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;下圖為 Faster R-CNN 的簡易架構圖:&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;faster_rcnn_figure_2.png&#34;/&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: original paper&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;下圖是我參考了許多相關的部落格文章後，覺得在呈現 Faster R-CNN 的架構上最容易讓人了解的一張圖，可以搭配著上圖來對照一下！&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;faster_rcnn_arch.png&#34;/&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: https://zhuanlan.zhihu.com/p/44599606&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;1-feature-extractor&#34;&gt;1. Feature Extractor&lt;/h2&gt;

&lt;p&gt;透過五層的 conv layer 來取得 feature maps，作為後續的共享的 input。 作者採用了 &lt;strong&gt;ZF Net&lt;/strong&gt; 以及 &lt;strong&gt;VGG-16&lt;/strong&gt; 為主，依據 input size 的不同，最後 feature maps 的 W x H 也有所不同，但 channel 數是相同的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ZF Net 取第五層的 Feature maps，output 為 13 x 13 x 256&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ZFNet.png&#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VGG-16 取第五層的 Feature maps，output 為 7 x 7 x 256&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;VGG16.png&#34; width=&#34;750px&#34; height=&#34;300px&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-region-proposal-networks&#34;&gt;2. Region Proposal Networks&lt;/h2&gt;

&lt;p&gt;在解析 RPN 的內容前，先來談談 RPN 與 Anchors 之所會被提出來，是來解決什麼樣的問題。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;簡略說明兩個方法面向的問題:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RPN&lt;/strong&gt; 主要的目的是為了產生只具有 foreground 的候選區域(region proposals)， 所以在 RPN 的輸出會有所謂的 foreground 與 background 的分類機率(此處的foreground 與 background 可以理解成是否含有 objects )。相較原本 Selective Search 用比較相鄰區域的相似度(顏色, 紋理, …等)合併再一起的方式，加快了運算的速度，同時也加強了物件檢測的精準度。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Anchors&lt;/strong&gt; 主要的目的是要來解決由於圖片大小不同，所以導致每張圖片在最後要將結果還原成原圖的座標時，會產生複雜的計算。固定 bounding boxes 的大小，可以加快計算的效率，也可以減少過多不必要的後選區域的產生。&lt;/p&gt;

&lt;p&gt;接下來進入 RPN 生成 region proposals 的解析:&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;faster_rcnn_figure_3.png&#34; width=&#34;500px&#34; height=&#34;350px&#34; /&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: original paper&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;對最後一層 conv layer 的 feature maps 用一個 n x n 的 spatial window 做 sliding window (stride=1, pad=1)，在論文中 &lt;strong&gt;n=3&lt;/strong&gt; 是因為對於較大的圖片在計算上比較有效率。此處可以將 sliding windows 這個過程想成是做一個 convolution 的過程來理解，output 則會是 ( feature map width, feature map height, channel )。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sliding windows 的結果 mapping 到 lower-dimensional feature，此處將帶入關鍵的 &lt;strong&gt;anchors&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Anchors&lt;/strong&gt; 是事先設定好的多組 bounding boxes，設定的組成是透過 image 對於 &lt;strong&gt;scale&lt;/strong&gt; 與 &lt;strong&gt;aspect ratio&lt;/strong&gt; 的參數設定來決定的。如下圖的右圖，論文中選擇 3 個 scale x 3 個 aspect ratio，所以共產生 9 個 archors。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;scale: the size of image, ex: $(128^2, 256^2, 512^2)$&lt;/li&gt;
&lt;li&gt;aspect ratio: width of image / height of image, ex: (1:1, 1:2, 2:1)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;表示在對 feature maps 進行 sliding windows 時，每個 sliding windows 對應原圖區域中的 9 個anchors，而 sliding windows 的中心點就對應 anchors 的中心點位置，藉由中心點與圖片的大小，就可以得到 sliding windows 的位置和原圖位置的映射關係(這邊可以用 receptive field 來理解)，就可以由原圖的位置與 ground truth 計算 &lt;a href=&#34;https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/&#34;&gt;&lt;strong&gt;Intersection over Union(IOU)&lt;/strong&gt;&lt;/a&gt;，並且判斷是否有 objects。&lt;/p&gt;

&lt;p&gt;下面左圖示的紅點就是表示每個 sliding window 的中心點對應原圖的位置，而右圖是在表示 9 種不同大小的 anchor 在原圖的呈現。&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;anchors-centers.png&#34; width=&#34;400px&#34; height=&#34;350px&#34; /&gt; &lt;img src=&#34;anchors-boxes.png&#34; width=&#34;370px&#34; height=&#34;350px&#34; /&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;下圖呈現出當 9 個不同 anchor 映射到 sliding windows 的中心點，在原圖上的呈現，由這樣的步驟可以理解這 9 個 anchor 剛好足夠可以框出圖片上的所有 object。這邊要注意，如果 anchor boxes 超出原圖的邊框就要被忽略掉。&lt;/p&gt;

&lt;figure&gt;
    &lt;center&gt;&lt;img src=&#34;anchors-progress.png&#34; /&gt;&lt;/center&gt;
  &lt;figcaption&gt;&lt;center&gt;Image credit: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&lt;/center&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;p&gt;在上圖可以看到，每個 sliding windows 映射到原圖，原圖上每個 sliding windows 的中心點對應 9 個 anchors，所以將 intermediate layer 所得到的 features 輸入給 兩個 sliding fully-connected layers。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;box-regression layer (reg layer): 輸出 4 x k個 boxes 的 encoding 座標值。&lt;/li&gt;
&lt;li&gt;box-classification layer (cls layer): 輸出 2 x k 個關於 forground / background 的機率&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;此方法有效的原因:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The anchors 與 ground truth 的 intersection-over-union (IOU) 重疊率很高&lt;/li&gt;
&lt;li&gt;IOU &amp;gt; 0.7 為 positive，IOU &amp;lt; 0.3 為 negative，介於 0.7 &amp;gt;= IOU &amp;gt;= 0.3 則忽略，期望 positive 的 proposal 包含前景的機率高，negative 包含背景的機率高。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Faster R-CNN 的缺點:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在單一 scale 的 feature map 做 object localization and Classification，而且還是 scale=1/32 下，在小物件偵測效果相對不佳，有可能在down-scale時小物件的特徵就消失了&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&#34;&gt;https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@tanaykarmarkar/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9&#34;&gt;https://medium.com/@tanaykarmarkar/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/baderange/article/details/79643478&#34;&gt;https://blog.csdn.net/baderange/article/details/79643478&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/lanran2/article/details/54376126&#34;&gt;https://blog.csdn.net/lanran2/article/details/54376126&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Word2Vec</title>
      <link>https://roymondliao.github.io/post/2019-05-02_word2vec/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/2019-05-02_word2vec/</guid>
      <description>&lt;p&gt;最近剛好看到一篇關於 &lt;a href=&#34;https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb&#34;&gt;Skip-gram word2vec&lt;/a&gt;的介紹，內文寫的淺顯易懂，衍生的閱讀也十分詳細，決定動手寫篇記錄下來。&lt;/p&gt;

&lt;p&gt;人對於文字的理解，可以很簡單的就能了解字面的意義，但是對於機器來說，要如何理解文字是一個很困難的問題。
要如何讓機器來理解文字的意義？ 透過將文字轉換成向量，來讓機器能夠讀的懂，所以其實文字對於機器來說只是數字，而我們在做的就只是數字的遊戲。&lt;/p&gt;

&lt;h2 id=&#34;word-embeddings&#34;&gt;Word embeddings&lt;/h2&gt;

&lt;p&gt;在將字詞轉換成向量的實作中，大家常用的方法肯定是 &lt;strong&gt;one-hot-encoding&lt;/strong&gt;，但是 one-hot-encoding 在計算上卻是非常沒有效率的方式，如果一篇文章中總共有50,000的單詞，用 one-hot-encoding 來表示某個單詞的話，將會變成1與49999個0的向量表示。就如同下圖表示，如果要做 matrix multiplication 的話，那將會浪費許多的計算資源。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;one_hot_encoding.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;透過 &lt;strong&gt;Word Embedding&lt;/strong&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; 可以有效的解決上述的問題。 Embedding 可以想成與 full connected layer 一樣，將這個 layer 稱做為 embedding layer ， weight 則稱為 embedding weights。藉由這樣的概念，可以省略掉 multiplication 的過程，直接透過 hidden layer 的 weigth matrix 來當作輸入字詞的 word vector。之所以可以這樣來執行是因為在處理 one-hot-encoding 與 weight matrix 相乘的結果，其實就是 matrix 所對應&amp;quot;詞&amp;quot;的索引值所得到的結果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;lookup_matrix.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;舉例來說： &amp;quot;heart&amp;quot; 的在 one-hot-encoding 的索引位置為958，我們直接拿取 heart 所對應 hidden layer 的值，也就是 embedding weights 的第958列(row)，這樣的過程叫做 &lt;strong&gt;embedding lookup&lt;/strong&gt;，而 hidden layer 的神經元數量則為 &lt;strong&gt;embedding dimension&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;tokenize_lookup.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;另一個解釋是在於 word2vec 是一個三層架構，分別是 input layer、hidden layer、output layer，但是在 hidden layer 並沒有非線性的 activation function，由於 input layer 是經由 one-hot-encoding 過的資訊，所以在 hidden layer 所取得的值，其實就是對應輸入層得值；另外一提 output layer 的 activation function 是 sigmoid。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;word2vec_weight_matrix_lookup_table.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;原文中最後提到的三個主要重點：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The embedding lookup table is just a weight matrix.&lt;/li&gt;
&lt;li&gt;The embedding layer is just a hidden layer.&lt;/li&gt;
&lt;li&gt;The lookup is just a shortcut for the matrix multiplication.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;

&lt;p&gt;介紹完 word embedding 後，要來介紹 word2vec algorithm 中的兩個 model：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Skip-gram&lt;/li&gt;
&lt;li&gt;CBOW(Continous Bag-Of-Words)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#39;word2vec_architectures.png&#39; width=500&gt;&lt;/p&gt;

&lt;h3 id=&#34;skipgram-model&#34;&gt;Skip-gram model&lt;/h3&gt;

&lt;p&gt;用下列兩張圖來解釋 skip-gram model 的結構，假設model是一個simple logistic regression(softmax)，左邊的圖表示為概念上的架構(conceptual architecture)，右邊的圖則為實作上的架構(implemented architectures)，雖然圖的架構有些微不同，但是實際上是一樣的，並沒有任何的改變。
首先定義參數：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;V - Vocabulary Size (Number of unique words in the corpora)&lt;/li&gt;
&lt;li&gt;P - The Projection or the Embedding Layer&lt;/li&gt;
&lt;li&gt;D - Dimensionality of the Embedding Space&lt;/li&gt;
&lt;li&gt;b - Size of a single Batch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#39;skip_gram.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;由左圖可以了解到，Skip-gram model 的 &lt;strong&gt;input(X)&lt;/strong&gt; 為一個單詞，而你的目標，也就是你的  &lt;strong&gt;output(Y)&lt;/strong&gt; 為相鄰的單詞。換句話就是在一句話中，選定句子當中的任意詞作為 input word，而與 input word 相鄰的字詞則為 model 的所要預測的目標(labels)，最後會得到相鄰字詞與 input word 相對應的機率。&lt;/p&gt;

&lt;p&gt;但是上述的想法會出現一個問題，就是你只提供一個字詞的訊息，然而要得到相鄰字詞出現的機率，這是很困難的一件事，效果也不佳。所以這邊提出兩個方法:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;針對&amp;quot;相鄰字詞&amp;quot;這部分，加入了 &lt;strong&gt;window size&lt;/strong&gt; 的參數做調整&lt;/li&gt;
&lt;li&gt;將輸出所有字詞的方式轉成一對一成對的方式&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;舉例來說：&lt;strong&gt;&amp;quot;The dog barked at the mailman.&amp;quot;&lt;/strong&gt; 這樣一句話，選定 dog 做為 input，設定window size = 2，則 &amp;quot;dag&amp;quot; 下上兩個相鄰字詞為 &lt;strong&gt;[&#39;the&#39;, &#39;barked&#39;, &#39;at&#39;]&lt;/strong&gt; 就會是我們的 output。此外將原本的(input: &#39;dag&#39;, output: &#39;[&#39;the&#39;, &#39;barked&#39;, &#39;at&#39;]) 轉換成 (input: &#39;dag&#39;, output: &#39;the&#39;), (input: &#39;dag&#39;, output: &#39;barked&#39;), (input: &#39;dag&#39;, output: &#39;at&#39;) 這樣一對一的方式。這樣的過程就如同右圖 implemented architectures。&lt;/p&gt;

&lt;p&gt;下圖解釋一個語句的training samples產生:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;training_data.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;所以當training samples: (brown, fox)的數量越多時，輸入&lt;code&gt;brown&lt;/code&gt;得到&lt;code&gt;fox&lt;/code&gt;的機率越高。&lt;/p&gt;

&lt;h4 id=&#34;model-details&#34;&gt;Model Details&lt;/h4&gt;

&lt;p&gt;Input layer: 字詞經過 one-hot-encoding 的向量表示。
hidden layer: no activation function，上述介紹 embedding layer 已經解釋過。
output layer: use softmax regression classifier，output 的結果介於0與1之間，且加總所有的值和為1。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;skip_gram_net_arch.png&#39; width=750&gt;&lt;/p&gt;

&lt;p&gt;假設輸入的 word pair 為(ants, able)，則模型的目標是 &lt;span  class=&#34;math&#34;&gt;\(max P\left(able | ants \right)\)&lt;/span&gt;，同時也需要滿足 &lt;span  class=&#34;math&#34;&gt;\(min P\left(other \space words | ants \right)\)&lt;/span&gt;，這裡利用 log-likehood function 作為目標函數。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
P\left(able | ants \right) = softmax\left( X_{ants 1\times 10000} \cdot W_{10000 \times 300}\right) 
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
Y = sotfmax(Y) =\frac{exp(X_{1 \times 300} \cdot W_{300 \times 1})}{\sum_{i=1}^{10000} exp(X_{1 \times 300}^i \cdot W_{300 \times 1})}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;log-likehood function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
L(W) = P\left(able \mid ants \right)^{y=able} \times P\left(other \space words | ants \right)^{y=other \space words}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Objective function可以表示如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
LogL\left(W\right) 
&amp; = \{y = target \space word\} \{logP\left(able | ants \right) + logP\left(other \space words | ants \right)\}\\
&amp; = \sum_{i}^{10000}\{ y = target \space word\}logP\left( word_{i} | ants \right)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;之後就是 Maxmium log-likehood function。
由上述的介紹，會發現一個問題，這是一個非常巨大的 NN model。假設 word vectors 為300維的向量，具有10,000個字詞時，總共會有 &lt;span  class=&#34;math&#34;&gt;\(300 \times10000 = 3\)&lt;/span&gt; 百萬的 weight 需要訓練!! 這樣的計算 gradient descent 時造成模型的訓練時間會非常的久。&lt;/p&gt;

&lt;p&gt;對於這問題，Word2Vec 的作者在&lt;a href=&#34;https://arxiv.org/pdf/1310.4546.pdf&#34;&gt;paper&lt;/a&gt;第二部分有提出以下的解決方法:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Treating common word pairs or phrases as single words in their model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subsampling frequent words&lt;/strong&gt; to decrease the number of training examples.&lt;/li&gt;
&lt;li&gt;Modifying the optimization objective with a technique they called &lt;strong&gt;Negative Sampling&lt;/strong&gt;, which causes each training sample to update only a small percentage of the model’s weights&lt;/li&gt;
&lt;li&gt;A computationally efficient approximation of the full softmax is the &lt;strong&gt;hierarchical softmax&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Subsampling 與 Negative Sampling 這兩個實作方法不只加速了模型的訓練速度，同時也提升模型的準確率。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Words pairs and phrases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如說 New York City 在字詞訓練時，會拆成 New、York、City 三個字詞，但是這樣分開來無法表達出原意，所以將&amp;quot;New York City&amp;quot;組合為一個單詞做訓練。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Subsampling frequent words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在剛剛透過下圖解釋了相關的原理，但是這會發現兩個問題，一是像是出現(the, fox)這樣的 pair，並沒有告訴我們有用的資訊，並且&amp;quot;the&amp;quot;是常出現的字詞；二是有大量像是&amp;quot;the&amp;quot;這類的字詞出現在文章，要如何有意義地學習&amp;quot;the&amp;quot;字詞表示的意思。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;training_data.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;subsamplig 針對這樣的狀況，透過一個機率值來判斷詞是否應該保留。機率值計算公式如下:
&lt;span  class=&#34;math&#34;&gt;\(
P\left( w_{i} \right) = \left( \sqrt{\frac{Z(w_{i})}{0.001} + 1} \right) \cdot \frac{0.001}{Z(w_{i})}
\)&lt;/span&gt;
其中$P\left( w_{i} \right)$表示$w_{i}$的出現機率，0.001為默認值。具體結果如下圖，字詞出現的頻率越高，相對被採用的機率越低。
&lt;img src=&#39;subsample_func_plot.png&#39; width=600&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Negative Sampling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此方法目的是希望只透過正確的目標字詞來小改動 weight。比如說，ward pair (fox, qiuck)，在這個例子中&amp;quot;qiuck&amp;quot;為目標字詞，所以標記為1，而其他與 fox 無相關的字詞標記為0，就稱之為 negative sampling，這樣的 output 就有像是 one-hot vector，只有正確的目標字詞為1(positive word)，其他為0(negative word)。
  至於 Negative sampling size 需要多少，底下是Word2Vec的作者給出的建議:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以假設是以上面描述的狀況，qiuck 則為 postive word，另外加上5個 negative word，output 值為6個值，總共有 &lt;span  class=&#34;math&#34;&gt;\(300 \times 6 = 1800\)&lt;/span&gt; 個 weight 需要更新，這樣只佔了原本300萬的 weight 0.06%而已!&lt;/p&gt;

&lt;p&gt;該如何挑選 negative sampling? 則是透過 &lt;code&gt;unigram distribution&lt;/code&gt; 的機率來挑選， 在C語言實作 word2vec 的程式碼中得到以下公式&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  P\left( w_{i} \right) = \frac{f\left( w_{i} \right)^{\frac{3}{4}} }{\sum_{j=0}^{n} \left( f\left( w_{j}\right)^{\frac{3}{4}} \right)}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;$\frac{3}{4}$次方的選擇是來至於實驗測試的結果。&lt;/p&gt;

&lt;p&gt;Define Objective function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(
  log \space \sigma \left( v_{I}^{T} \cdot v_{o} \right) - \sum_{i=1}^{k} E_{w_{i} -&gt; P_{v}}[\sigma\left( -v_{w_{i}}^{T}v_{wI} \right)]
  \)&lt;/span&gt;
  $ Note \space \sigma(-x) = 1 - \sigma(x)$&lt;/p&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;An implementation &lt;a href=&#34;http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/&#34;&gt;skip-gram of word2vec&lt;/a&gt; from Thushan Ganegedara&lt;/li&gt;
&lt;li&gt;An implementation &lt;a href=&#34;http://www.thushv.com/natural_language_processing/word2vec-part-2-nlp-with-deep-learning-with-tensorflow-cbow/&#34;&gt;CBOW of word2vec&lt;/a&gt; from Thushan Ganegedara&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&#34;&gt;Word2Vec Tutorial Part1&lt;/a&gt; and &lt;a href=&#34;http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/&#34;&gt;Part2&lt;/a&gt; from Chris McCormick&lt;/li&gt;
&lt;li&gt;Deep understand with word2vec form &lt;a href=&#34;http://cpmarkchang.logdown.com/posts/773062-neural-network-word2vec-part-1&#34;&gt;Mark Chang&#39;s Blog (Chinese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;plus-reference&#34;&gt;Plus reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/fastText&#34;&gt;FastText&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;word embedding: 將單詞word映射到另一個空間，其中這個映射具有injective和structure-preserving的特性。
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
