<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | Roymond Liao</title>
    <link>https://roymondliao.github.io/categories/nlp/</link>
      <atom:link href="https://roymondliao.github.io/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 - © 2020</copyright><lastBuildDate>Fri, 28 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://roymondliao.github.io/img/icon-192.png</url>
      <title>NLP</title>
      <link>https://roymondliao.github.io/categories/nlp/</link>
    </image>
    
    <item>
      <title>Transformer Part 2 - Attention</title>
      <link>https://roymondliao.github.io/post/2019-12-16_transformer_part2/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/2019-12-16_transformer_part2/</guid>
      <description>&lt;h1 id=&#34;attention-mechanism&#34;&gt;Attention Mechanism&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt; 的概念在 2014 年被 Bahdanau et al. [Paper 1] 所提出，解決了 encoder-decoder 架構的模型在 decoder 必須依賴一個固定向量長度的 context vector 的問題。實際上 attention mechanism 也符合人類在生活上的應用，例如：當你在閱讀一篇文章時，會從上下文的關鍵字詞來推論句子所以表達的意思，又或者像是在聆聽演講時，會捕捉講者的關鍵字，來了解講者所要描述的內容，這都是人類在注意力上的行為表現。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;用比較簡單的講法來說， attention mechanism 可以幫助模型對輸入 sequence 的每個部分賦予不同的權重， 然後抽出更加關鍵的重要訊息，使模型可以做出更加準確的判斷。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;複習一下在之前介紹的 Seq2Seq model 中，decoder 要預測在給定 context vector 與先前預測字詞 &lt;span  class=&#34;math&#34;&gt;\({y_1, \cdots, y_{t-1}}\)&lt;/span&gt; 的條件下字詞 $y_{t}$ 的機率，所以 decoder  的定義是在有序的條件下所有預測字詞的聯合機率：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
p(\mathrm{y}) &amp; = \prod_{t=1}^T p(y_t | \{y_1, \cdots, y_{t-1}\}, c) \tag 1 \\
\mathrm{y} &amp; = (y_1, \cdots, y_T)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;在第 $t$ 時間，字詞 $y_t$ 的條件機率：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
p(y_t | \{y_1, \cdots, y_{t-1}\}, c) = g(y_{t-1}, s_t, c) \tag 2
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;當中 $g$ 唯一個 nonlinear function，可以為多層的架構，$s_t$ 為 hidden state，c 為 context vector。&lt;/p&gt;

&lt;p&gt;而在 Attention model 中，作者將 decoder 預測下一個字詞的的條件機率重新定義為：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
p(y_i | \{y_1, \cdots, y_{i-1}\}, \mathrm{x}) = g(y_{i-1}, s_t, c_i) \tag 3
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;當中 $s_i$ 表示 RNN 在 $i$ 時間的 hiddent state。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
s_i = f\left(s_{i-1}, y_{i-1}, c_i\right) \tag 4
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;將式子 (3) 與 (2) 相比就可以發現，每一個預測字詞 $y_i$ 對於 context vector 的取得，由原本都是固定的 C  轉變成 每個字詞預測都會取得不同的 $C_i$。&lt;/p&gt;

&lt;p&gt;Bahdanau Attention model 的架構如圖1：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_bahdanau.png&#34; style=&#34;zoom:60%&#34; /&gt;
  &lt;figcaption&gt;
  圖1 (Image credit:[Paper 1])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Context vector $c_i$ 是取決於 sequence of annotations &lt;span  class=&#34;math&#34;&gt;\((h_1, h_2, \cdots, h_{T_x})\)&lt;/span&gt; 的訊息，annotation $h_i$ 包含了在第 $i$ 步下， input sequence 輸入到 econder 的訊息。計算方法是透過序列權重加總 annotation $h_i$，公式如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{equation}
c_i = \displaystyle\sum_{j=1}^{T_x}\alpha_{ij}h_j \tag5
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $i$ 表示 decoder 在第 $i$ 個字詞，$j$ 表示 encoder 中第 $j$ 個詞。&lt;/p&gt;

&lt;p&gt;$\alpha_{ij} $ 則稱之為 attention distribution，可以用來衡量 input sequence 中的每個文字對 output sequence 中的每個文字所帶來重要性的程度，計算方式如下
：
&lt;span  class=&#34;math&#34;&gt;\(
\begin{align}
\alpha_{ij} &amp; = softmax(e_{ij}) \\
&amp; = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} \tag6 \\
\end{align}
\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
e_{ij} = a(s_{i-1}, h_j) \tag7
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;計算 attention  score $e_{ij}$ 中 $a$ 表示為 alignment model (對齊模型)，是衡量 input sequence 在位置 $j$ 與 output sequence 位置 $i$ 這兩者之間的關係&lt;/strong&gt;。
這邊作者為了解決在計算上需要 $T_{x} \times T_{y}$ 的計算量，所以採用了 singlelayer multilayer perceptron 的方式來減少計算量，其計算公式：
&lt;span  class=&#34;math&#34;&gt;\(
\begin{align}
a(s_{i-1}, h_j) = v_a^Ttanh(W_aS_{i-1} + U_ah_j) \tag8
\end{align}
\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $W_a \in R^{n\times n}，U_a \in R^{n \times 2n}，v_a \in R^n$ 都是 weight。&lt;/p&gt;

&lt;p&gt;另外作者在此採用了 BiRNN(Bi-directional RNN) 的 forward 與 backward 的架構，由圖一可以得知&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Forward hidden state 為 &lt;span  class=&#34;math&#34;&gt;\((\overrightarrow{h_1}, \cdots, \overrightarrow{h_{T_x}})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Backward hidden state 為 &lt;span  class=&#34;math&#34;&gt;\((\overleftarrow{h_1}, \cdots, \overleftarrow{h_{T_x}})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Concatenate forward 與 backward 的 hidden state，所以 annotation $h_j$ 為 &lt;span  class=&#34;math&#34;&gt;\(\left[\overrightarrow{h_j^T};\overleftarrow{h_j^T}\right]^T\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這樣的方式更能理解句子所要表達的意思，並得到更好的預測結果。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;例如以下兩個句子的比較：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;我喜歡蘋果，因為它很好吃。&lt;/li&gt;
&lt;li&gt;我喜歡蘋果，因為它很潮。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;下圖為 Bahdanau Attention model 的解析可以與圖1對照理解，這樣更能了解圖一的結構：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;需要注意的一點是在最一開始的 decoder hidden state $S_0$ 是採用 encoder 最後一層的 output&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_bahdanau_example.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;
  圖2
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;下圖為論文中英文翻譯成法語的 attention distribution：&lt;/p&gt;

&lt;p&gt;在圖中 $[European \space Economic \space Area]$ 翻譯成$ [zone \space \acute{a}conomique \space europ\acute{e}enne] $ 的注意力分數上，模型成功地專注在對應的字詞上。&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_bahdanau_output.png&#34; style=&#34;zoom:90%&#34; /&gt;
  &lt;figcaption&gt;
  圖3 (Image credit:[Paper 1])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;最後作者後續還有實驗了採用 LSTM 來替換 Vainlla RNN 進行實驗，詳細的公式都有列出在論文中，有興趣的可以看一下。&lt;/p&gt;

&lt;h1 id=&#34;attention-mechanism-family&#34;&gt;Attention Mechanism Family&lt;/h1&gt;

&lt;h3 id=&#34;hard-attention--soft-attention&#34;&gt;Hard Attention &amp;amp; Soft Attention&lt;/h3&gt;

&lt;p&gt;Xu et al. [Paper 2] 對於圖像標題(caption)的生成研究中提出了 hard attention 與 soft attention 的方法，作者希望透過 attention mechanism 的方法能夠讓 caption 的生成從圖像中獲得更多有幫助的訊息。下圖為作者所提出的模型架構：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./nic_figure1.jpg&#34; style=&#34;zoom:70%&#34; /&gt;
  &lt;figcaption&gt;
  圖4 (Image credit:[Paper 2])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;模型結構&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 encoder 端模型使用 CNN 來提取 low-level 的卷積層特徵，每一個特徵都對應圖像的一個區域&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ a = \{a_1, \dots, a_L\}, a_i \in R^D \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;總共有 $L$ 個特徵，特徵向量維度為 $D$。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Decoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;採用 LSTM 模型來生成字詞，而因應圖片的內容不同，所以標題的長度是不相同的，作者將標題 $y $ encoded 成一個 one-hot encoding 的方式來表示&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[ y = \{y_1, \dots, y_C\}, y_i \in R^K \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;K 為字詞的數量，C 為標題的長度。下圖為作者這本篇論文所採用的 LSTM 架構：&lt;/p&gt;

&lt;p&gt;&lt;figure class=&#34;image&#34;&gt;
  &lt;center&gt;
  &lt;img src=&#34;./attention_soft_and_hard.png&#34; style=&#34;zoom:50%&#34; /&gt;
  &lt;figcaption&gt;
  圖5 (Image credit:[Paper 2])
  &lt;/figcaption&gt;
  &lt;/center&gt;
  &lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;利用 affine transformation 的方式  &lt;span  class=&#34;math&#34;&gt;\(T_{s, t} : R^s \rightarrow R^t\)&lt;/span&gt; 來表達 LSTM 的公式：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{pmatrix}
  i_t \\
  f_t \\
  o_t \\
  g_t 
  \end{pmatrix}
  = 
  \begin{pmatrix}
  \sigma \\
  \sigma \\
  \sigma \\
  tanh 
  \end{pmatrix}
  T_{D+m+n, n}
  \begin{pmatrix}
  Ey_{t-1} \\
  h_{t-1} \\
  \hat{Z_t}
  \end{pmatrix}  \tag1 \\
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{align}
  c_t &amp; = f_t \odot c_{t-1} + i_t \odot g_t \tag2 \\
  h_t &amp; = o_t \odot tanh(c_t) \tag3
  \end{align}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(i_t\)&lt;/span&gt; : input gate&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(f_t\)&lt;/span&gt; : forget gate&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(o_t\)&lt;/span&gt; : ouput gate&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(g_t\)&lt;/span&gt; : canaidate cell&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(c_t\)&lt;/span&gt; : memory cell&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(h_t\)&lt;/span&gt; : hidden state&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(Ey_{t-1}\)&lt;/span&gt; 是詞 &lt;span  class=&#34;math&#34;&gt;\(y_{t-1}\)&lt;/span&gt; 的 embedding vector，&lt;span  class=&#34;math&#34;&gt;\(E \in R^{m \times k}\)&lt;/span&gt; 為 embedding matrix，m 為 embedding dimention&lt;/li&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(\hat{Z} \in R^D\)&lt;/span&gt; 是 context vector，代表捕捉特定區域視覺訊息的上下文向量，與時間 $t$ 有關，所以是一個動態變化的量
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;特別注意的是作者在給定 memory state 與 hidden state 的初始值的計算方式使用了兩個獨立的多層感知器(MLP)，其輸入是各個圖像區域特徵的平均，計算公式如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{align}
  c_0 = f_{init, c}( \frac{1}{L} \sum_{i}^L a_i) \\
  h_0 = f_{init, h}( \frac{1}{L} \sum_{i}^L a_i)
  \end{align}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;以及作者為了計算在 $t$ 時間下所關注的 context vector &lt;span  class=&#34;math&#34;&gt;\(\hat{Z_t}\)&lt;/span&gt; &lt;strong&gt;定義了 attention machansim $\phi$ 為在 $t$ 時間，對於每個區域 $i$ 計算出一個權重 &lt;span  class=&#34;math&#34;&gt;\(\alpha_{ti}\)&lt;/span&gt; 來表示產生字詞 $y_t$ 需要關注哪個圖像區域  annotation vectors $a_i, i=1, \dots, L$ 的訊息。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;權重 &lt;span  class=&#34;math&#34;&gt;\(\alpha_i\)&lt;/span&gt; 的產生是透過輸入 annotation vector &lt;span  class=&#34;math&#34;&gt;\(a_i\)&lt;/span&gt; 與前一個時間的 hidden state  $h_{t-1}$ 經由 attention model $f_{att}$ 計算所產生。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  \begin{align}
  e_{ti} = f_{att}(a_i, h_{t-1}) \tag4 \\
  \alpha_{ti} = \frac{exp(e_{ti})}{\sum_{k=1}^{L}exp{e_{tk}}} \tag5 \\
  \hat{Z_t} = \phi(\{a_i\}, \{\alpha_{i}\}) \tag6
  \end{align}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;有了上述的資訊，在生成下一個 $t$ 時間的字詞機率可以定義為：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  p(y_t | a, y_1, y_2, \dots, y_{t-1}) \propto exp(L_o(Ey_{t-1} + L_hh_t + L_z\hat{Z_t})) \tag7
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 &lt;span  class=&#34;math&#34;&gt;\(L_o \in R^{K \times m}, L_h \in R^{m \times n}, L_z \in R^{m \times D}\)&lt;/span&gt;，m 與 n 分別為 embedding dimension 與 LSTM dimension。&lt;/p&gt;

&lt;p&gt;對於函數 $\phi$ 作者提出了兩種 attention  machansim，對應於將權重附加到圖像區域的兩個不同策略。根據上述的講解，搭配下圖為 Xu et al. [Paper 2] 的模型架構解析，更能了解整篇論文模型的細節：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_soft_and_hard_example.png&#34; style=&#34;zoom:90%&#34; /&gt;
  &lt;figcaption&gt;
  圖6
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h4 id=&#34;hard-attention-stochastic-hard-attention&#34;&gt;Hard attention (Stochastic Hard Attention)&lt;/h4&gt;

&lt;p&gt;在 hard attention 中定義區域變數(location variables) $s_{t, i}$ 為在 t 時間下，模型決定要關注的圖像區域，用 one-hot 的方式來表示，要關注的區域 $i$ 為 1，否則為 0。&lt;/p&gt;

&lt;p&gt;$s_{t, i}$ 被定為一個淺在變數(latent variables)，並且以 &lt;strong&gt;multinoulli distriubtion&lt;/strong&gt; 作為參數 $\alpha_{t, i}$ 的分佈，而 $\hat{Z_t}$ 則被視為一個隨機變數，公式如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
p(s_{t, i} = 1 | s_{j, t}, a) = \alpha_{t, i} \tag8 
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\hat{Z_t} = \sum_{i} s_{t, i}a_i \tag9
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;定義新的 objective function $L_s$ 為 marginal log-likelihood $\text{log }p(y|a)$ 的下界(lower bound)&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
L_s &amp; = \sum_s p(s|a)\text{log }p(y|s,a) \\
&amp; \leq \text{log } \sum_s p(s|a)p(y|s,a) \\
&amp; = \text{log }p(y|a)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;在後續的 $L_s$ 推導求解的過程，作者利用了&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Monte Carlo 方法來估計梯度，利用 moving average 的方式來減小梯度的變異數&lt;/li&gt;
&lt;li&gt;加入了 multinouilli distriubtion 的 entropy term $H[s]$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;透過這兩個方法提升隨機算法的學習，作者在文中也提到，最終的公式其實等價於 &lt;strong&gt;Reinforce learing&lt;/strong&gt;。作者在論文中有列出推導的公式，有興趣的可以直接參考論文。&lt;/p&gt;

&lt;h4 id=&#34;soft-attention-deterministic-soft-attention&#34;&gt;Soft attention (Deterministic Soft Attention)&lt;/h4&gt;

&lt;p&gt;Soft attention 所關注的圖像區域並不像 hard attention 在特定時間只關注特定的區域，在 soft attention 中則是每一個區域都關注，只是關注的程度不同。透過對每個圖像區域 $a_{i}$ 與對應的 weight $\alpha_{t,i}$ ，$\hat{Z}_t$ 就可以直接對權重做加總求和，從 hard attention  轉換到 soft attention 的 context vector：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\hat{Z_t} = \sum_{i} s_{t, i}a_i \implies \mathbb{E}{p(s_t|a)}[\hat{Z_t}] = \sum_{i=1}^L \alpha_{t,i}a_i
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;這計算方式將 weight vector $\alpha_i$ 參數化，讓公式是可微的，可以透過 backpropagation 做到 end-to-end 的學習。其方法是參考前面所介紹的 Bahdanau attention 而來。&lt;/p&gt;

&lt;p&gt;作者在這邊提出三個理論：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;span  class=&#34;math&#34;&gt;\(\mathbb{E}{p(s_t|a)}[h_t]\)&lt;/span&gt; 等同於透過 context vector &lt;span  class=&#34;math&#34;&gt;\(\mathbb{E}{p(s_t|a)}[\hat{Z_t}]\)&lt;/span&gt; 使用 forward propagation 的方法計算 $h_t$&lt;/li&gt;
&lt;li&gt;Normalized weighted geometric mean approximation&lt;/li&gt;
&lt;li&gt;根據公式(7)定義 &lt;span  class=&#34;math&#34;&gt;\(n_t = L_o(Ey_{t-1} + L_hh_t + L_z\hat{Z_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以 soft attention 在最後做文字的預測時作者定義了 softmax $k^{th}$ 的 normalized weighted geometric mean。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
NWGM[p(y_t=k|a)] &amp; = \frac{\prod_i exp(n_{t,k,i})^{p(s_{t,i} = 1 | a)}}{\sum_j\prod_i exp(n_{t,j,i})^{p(s_{t,i} = 1 | a)}} \\
&amp; = \frac{exp\left(\mathbb{E_{p(s_t) | a}[n_{t,k}]}\right)}{\sum_j exp\left(\mathbb{E_{p(s_t) | a}[n_{t,j}]}\right)}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\mathbb{E}[n_t] =  L_o(Ey_{t-1} + L_h\mathbb{E}[h_t] + L_z\mathbb{E}[\hat{Z_t}])
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;這邊的部分就是 soft attention  與 Bahdanau attention 的主要差異，在 Bahdanau attention 最後的 output 是透過 softmax 來取得下一次詞的機率，而作者在這邊採用了 NWGM 的方式。這邊並不是很清楚作者怎麼來證明這樣的論述，日後有理解出來或是有找到相關的參考再補上來。&lt;/p&gt;

&lt;p&gt;最後來看看 soft attention 與 hard attention 的圖像視覺化結果，下圖是兩種 attention 對於圖像區域注意程度，可以看得出 hard attention 都會專注在很小的區域，而 soft attention 的注意力相對發散，這也是因為 soft 與 hard 在關注圖像區域上的一個是注意全部的圖像區域，一個是注意特定的區域。&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_soft_and_hard_visualization.png&#34; style=&#34;zoom:90%&#34; /&gt;
  &lt;figcaption&gt;
  圖7 (Image credit:[Paper 2])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;global-attention--local-attention&#34;&gt;Global Attention &amp;amp; Local Attention&lt;/h3&gt;

&lt;p&gt;Loung et al. [Paper 3] 在 2015 年所發表了 global / local attention 來提升 NMT 任務上的準確度，global attention 類似於 soft attention，而 local attention 則介於 hard 與 soft attention 的混合。&lt;/p&gt;

&lt;p&gt;Global / local attention 相同之處：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;採用的 target hidden state $h_t$ 是 stacking LSTM 最後一層的 output&lt;/li&gt;
&lt;li&gt;Context vector $c_t$ 都是將 $h_t$ 與 source-side $\bar{h_s}$ 作為 input 計算&lt;/li&gt;
&lt;li&gt;結合 $c_t$ 與 $h_t$ 的訊息計算 &lt;span  class=&#34;math&#34;&gt;\(\tilde{h_t} = tanh\left(W_c[c_t;h_t]\right)\)&lt;/span&gt;，稱為 attentional vector&lt;/li&gt;
&lt;li&gt;預測 $t$ 時間下的生成字詞的機率 &lt;span  class=&#34;math&#34;&gt;\( p(y_t|y_{\text{&lt;}t}, x) = softmax(W_s\tilde{h_t})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Global / local attention 不同之處：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Context vector $c_t$ 的計算方式不同&lt;/li&gt;
&lt;li&gt;採用 source side  $\bar{h_s}$ 的數量不同&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下來分別介紹 global attention 與 local attention 當中的細節。&lt;/p&gt;

&lt;h4 id=&#34;global-attention&#34;&gt;Global attention&lt;/h4&gt;

&lt;p&gt;作者將 alignment vector $a_t$ 定義是一個可變長度向量，所以在 global attention 中 $a_t$ 將全部時間的 source side 的資訊當作 input ，公式如下：
&lt;span  class=&#34;math&#34;&gt;\(
\begin{align}
a_t(s) &amp;= align(h_t, \bar{h_s}) \tag 1 \\ 
&amp;= \frac{exp(score(h_t,\bar{h_s}))}{\sum_{s&#39;}exp\left(score(h_t,\bar{h_{s&#39;}})\right)}
\end{align}
\)&lt;/span&gt;
在 score function 的部分，這邊定義了 &lt;strong&gt;content-based function&lt;/strong&gt;，可以有以下三種形式：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
score(h_t, \bar{h_s}) = 
\begin{cases}
h_{t}^T\bar{h_s} &amp; \text{dot} \\
h_{t}^TW_a\bar{h_s} &amp; \text{general} \\
v_a^Ttanh\left(W_a[h_t;\bar{h_s}]\right) &amp; \text{concat}
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $W_a, v_a$ 都是透過訓練所得到的參數。&lt;/p&gt;

&lt;p&gt;另外還有一種 &lt;strong&gt;local-based function&lt;/strong&gt;，score只單存參考 target hidden state $h_t$ 的結果&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
a_t = softmax(W_ah_t) &amp;&amp; \text{location} 
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;下圖為 global attention 的模型架構：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_global.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖8 Global Attention (Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Global attention 與 Bahdanau attention 對比，差異的地方如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在 encoder 與 decoder 都採用最後一層的 LSTM output 作為 hidden state&lt;/li&gt;
&lt;li&gt;計算順序為 &lt;span  class=&#34;math&#34;&gt;\(h_t \rightarrow a_t \rightarrow c_t \rightarrow \tilde{h_t}\)&lt;/span&gt;，而 bahdanau attention 是 &lt;span  class=&#34;math&#34;&gt;\(h_{t-1} \rightarrow a_t \rightarrow c_t \rightarrow h_t\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;local-attention&#34;&gt;Local attention&lt;/h4&gt;

&lt;p&gt;相較於 global attention 採用的所有 soucre side 的字詞，在計算上可能較為龐大，而且面對較長的句子可能無法翻譯正確的狀況，提出了 local attention，只專注關心一小部分的字詞來替代關注全部的字詞。&lt;/p&gt;

&lt;p&gt;前面提到 local attention 是 hard 與 soft attention 的混合，選擇性地關注一個數量較小的上下文窗口，並且是可以微分的，減少了 soft attention 的計算量以及避免了 hard attention 不可微分的問題，更容易的訓練。&lt;/p&gt;

&lt;p&gt;注意的重點：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在每個 $t$ 時間下生成 aligned position $p_t$&lt;/li&gt;
&lt;li&gt;Context vector $c_t$ 則是計算 $[p_t - D, p_t + D]$ 之間的 source hidden 做加權平均，$D$ 是可調的參數。如果所選擇的範圍超過句子本身的長度，則忽略掉多出來的部分，只考慮有存在的部分。&lt;/li&gt;
&lt;li&gt;Alignment vector $a_t \in R^{2D+1}$ 是固定的維度&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;作者針對模型提出了兩遍變形：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Monotonic aligment (&lt;strong&gt;local-m&lt;/strong&gt;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;假設 source 與 target sequences 是單調對齊，就是指 source 與  target 長度相同：
&lt;span  class=&#34;math&#34;&gt;\(
p_t = t
\)&lt;/span&gt;
Alignment vector $a_t$ 的計算就跟公式(1)一致。&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Predictive alignment (&lt;strong&gt;local-p&lt;/strong&gt;)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;認為 source 與 target sequences 是並非單調對齊，就是長度不相同：
&lt;span  class=&#34;math&#34;&gt;\(
p_t = S \cdot sigmoid\left( v_p^Ttanh(W_ph_t)\right)
\)&lt;/span&gt;
$W_p, v_p$ 都是可訓練的模型參數，$S$ 則表示 source sequence 的長度，$p_t \in [0, S]$。&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alignment vector $a_t$ 的計算採用的 Gaussian distribution 來賦予 alignment 的權重：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
a_t(s) = align(h_t,\bar{h_s})exp\left(-\frac{(s-p_t)^2}{2\sigma^2}\right)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;$align(h_t,\bar{h_s})$ 與公式(1)一致，標準差設定為 $\sigma = \frac{D}{2}$，這是透過實驗所得來。&lt;/p&gt;

&lt;p&gt;下圖為 local attention 的模型架構：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_local.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖9 Local Attention (Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Local attention 可能的會遇到的問題：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;當 encoder 的 input sequence 長度不長時，計算量並不會減少&lt;/li&gt;
&lt;li&gt;當 Aligned position $p_t$ 不準確時，會直接影響到 local attention 的準確度&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;inputfeeding-approach&#34;&gt;Input-feeding Approach&lt;/h4&gt;

&lt;p&gt;作者認為在 global 與 local attention 的方法中，模型的注意力機制是獨立的，但是在整個翻譯的過程中，必須要去了解哪些資訊已經被翻譯了，所以在預測下一個翻譯字詞時，應該結合過去 attentional vectors $\tilde{h_t}$ 的資訊，也就是說在 deocder 這邊多考慮了 alignment model 的結果，如下圖所示：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_global_and_local_input_feeding.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖10 Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;來看看論文中的針對各個模型在 WMT&#39;14 英文翻譯成德文資料集的訓練結果：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;這邊的實驗限制了字詞的數量，只取在資料集中最常出現的 50k 字詞當作 corpos&lt;/li&gt;
&lt;li&gt;如果出現字詞沒有在 corpos 中，則用 &lt;strong&gt;&amp;lt;unk&amp;gt;&lt;/strong&gt; 來取代&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./attention_global_and_local_result.png&#34; style=&#34;zoom:40%&#34; /&gt;
  &lt;figcaption&gt;
  圖11 (Image credit:[Paper 3])
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Global attention 與 local attention 都有自己的優勢，如果說要選用哪個方式來當作模型，認為因應不同的任務可能表現都會有所差異，所以建議兩種都實驗看看結果來比較優劣，而在實際上大多數採用的都是以 Global attention 為主。&lt;/p&gt;

&lt;p&gt;底下列出上面各篇論文所提到的 &lt;strong&gt;Attention score function&lt;/strong&gt;：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Attention score function&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Dot-product&lt;/td&gt;
&lt;td&gt;$score(s_t, h_i) = S_t^Th_i$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;General&lt;/td&gt;
&lt;td&gt;$score(s_t, h_i) = S_t^TW_ah_i$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Additive&lt;/td&gt;
&lt;td&gt;$score(s_t, h&lt;em&gt;i) = v^Ttanh(WS&lt;/em&gt;{t-1} + Uh_i) $&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Scaled Dot-product&lt;/td&gt;
&lt;td&gt;$score(s_t, h_i) = \frac{S_t^Th_i}{\sqrt{d}}$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Loocation&lt;/td&gt;
&lt;td&gt;$a_{t} = softmax(W_aS_t)$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;總結來說：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Attention 的目的就是要實現的就是在 decoder 的不同時刻可以關注不同的圖像區域或是句子中的文字，進而可以生成更合理的詞或是結果。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;refenece&#34;&gt;Refenece&lt;/h2&gt;

&lt;p&gt;Paper:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34;&gt;Dzmitry Bahdanau, KyungHyun Cho Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.03044.pdf&#34;&gt;Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1508.04025.pdf&#34;&gt;Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.02874&#34;&gt;Sneha Chaudhari, Gungor Polatkan , Rohan Ramanath , Varun Mithal, An Attentive Survey of Attention Models(2019)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Illustrate:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/37601161h&#34;&gt;https://zhuanlan.zhihu.com/p/37601161h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/31547842&#34;&gt;https://zhuanlan.zhihu.com/p/31547842&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.floydhub.com/attention-mechanism/#bahdanau-atth&#34;&gt;https://blog.floydhub.com/attention-mechanism/#bahdanau-atth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf&#34;&gt;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/Determined22/p/6914926.html&#34;&gt;https://www.cnblogs.com/Determined22/p/6914926.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&#34;&gt;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf&#34;&gt;http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jiqizhixin.com/articles/2018-06-11-16&#34;&gt;https://www.jiqizhixin.com/articles/2018-06-11-16&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@joealato/attention-in-nlp-734c6fa9d983&#34;&gt;https://medium.com/@joealato/attention-in-nlp-734c6fa9d983&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3&#34;&gt;https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms&#34;&gt;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/80692530&#34;&gt;https://zhuanlan.zhihu.com/p/80692530&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tutorial:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/nmt#background-on-the-attention-mechanism&#34;&gt;Neural Machine Translation (seq2seq) Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/text/transformerG&#34;&gt;https://www.tensorflow.org/tutorials/text/transformerG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/2018/04/03/attention.html&#34;&gt;Guide annotating the paper with PyTorch implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Visualization:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jessevig/bertviz&#34;&gt;https://github.com/jessevig/bertviz&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Transformer Part 1 - Seq2Seq</title>
      <link>https://roymondliao.github.io/post/2019-12-16_transformer_part1/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/2019-12-16_transformer_part1/</guid>
      <description>&lt;p&gt;一開始接觸 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇論文是從 &lt;a href=&#34;https://www.youtube.com/playlist?list=PLqFaTIg4myu8t5ycqvp7I07jTjol3RCl9&#34;&gt;Kaggle Reading Group&lt;/a&gt; 這個 channel 開始，非常推薦可以跟著一起讀!!&lt;/p&gt;

&lt;p&gt;主持人 &lt;a href=&#34;https://www.kaggle.com/rtatman&#34;&gt;Rachael Atman&lt;/a&gt; 本身是 Kaggle 的 Data Scientist，她的導讀我覺得是流暢的，但她自己本身有說過並非是 NLP 領域的專家，所以在 kaggle reading group 裡閱讀的論文也有可能是她完全沒接觸過的，整個 channel 帶給你的就是一個啟發，讓你覺得有人跟你一起閱讀的感覺，然後過程中也些人會在 channel 的 chat room 提出一些看法或是連結，Rachael 本身也會提出自己的見解，可以多方面參考。&lt;/p&gt;

&lt;p&gt;在跟完整個 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 的影片後，還是有太多細節是不清楚的，因為自己本身也不是這個領域的，所以開始追論文中所提到的一些關鍵名詞，就開始從 $seq2seq \rightarrow attention \rightarrow self-attention$。這中間 有太多知識需要記錄下來，所以將論文的內容分成三部曲，來記錄閱讀下來的點點滴滴:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Part 1: Sequence to sequence model 起源&lt;/li&gt;
&lt;li&gt;Part 2: Attention 與 Self-attention 的理解&lt;/li&gt;
&lt;li&gt;Part 3: Transformer 的架構探討與深入理解&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要談論 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇 paper 就必須從 seq2seq 講起，seq2seq 全名為 Sequence to Sequence[1]，是一個 Encoder - Decoder 架構的模型，在 2014 年被提出，被廣泛的應用於 Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.&lt;/p&gt;

&lt;h3 id=&#34;sequence-to-sequence-model&#34;&gt;Sequence to sequence model&lt;/h3&gt;

&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;簡單來說，期望輸入一串序列(source)，輸出一串序列(target)，而這個 source 與 target 可以是什麼呢？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果以 machine translation 來說，任務是中翻英，輸入是一句中文，而輸出則會是一句英文。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果是 text summarization，輸入則會是一段文章，而輸出則會是一段摘要&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這就是 seq2seq model 所要解決的問題，在輸入一些資訊後，經過 encoder-decoder 的訓練，可以得到相對應的回答或是其他資訊。&lt;/p&gt;

&lt;h4 id=&#34;model&#34;&gt;Model&lt;/h4&gt;

&lt;p&gt;模型的架構也非常簡單，就如下圖所示：&lt;/p&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./seq2seq.png&#34; style=&#34;zoom:100%&#34; /&gt;
  &lt;figcaption&gt;圖一&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;假設今天我們執行中翻英的任務，輸入(source: X)是一句中文，可以是 &amp;quot;我愛機器學習&amp;quot;，而輸出(target: Y)則會是 &amp;quot;I love machine learning&amp;quot;，所以整個訓練的步驟如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Input sequence $(x_1, x_2, x_3, \dots, x_s)$  經過 embedding layer 的轉換，得到每個 word 的 embedding vector&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Encoder 把所有輸入序列 embedding vector 消化後，將資訊壓縮轉換為一個向量 $C$，稱之為 context vector&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
   \begin{align}
   h_s^{e} &amp; = f_{enc}\left(h_{s-1}^{e}, e_{x_{s-1}}, W_{enc}\right) \\
   C &amp; = h_s^e \text {，最後一步的 hidden state} \\
   C &amp; = q(h_s^e) \text {，最後一步的 hidden state 做 transform } \\
   C &amp; = q\left(h_1^e, h_2^e, \dots, h_s^e\right) \text {，每一步的 hidden state 做 transform }
   \end{align}
   \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $f_{enc}(\cdot)$ 表示 Encoder 中的 RNN function，參數為 $W_{enc}$。$e_{x_s}$ 表示 $x_s$ 的 embedding vector，$h_s^e$ 表示在時間 $s$ 的 hidden state，$C$ 可以表示為 Encoder 最後的 hidden state 或是經過函數 $q(\cdot)$ 的轉換。&lt;/p&gt;

&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Decoder 則根據 context vector 的資訊來生成文字，output sequence $(y_1, y_2, y_3, \dots, y_t)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
   \begin{align}
   h_0^{d} &amp; = C \\
   h_{t}^{d} &amp; = f_{dec}\left(h_{t-1}^{d}, e_{y_{t-1}}, W_{dec}\right) \\
   O_{t} &amp; = g\left(h_t^d\right)
   \end{align}
   \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 $h_{0}^{d}$ 為 context vector 傳進來當作 Decoder 的初始 hidden state，$f_{dec}(\cdot)$ 表示 Decoder 中的 RNN function，參數為 $W_{dec}$。$h_{t}^{d}$ 表示 Decoder在時間 $t$ 的 hidden state，$e_{y_{t}}$ 表示前一步的所得到的 $y_{t-1}$ 結果當作輸入，$y_0$ 都是以特殊索引 &amp;lt;BOS&amp;gt; 當作輸入。
   $g(\cdot)$ 為 output layer，一般都是 softmax function。&lt;/p&gt;

&lt;p&gt;過程就只是簡單的三個步驟，雖然看起來簡單，但當中有些細節是需要注意的。&lt;/p&gt;

&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;特殊索引&lt;/p&gt;

&lt;p&gt;在每個句子做 one-hot-encoder 的轉換時，會在句子的前後加上 &amp;lt;BOS&amp;gt; 與 &amp;lt;EOS&amp;gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BOS: Begin of sequence，在預測的時候我們並沒有對應的答案，所以會先以 &amp;lt;BOS&amp;gt; 當作 $Y_0$ 的 target input&lt;/li&gt;
&lt;li&gt;EOS: End of sequence，用意是要告訴 model 當出現這個詞的時候就是停止生成文字，如果沒有這個詞，模型會無限迴圈的一直生成下去&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了上述的 &amp;lt;BOS&amp;gt; 與 &amp;lt;EOS&amp;gt; 外，還有 &amp;lt;PAD&amp;gt; 與 &amp;lt;UNK&amp;gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PAD: 由於 RNN 的 parameters 是共享的，所以在 input 的維度就需要保持相同，但並不是每個句子的長度都是同樣的，有的可能長度是 3 ，有的長度可能是 5，所以為了處理不同 input sequence 長度不同的狀況，增加了 &amp;lt;PAD&amp;gt; 的字詞，來讓每次 &lt;strong&gt;batch&lt;/strong&gt; 的 input sequence 的長度都是相同的&lt;/li&gt;
&lt;li&gt;UNK: 如果輸入的字詞在 corpus 是沒有出現過的，就會用 &amp;lt;UNK&amp;gt; 索引來代替&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Encoder layer 與 Decoder layer 的選擇&lt;/p&gt;

&lt;p&gt;Encoder 與 Decoder 中的 RNN function 可以是 simple RNN / LSTM / GRU，又或者是一個 bidirectional LSTM 的架構在裡面，也可以是一個 multi layer LSTM&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Teacher forcing&lt;/p&gt;

&lt;p&gt;在 training model 時 ，為了提高 model 的準確度與訓練速度，採用了 &lt;a href=&#34;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&#34;&gt;Teacher forcing training&lt;/a&gt; 方法，圖二就是 teacher foring 的概念，在 training 的時候直接告訴 model 實際的答案，省去 model 自己去尋找到正確的答案。另外也有提出 &lt;a href=&#34;https://arxiv.org/abs/1610.09038&#34;&gt;Professor Forcing&lt;/a&gt; 的做法，尚未理解這方法的概念，提供當作參考。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./teacher_forcing.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;
  圖二 (Image credit: &lt;a href=&#34;https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c&#34;&gt;LINK&lt;/a&gt;)
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;h4 id=&#34;prediction&#34;&gt;Prediction&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Beam search&lt;/p&gt;

&lt;p&gt;在 prediction  model 時，每一步的 output 都是要計算出在 corpus 中生成最可能的那個字詞&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
\hat{y_t} = argmax\space p_{\theta}(y | \hat{y}_{1:(t-1)}) 
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;其中 &lt;span  class=&#34;math&#34;&gt;\(\hat{y}_{1:(t-1)} = \hat{y}_1,\dots,\hat{y}_{t-1}\)&lt;/span&gt; 為前面 $t-1$ 步所生成的字詞。以一個簡單的概念來思考，每一步的 output 都是該步所得到的最大條件機率，那這樣的 greedy search 所得到的結果對於我們的目標並非是最優的，得到的是每個字詞的最大條件機率，而並非是整個句子的最大條件機率，所以這樣的狀況下你所得的的翻譯可能不會是最適合的。&lt;/p&gt;

&lt;p&gt;Beam search 就是為了解決這樣的問題而提出的，在每一步的生成過程中，生成 $B$ 的最可能的文字序列作為約束，其中 $B$ 的大小為 beam width，是一個 hyperparamter。$B$ 值越大可以得到更好的結果，但相對的計算量也增加。&lt;/p&gt;

&lt;p&gt;過程就如同圖三所示:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;建立一個 search tree，該 root 為一個開始符號 (非序列的第一個詞)&lt;/li&gt;
&lt;li&gt;由序列的左到右開始，順序生成目標語言序列，同時成長對應的搜尋樹&lt;/li&gt;
&lt;li&gt;在生成序列的每一步，對  search tree 的每個 leaf node，選取 $B$ 個擁有最高條件機率的生成單詞，並生成 B 個子節點。&lt;/li&gt;
&lt;li&gt;在成長搜尋樹後，進行剪枝的工作，只留下 B 個最高條件機率的葉節點後，再進行下一個位置的序列生成。
&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Beam search 的實作可以參考 Blog:[4]。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
  &lt;img src=&#34;./beam_search.png&#34; style=&#34;zoom:80%&#34; /&gt;
  &lt;figcaption&gt;圖三 (Image credit: &lt;a href=&#34;https://distill.pub/2017/ctc/&#34;&gt; LINK&lt;/a&gt;)
  &lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;另外要注意在使用 beam search 所謂遇到的問題，在 Andrew Ng 大師的&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;課程&lt;/a&gt;中提到&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;消除長度對計算機率影響（Length Normalization）&lt;/li&gt;
&lt;li&gt;如何選擇 Beam Width 參數（The Choice of Beam Width）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;詳細的解說可以參考&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;課程&lt;/a&gt;或是 Blog:[3]。&lt;/p&gt;

&lt;h4 id=&#34;problems&#34;&gt;Problems&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;所有資訊只壓縮成一個 context vector，input sequence 的資訊很難全部保存在一個固定維度的向量裡&lt;/li&gt;
&lt;li&gt;當 sequence 的長度很長時，在 decoder 解碼時，由於 Recurrent neural network 的依賴問題，容易丟失 input sequence 的訊息&lt;/li&gt;
&lt;/ol&gt;

&lt;hr&gt;

&lt;p&gt;在理解完 Seq2Seq model 後，所遇到的問題該如何解決? 那就是要靠 &lt;code&gt;Attention Is All You Need&lt;/code&gt; 這篇 paper 的主要重點之一 &lt;code&gt;Attention mechanism(注意力機制)&lt;/code&gt;，
這部分將會在 part 2 的時候介紹。&lt;/p&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34;&gt;Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, Sequence to Sequence Learning with Neural Networks(2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/pdf/1610.09038.pdf&#34;&gt;Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio, Professor Forcing: A New Algorithm for Training Recurrent Networks(2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio, NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(2014)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&#34;https://arxiv.org/abs/1508.04025&#34;&gt;Minh-Thang Luong, Hieu Pham, Christopher D. Manning, Effective Approaches to Attention-based Neural Machine Translation(2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Blog:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&#34;&gt;https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdfh&#34;&gt;https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdfh&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&#34;https://ithelp.ithome.com.tw/articles/10208587&#34;&gt;https://ithelp.ithome.com.tw/articles/10208587&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&#34;https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/&#34;&gt;https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&#34;https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-%E4%B8%AD%E6%96%87%E7%89%88-2714bbd92727&#34;&gt;Seq2seq pay Attention to Self Attention: Part 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&#34;https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-%E4%B8%AD%E6%96%87%E7%89%88-ef2ddf8597a4&#34;&gt;Seq2seq pay Attention to Self Attention: Part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;&gt;Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&#34;http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/&#34;&gt;http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&#34;https://www.zhihu.com/question/54356960&#34;&gt;https://www.zhihu.com/question/54356960&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Word2Vec</title>
      <link>https://roymondliao.github.io/post/2019-05-02_word2vec/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      <guid>https://roymondliao.github.io/post/2019-05-02_word2vec/</guid>
      <description>&lt;p&gt;最近剛好看到一篇關於 &lt;a href=&#34;https://github.com/priya-dwivedi/Deep-Learning/blob/master/word2vec_skipgram/Skip-Grams-Solution.ipynb&#34;&gt;Skip-gram word2vec&lt;/a&gt;的介紹，內文寫的淺顯易懂，衍生的閱讀也十分詳細，決定動手寫篇記錄下來。&lt;/p&gt;

&lt;p&gt;人對於文字的理解，可以很簡單的就能了解字面的意義，但是對於機器來說，要如何理解文字是一個很困難的問題。
要如何讓機器來理解文字的意義？ 透過將文字轉換成向量，來讓機器能夠讀的懂，所以其實文字對於機器來說只是數字，而我們在做的就只是數字的遊戲。&lt;/p&gt;

&lt;h2 id=&#34;word-embeddings&#34;&gt;Word embeddings&lt;/h2&gt;

&lt;p&gt;在將字詞轉換成向量的實作中，大家常用的方法肯定是 &lt;strong&gt;one-hot-encoding&lt;/strong&gt;，但是 one-hot-encoding 在計算上卻是非常沒有效率的方式，如果一篇文章中總共有50,000的單詞，用 one-hot-encoding 來表示某個單詞的話，將會變成1與49999個0的向量表示。就如同下圖表示，如果要做 matrix multiplication 的話，那將會浪費許多的計算資源。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;one_hot_encoding.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;透過 &lt;strong&gt;Word Embedding&lt;/strong&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; 可以有效的解決上述的問題。 Embedding 可以想成與 full connected layer 一樣，將這個 layer 稱做為 embedding layer ， weight 則稱為 embedding weights。藉由這樣的概念，可以省略掉 multiplication 的過程，直接透過 hidden layer 的 weigth matrix 來當作輸入字詞的 word vector。之所以可以這樣來執行是因為在處理 one-hot-encoding 與 weight matrix 相乘的結果，其實就是 matrix 所對應&amp;quot;詞&amp;quot;的索引值所得到的結果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;lookup_matrix.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;舉例來說： &amp;quot;heart&amp;quot; 的在 one-hot-encoding 的索引位置為958，我們直接拿取 heart 所對應 hidden layer 的值，也就是 embedding weights 的第958列(row)，這樣的過程叫做 &lt;strong&gt;embedding lookup&lt;/strong&gt;，而 hidden layer 的神經元數量則為 &lt;strong&gt;embedding dimension&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;tokenize_lookup.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;另一個解釋是在於 word2vec 是一個三層架構，分別是 input layer、hidden layer、output layer，但是在 hidden layer 並沒有非線性的 activation function，由於 input layer 是經由 one-hot-encoding 過的資訊，所以在 hidden layer 所取得的值，其實就是對應輸入層得值；另外一提 output layer 的 activation function 是 sigmoid。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;word2vec_weight_matrix_lookup_table.png&#39; width=500&gt;&lt;/p&gt;

&lt;p&gt;原文中最後提到的三個主要重點：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The embedding lookup table is just a weight matrix.&lt;/li&gt;
&lt;li&gt;The embedding layer is just a hidden layer.&lt;/li&gt;
&lt;li&gt;The lookup is just a shortcut for the matrix multiplication.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;

&lt;p&gt;介紹完 word embedding 後，要來介紹 word2vec algorithm 中的兩個 model：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Skip-gram&lt;/li&gt;
&lt;li&gt;CBOW(Continous Bag-Of-Words)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#39;word2vec_architectures.png&#39; width=500&gt;&lt;/p&gt;

&lt;h3 id=&#34;skipgram-model&#34;&gt;Skip-gram model&lt;/h3&gt;

&lt;p&gt;用下列兩張圖來解釋 skip-gram model 的結構，假設model是一個simple logistic regression(softmax)，左邊的圖表示為概念上的架構(conceptual architecture)，右邊的圖則為實作上的架構(implemented architectures)，雖然圖的架構有些微不同，但是實際上是一樣的，並沒有任何的改變。
首先定義參數：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;V - Vocabulary Size (Number of unique words in the corpora)&lt;/li&gt;
&lt;li&gt;P - The Projection or the Embedding Layer&lt;/li&gt;
&lt;li&gt;D - Dimensionality of the Embedding Space&lt;/li&gt;
&lt;li&gt;b - Size of a single Batch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#39;skip_gram.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;由左圖可以了解到，Skip-gram model 的 &lt;strong&gt;input(X)&lt;/strong&gt; 為一個單詞，而你的目標，也就是你的  &lt;strong&gt;output(Y)&lt;/strong&gt; 為相鄰的單詞。換句話就是在一句話中，選定句子當中的任意詞作為 input word，而與 input word 相鄰的字詞則為 model 的所要預測的目標(labels)，最後會得到相鄰字詞與 input word 相對應的機率。&lt;/p&gt;

&lt;p&gt;但是上述的想法會出現一個問題，就是你只提供一個字詞的訊息，然而要得到相鄰字詞出現的機率，這是很困難的一件事，效果也不佳。所以這邊提出兩個方法:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;針對&amp;quot;相鄰字詞&amp;quot;這部分，加入了 &lt;strong&gt;window size&lt;/strong&gt; 的參數做調整&lt;/li&gt;
&lt;li&gt;將輸出所有字詞的方式轉成一對一成對的方式&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;舉例來說：&lt;strong&gt;&amp;quot;The dog barked at the mailman.&amp;quot;&lt;/strong&gt; 這樣一句話，選定 dog 做為 input，設定window size = 2，則 &amp;quot;dag&amp;quot; 下上兩個相鄰字詞為 &lt;strong&gt;[&#39;the&#39;, &#39;barked&#39;, &#39;at&#39;]&lt;/strong&gt; 就會是我們的 output。此外將原本的(input: &#39;dag&#39;, output: &#39;[&#39;the&#39;, &#39;barked&#39;, &#39;at&#39;]) 轉換成 (input: &#39;dag&#39;, output: &#39;the&#39;), (input: &#39;dag&#39;, output: &#39;barked&#39;), (input: &#39;dag&#39;, output: &#39;at&#39;) 這樣一對一的方式。這樣的過程就如同右圖 implemented architectures。&lt;/p&gt;

&lt;p&gt;下圖解釋一個語句的training samples產生:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;training_data.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;所以當training samples: (brown, fox)的數量越多時，輸入&lt;code&gt;brown&lt;/code&gt;得到&lt;code&gt;fox&lt;/code&gt;的機率越高。&lt;/p&gt;

&lt;h4 id=&#34;model-details&#34;&gt;Model Details&lt;/h4&gt;

&lt;p&gt;Input layer: 字詞經過 one-hot-encoding 的向量表示。
hidden layer: no activation function，上述介紹 embedding layer 已經解釋過。
output layer: use softmax regression classifier，output 的結果介於0與1之間，且加總所有的值和為1。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;skip_gram_net_arch.png&#39; width=750&gt;&lt;/p&gt;

&lt;p&gt;假設輸入的 word pair 為(ants, able)，則模型的目標是 &lt;span  class=&#34;math&#34;&gt;\(max P\left(able | ants \right)\)&lt;/span&gt;，同時也需要滿足 &lt;span  class=&#34;math&#34;&gt;\(min P\left(other \space words | ants \right)\)&lt;/span&gt;，這裡利用 log-likehood function 作為目標函數。&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
P\left(able | ants \right) = softmax\left( X_{ants 1\times 10000} \cdot W_{10000 \times 300}\right) 
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
Y = sotfmax(Y) =\frac{exp(X_{1 \times 300} \cdot W_{300 \times 1})}{\sum_{i=1}^{10000} exp(X_{1 \times 300}^i \cdot W_{300 \times 1})}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;log-likehood function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
L(W) = P\left(able \mid ants \right)^{y=able} \times P\left(other \space words | ants \right)^{y=other \space words}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Objective function可以表示如下：&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\begin{align}
LogL\left(W\right) 
&amp; = \{y = target \space word\} \{logP\left(able | ants \right) + logP\left(other \space words | ants \right)\}\\
&amp; = \sum_{i}^{10000}\{ y = target \space word\}logP\left( word_{i} | ants \right)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;之後就是 Maxmium log-likehood function。
由上述的介紹，會發現一個問題，這是一個非常巨大的 NN model。假設 word vectors 為300維的向量，具有10,000個字詞時，總共會有 &lt;span  class=&#34;math&#34;&gt;\(300 \times10000 = 3\)&lt;/span&gt; 百萬的 weight 需要訓練!! 這樣的計算 gradient descent 時造成模型的訓練時間會非常的久。&lt;/p&gt;

&lt;p&gt;對於這問題，Word2Vec 的作者在&lt;a href=&#34;https://arxiv.org/pdf/1310.4546.pdf&#34;&gt;paper&lt;/a&gt;第二部分有提出以下的解決方法:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Treating common word pairs or phrases as single words in their model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subsampling frequent words&lt;/strong&gt; to decrease the number of training examples.&lt;/li&gt;
&lt;li&gt;Modifying the optimization objective with a technique they called &lt;strong&gt;Negative Sampling&lt;/strong&gt;, which causes each training sample to update only a small percentage of the model’s weights&lt;/li&gt;
&lt;li&gt;A computationally efficient approximation of the full softmax is the &lt;strong&gt;hierarchical softmax&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Subsampling 與 Negative Sampling 這兩個實作方法不只加速了模型的訓練速度，同時也提升模型的準確率。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Words pairs and phrases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;比如說 New York City 在字詞訓練時，會拆成 New、York、City 三個字詞，但是這樣分開來無法表達出原意，所以將&amp;quot;New York City&amp;quot;組合為一個單詞做訓練。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Subsampling frequent words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在剛剛透過下圖解釋了相關的原理，但是這會發現兩個問題，一是像是出現(the, fox)這樣的 pair，並沒有告訴我們有用的資訊，並且&amp;quot;the&amp;quot;是常出現的字詞；二是有大量像是&amp;quot;the&amp;quot;這類的字詞出現在文章，要如何有意義地學習&amp;quot;the&amp;quot;字詞表示的意思。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;training_data.png&#39; width=600&gt;&lt;/p&gt;

&lt;p&gt;subsamplig 針對這樣的狀況，透過一個機率值來判斷詞是否應該保留。機率值計算公式如下:
&lt;span  class=&#34;math&#34;&gt;\(
P\left( w_{i} \right) = \left( \sqrt{\frac{Z(w_{i})}{0.001} + 1} \right) \cdot \frac{0.001}{Z(w_{i})}
\)&lt;/span&gt;
其中$P\left( w_{i} \right)$表示$w_{i}$的出現機率，0.001為默認值。具體結果如下圖，字詞出現的頻率越高，相對被採用的機率越低。
&lt;img src=&#39;subsample_func_plot.png&#39; width=600&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Negative Sampling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此方法目的是希望只透過正確的目標字詞來小改動 weight。比如說，ward pair (fox, qiuck)，在這個例子中&amp;quot;qiuck&amp;quot;為目標字詞，所以標記為1，而其他與 fox 無相關的字詞標記為0，就稱之為 negative sampling，這樣的 output 就有像是 one-hot vector，只有正確的目標字詞為1(positive word)，其他為0(negative word)。
  至於 Negative sampling size 需要多少，底下是Word2Vec的作者給出的建議:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以假設是以上面描述的狀況，qiuck 則為 postive word，另外加上5個 negative word，output 值為6個值，總共有 &lt;span  class=&#34;math&#34;&gt;\(300 \times 6 = 1800\)&lt;/span&gt; 個 weight 需要更新，這樣只佔了原本300萬的 weight 0.06%而已!&lt;/p&gt;

&lt;p&gt;該如何挑選 negative sampling? 則是透過 &lt;code&gt;unigram distribution&lt;/code&gt; 的機率來挑選， 在C語言實作 word2vec 的程式碼中得到以下公式&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
  P\left( w_{i} \right) = \frac{f\left( w_{i} \right)^{\frac{3}{4}} }{\sum_{j=0}^{n} \left( f\left( w_{j}\right)^{\frac{3}{4}} \right)}
  \]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;$\frac{3}{4}$次方的選擇是來至於實驗測試的結果。&lt;/p&gt;

&lt;p&gt;Define Objective function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(
  log \space \sigma \left( v_{I}^{T} \cdot v_{o} \right) - \sum_{i=1}^{k} E_{w_{i} -&gt; P_{v}}[\sigma\left( -v_{w_{i}}^{T}v_{wI} \right)]
  \)&lt;/span&gt;
  $ Note \space \sigma(-x) = 1 - \sigma(x)$&lt;/p&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;An implementation &lt;a href=&#34;http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/&#34;&gt;skip-gram of word2vec&lt;/a&gt; from Thushan Ganegedara&lt;/li&gt;
&lt;li&gt;An implementation &lt;a href=&#34;http://www.thushv.com/natural_language_processing/word2vec-part-2-nlp-with-deep-learning-with-tensorflow-cbow/&#34;&gt;CBOW of word2vec&lt;/a&gt; from Thushan Ganegedara&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&#34;&gt;Word2Vec Tutorial Part1&lt;/a&gt; and &lt;a href=&#34;http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/&#34;&gt;Part2&lt;/a&gt; from Chris McCormick&lt;/li&gt;
&lt;li&gt;Deep understand with word2vec form &lt;a href=&#34;http://cpmarkchang.logdown.com/posts/773062-neural-network-word2vec-part-1&#34;&gt;Mark Chang&#39;s Blog (Chinese)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;plus-reference&#34;&gt;Plus reference&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/fastText&#34;&gt;FastText&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;word embedding: 將單詞word映射到另一個空間，其中這個映射具有injective和structure-preserving的特性。
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
